{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b575063",
   "metadata": {},
   "source": [
    "# Scientific Literature Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9c1556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from -r ../requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: numpy in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from -r ../requirements.txt (line 2)) (2.4.2)\n",
      "Requirement already satisfied: requests in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from -r ../requirements.txt (line 3)) (2.32.5)\n",
      "Requirement already satisfied: feedparser in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from -r ../requirements.txt (line 4)) (6.0.12)\n",
      "Requirement already satisfied: nltk in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from -r ../requirements.txt (line 5)) (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from -r ../requirements.txt (line 6)) (1.8.0)\n",
      "Requirement already satisfied: sentence-transformers in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from -r ../requirements.txt (line 7)) (5.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from pandas->-r ../requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from pandas->-r ../requirements.txt (line 1)) (2025.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from requests->-r ../requirements.txt (line 3)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from requests->-r ../requirements.txt (line 3)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from requests->-r ../requirements.txt (line 3)) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from requests->-r ../requirements.txt (line 3)) (2026.1.4)\n",
      "Requirement already satisfied: sgmllib3k in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from feedparser->-r ../requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: click in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from nltk->-r ../requirements.txt (line 5)) (8.3.1)\n",
      "Requirement already satisfied: joblib in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from nltk->-r ../requirements.txt (line 5)) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from nltk->-r ../requirements.txt (line 5)) (2026.2.19)\n",
      "Requirement already satisfied: tqdm in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from nltk->-r ../requirements.txt (line 5)) (4.67.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from scikit-learn->-r ../requirements.txt (line 6)) (1.17.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from scikit-learn->-r ../requirements.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from sentence-transformers->-r ../requirements.txt (line 7)) (5.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from sentence-transformers->-r ../requirements.txt (line 7)) (1.4.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from sentence-transformers->-r ../requirements.txt (line 7)) (2.10.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from sentence-transformers->-r ../requirements.txt (line 7)) (4.15.0)\n",
      "Requirement already satisfied: packaging>=20.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 7)) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 7)) (6.0.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 7)) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 7)) (0.24.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: filelock in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r ../requirements.txt (line 7)) (3.24.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r ../requirements.txt (line 7)) (2026.2.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r ../requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r ../requirements.txt (line 7)) (0.28.1)\n",
      "Requirement already satisfied: shellingham in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r ../requirements.txt (line 7)) (1.5.4)\n",
      "Requirement already satisfied: anyio in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers->-r ../requirements.txt (line 7)) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers->-r ../requirements.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers->-r ../requirements.txt (line 7)) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from python-dateutil>=2.8.2->pandas->-r ../requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 7)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 7)) (3.1.6)\n",
      "Requirement already satisfied: setuptools in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 7)) (82.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: colorama in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from tqdm->nltk->-r ../requirements.txt (line 5)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 7)) (3.0.3)\n",
      "Requirement already satisfied: typer>=0.24.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from typer-slim->transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 7)) (0.24.1)\n",
      "Requirement already satisfied: rich>=12.3.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from typer>=0.24.0->typer-slim->transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 7)) (14.3.3)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from typer>=0.24.0->typer-slim->transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 7)) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 7)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 7)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in C:\\Users\\sarth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers<6.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 7)) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b519eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e29161b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "<?xml version='1.0' encoding='UTF-8'?>\n",
      "<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <id>https://arxiv.org/api/CnvybtQa9uYuIE4pdtRKOb0rmDI</id>\n",
      "  <title>arXiv Query: search_query=cat:cs.LG&amp;id_list=&amp;start=0&amp;max_results=1000</title>\n",
      "  <updated>2026-02-23T15:51:44Z</updated>\n",
      "  <link href=\"https://arxiv.org/api/query?search_query=cat:cs.LG&amp;start=0&amp;max_results=1000&amp;id_list=\" type=\"application/atom+xml\"/>\n",
      "  <opensearch:itemsPerPage>1000</opensearch:itemsPerPage>\n",
      "  <opensearch:totalResults>254214</opensearch:totalResults>\n",
      "  <opensearch:startIndex>0</opensearch:startIndex>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.11510v1</id>\n",
      "    <title>Design Rule Checking with a CNN Based Feature Extractor</title>\n",
      "    <updated>2020-12-21T17:26:31Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.11510v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.11510v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Design rule checking (DRC) is getting increasingly complex in advanced nodes technologies. It would be highly desirable to have a fast interactive DRC engine that could be used during layout. In this work, we establish the proof of feasibility for such an engine. The proposed model consists of a convolutional neural network (CNN) trained to detect DRC violations. The model was trained with artificial data that was derived from a set of $50$ SRAM designs. The focus in this demonstration was metal 1 rules. Using this solution, we can detect multiple DRC violations 32x faster than Boolean checkers with an accuracy of up to 92. The proposed solution can be easily expanded to a complete rule set.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-21T17:26:31Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Luis Francisco</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tanmay Lagare</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arpit Jain</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Somal Chaudhary</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Madhura Kulkarni</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Divya Sardana</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>W. Rhett Davis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paul Franzon</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.11638v1</id>\n",
      "    <title>Unsupervised in-distribution anomaly detection of new physics through conditional density estimation</title>\n",
      "    <updated>2020-12-21T19:05:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.11638v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.11638v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Anomaly detection is a key application of machine learning, but is generally focused on the detection of outlying samples in the low probability density regions of data. Here we instead present and motivate a method for unsupervised in-distribution anomaly detection using a conditional density estimator, designed to find unique, yet completely unknown, sets of samples residing in high probability density regions. We apply this method towards the detection of new physics in simulated Large Hadron Collider (LHC) particle collisions as part of the 2020 LHC Olympics blind challenge, and show how we detected a new particle appearing in only 0.08% of 1 million collision events. The results we present are our original blind submission to the 2020 LHC Olympics, where it achieved the state-of-the-art performance.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"hep-ex\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"physics.data-an\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-21T19:05:22Z</published>\n",
      "    <arxiv:comment>Accepted to NeurIPS Machine Learning and the Physical Sciences workshop. See arXiv:2007.00674 for further methods</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>George Stein</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Uros Seljak</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Biwei Dai</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.11325v1</id>\n",
      "    <title>Detecting Botnet Attacks in IoT Environments: An Optimized Machine Learning Approach</title>\n",
      "    <updated>2020-12-16T16:39:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.11325v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.11325v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The increased reliance on the Internet and the corresponding surge in connectivity demand has led to a significant growth in Internet-of-Things (IoT) devices. The continued deployment of IoT devices has in turn led to an increase in network attacks due to the larger number of potential attack surfaces as illustrated by the recent reports that IoT malware attacks increased by 215.7% from 10.3 million in 2017 to 32.7 million in 2018. This illustrates the increased vulnerability and susceptibility of IoT devices and networks. Therefore, there is a need for proper effective and efficient attack detection and mitigation techniques in such environments. Machine learning (ML) has emerged as one potential solution due to the abundance of data generated and available for IoT devices and networks. Hence, they have significant potential to be adopted for intrusion detection for IoT environments. To that end, this paper proposes an optimized ML-based framework consisting of a combination of Bayesian optimization Gaussian Process (BO-GP) algorithm and decision tree (DT) classification model to detect attacks on IoT devices in an effective and efficient manner. The performance of the proposed framework is evaluated using the Bot-IoT-2018 dataset. Experimental results show that the proposed optimized framework has a high detection accuracy, precision, recall, and F-score, highlighting its effectiveness and robustness for the detection of botnet attacks in IoT environments.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-16T16:39:55Z</published>\n",
      "    <arxiv:comment>4 pages, 2 figures, 1 table, Accepted and presented at IEEE 32nd International Conference on Microelectronics (IEEE-ICM2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>MohammadNoor Injadat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abdallah Moubayed</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abdallah Shami</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.11327v1</id>\n",
      "    <title>Collaborative residual learners for automatic icd10 prediction using prescribed medications</title>\n",
      "    <updated>2020-12-16T07:07:27Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.11327v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.11327v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Clinical coding is an administrative process that involves the translation of diagnostic data from episodes of care into a standard code format such as ICD10. It has many critical applications such as billing and aetiology research. The automation of clinical coding is very challenging due to data sparsity, low interoperability of digital health systems, complexity of real-life diagnosis coupled with the huge size of ICD10 code space. Related work suffer from low applicability due to reliance on many data sources, inefficient modelling and less generalizable solutions. We propose a novel collaborative residual learning based model to automatically predict ICD10 codes employing only prescriptions data. Extensive experiments were performed on two real-world clinical datasets (outpatient &amp; inpatient) from Maharaj Nakorn Chiang Mai Hospital with real case-mix distributions. We obtain multi-label classification accuracy of 0.71 and 0.57 of average precision, 0.57 and 0.38 of F1-score and 0.73 and 0.44 of accuracy in predicting principal diagnosis for inpatient and outpatient datasets respectively.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-16T07:07:27Z</published>\n",
      "    <arxiv:comment>6 Pages, 5 Figures and 4 tables. Presented at AIDH (Australian Institute of Digital Health) Conference 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <arxiv:journal_ref>AIDH (Australian Institute of Digital Health) Conference 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Yassien Shaalan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexander Dokumentov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Piyapong Khumrin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Krit Khwanngern</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anawat Wisetborisu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Thanakom Hatsadeang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nattapat Karaket</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Witthawin Achariyaviriya</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sansanee Auephanwiriyakul</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nipon Theera-Umpon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Terence Siganakis</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.11333v1</id>\n",
      "    <title>Ensemble model for pre-discharge icd10 coding prediction</title>\n",
      "    <updated>2020-12-16T07:02:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.11333v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.11333v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The translation of medical diagnosis to clinical coding has wide range of applications in billing, aetiology analysis, and auditing. Currently, coding is a manual effort while the automation of such task is not straight forward. Among the challenges are the messy and noisy clinical records, case complexities, along with the huge ICD10 code space. Previous work mainly relied on discharge notes for prediction and was applied to a very limited data scale. We propose an ensemble model incorporating multiple clinical data sources for accurate code predictions. We further propose an assessment mechanism to provide confidence rates in predicted outcomes. Extensive experiments were performed on two new real-world clinical datasets (inpatient &amp; outpatient) with unaltered case-mix distributions from Maharaj Nakorn Chiang Mai Hospital. We obtain multi-label classification accuracies of 0.73 and 0.58 for average precision, 0.56 and 0.35 for F1-scores and 0.71 and 0.4 accuracy in predicting principal diagnosis for inpatient and outpatient datasets respectively.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-16T07:02:56Z</published>\n",
      "    <arxiv:comment>6 Pages, 2 Figures and 5 tables. Presented at AIDH (Australian Institute of Digital Health) Conference 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <arxiv:journal_ref>AIDH (Australian Institute of Digital Health) 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Yassien Shaalan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexander Dokumentov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Piyapong Khumrin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Krit Khwanngern</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anawat Wisetborisu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Thanakom Hatsadeang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nattapat Karaket</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Witthawin Achariyaviriya</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sansanee Auephanwiriyakul</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nipon Theera-Umpon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Terence Siganakis</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.11337v1</id>\n",
      "    <title>Single-level Optimization For Differential Architecture Search</title>\n",
      "    <updated>2020-12-15T18:40:33Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.11337v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.11337v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we point out that differential architecture search (DARTS) makes gradient of architecture parameters biased for network weights and architecture parameters are updated in different datasets alternatively in the bi-level optimization framework. The bias causes the architecture parameters of non-learnable operations to surpass that of learnable operations. Moreover, using softmax as architecture parameters' activation function and inappropriate learning rate would exacerbate the bias. As a result, it's frequently observed that non-learnable operations are dominated in the search phase. To reduce the bias, we propose to use single-level to replace bi-level optimization and non-competitive activation function like sigmoid to replace softmax. As a result, we could search high-performance architectures steadily. Experiments on NAS Benchmark 201 validate our hypothesis and stably find out nearly the optimal architecture. On DARTS space, we search the state-of-the-art architecture with 77.0% top1 accuracy (training setting follows PDARTS and without any additional module) on ImageNet-1K and steadily search architectures up-to 76.5% top1 accuracy (but not select the best from the searched architectures) which is comparable with current reported best result.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-15T18:40:33Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Pengfei Hou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ying Jin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.11400v2</id>\n",
      "    <title>Unifying Homophily and Heterophily Network Transformation via Motifs</title>\n",
      "    <updated>2020-12-27T15:36:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.11400v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.11400v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Higher-order proximity (HOP) is fundamental for most network embedding methods due to its significant effects on the quality of node embedding and performance on downstream network analysis tasks. Most existing HOP definitions are based on either homophily to place close and highly interconnected nodes tightly in embedding space or heterophily to place distant but structurally similar nodes together after embedding. In real-world networks, both can co-exist, and thus considering only one could limit the prediction performance and interpretability. However, there is no general and universal solution that takes both into consideration. In this paper, we propose such a simple yet powerful framework called homophily and heterophliy preserving network transformation (H2NT) to capture HOP that flexibly unifies homophily and heterophily. Specifically, H2NT utilises motif representations to transform a network into a new network with a hybrid assumption via micro-level and macro-level walk paths. H2NT can be used as an enhancer to be integrated with any existing network embedding methods without requiring any changes to latter methods. Because H2NT can sparsify networks with motif structures, it can also improve the computational efficiency of existing network embedding methods when integrated. We conduct experiments on node classification, structural role classification and motif prediction to show the superior prediction performance and computational efficiency over state-of-the-art methods. In particular, DeepWalk-based H2 NT achieves 24% improvement in terms of precision on motif prediction, while reducing 46% computational time compared to the original DeepWalk.</summary>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-21T15:03:18Z</published>\n",
      "    <arxiv:comment>10 Pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.SI\"/>\n",
      "    <author>\n",
      "      <name>Yan Ge</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jun Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Li Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Haiping Lu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.10040v1</id>\n",
      "    <title>Robustness to Spurious Correlations in Text Classification via Automatically Generated Counterfactuals</title>\n",
      "    <updated>2020-12-18T03:57:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.10040v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.10040v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Spurious correlations threaten the validity of statistical classifiers. While model accuracy may appear high when the test data is from the same distribution as the training data, it can quickly degrade when the test distribution changes. For example, it has been shown that classifiers perform poorly when humans make minor modifications to change the label of an example. One solution to increase model reliability and generalizability is to identify causal associations between features and classes. In this paper, we propose to train a robust text classifier by augmenting the training data with automatically generated counterfactual data. We first identify likely causal features using a statistical matching approach. Next, we generate counterfactual samples for the original training data by substituting causal features with their antonyms and then assigning opposite labels to the counterfactual samples. Finally, we combine the original data and counterfactual data to train a robust classifier. Experiments on two classification tasks show that a traditional classifier trained on the original data does very poorly on human-generated counterfactual samples (e.g., 10%-37% drop in accuracy). However, the classifier trained on the combined data is more robust and performs well on both the original test data and the counterfactual test data (e.g., 12%-25% increase in accuracy compared with the traditional classifier). Detailed analysis shows that the robust classifier makes meaningful and trustworthy predictions by emphasizing causal features and de-emphasizing non-causal features.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-18T03:57:32Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Zhao Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aron Culotta</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.10056v1</id>\n",
      "    <title>Transfer Learning Based Automatic Model Creation Tool For Resource Constraint Devices</title>\n",
      "    <updated>2020-12-18T05:38:58Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.10056v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.10056v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>With the enhancement of Machine Learning, many tools are being designed to assist developers to easily create their Machine Learning models. In this paper, we propose a novel method for auto creation of such custom models for constraint devices using transfer learning without the need to write any machine learning code. We share the architecture of our automatic model creation tool and the CNN Model created by it using pretrained models such as YAMNet and MobileNetV2 as feature extractors. Finally, we demonstrate accuracy and memory footprint of the model created from the tool by creating an Automatic Image and Audio classifier and report the results of our experiments using Stanford Cars and ESC-50 dataset.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-18T05:38:58Z</published>\n",
      "    <arxiv:comment>7 pages, 10 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Karthik Bhat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Manan Bhandari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>ChangSeok Oh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sujin Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jeeho Yoo</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.12056v1</id>\n",
      "    <title>Data Assimilation in the Latent Space of a Neural Network</title>\n",
      "    <updated>2020-12-22T14:43:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.12056v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.12056v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>There is an urgent need to build models to tackle Indoor Air Quality issue. Since the model should be accurate and fast, Reduced Order Modelling technique is used to reduce the dimensionality of the problem. The accuracy of the model, that represent a dynamic system, is improved integrating real data coming from sensors using Data Assimilation techniques. In this paper, we formulate a new methodology called Latent Assimilation that combines Data Assimilation and Machine Learning. We use a Convolutional neural network to reduce the dimensionality of the problem, a Long-Short-Term-Memory to build a surrogate model of the dynamic system and an Optimal Interpolated Kalman Filter to incorporate real data. Experimental results are provided for CO2 concentration within an indoor space. This methodology can be used for example to predict in real-time the load of virus, such as the SARS-COV-2, in the air by linking it to the concentration of CO2.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-22T14:43:50Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Maddalena Amendola</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rossella Arcucci</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Laetitia Mottet</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cesar Quilodran Casas</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shiwei Fan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christopher Pain</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paul Linden</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yi-Ke Guo</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.12089v1</id>\n",
      "    <title>Prediction of Chronic Kidney Disease Using Deep Neural Network</title>\n",
      "    <updated>2020-12-22T15:31:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.12089v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.12089v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep neural Network (DNN) is becoming a focal point in Machine Learning research. Its application is penetrating into different fields and solving intricate and complex problems. DNN is now been applied in health image processing to detect various ailment such as cancer and diabetes. Another disease that is causing threat to our health is the kidney disease. This disease is becoming prevalent due to substances and elements we intake. Death is imminent and inevitable within few days without at least one functioning kidney. Ignoring the kidney malfunction can cause chronic kidney disease leading to death. Frequently, Chronic Kidney Disease (CKD) and its symptoms are mild and gradual, often go unnoticed for years only to be realized lately. Bade, a Local Government of Yobe state in Nigeria has been a center of attention by medical practitioners due to the prevalence of CKD. Unfortunately, a technical approach in culminating the disease is yet to be attained. We obtained a record of 400 patients with 10 attributes as our dataset from Bade General Hospital. We used DNN model to predict the absence or presence of CKD in the patients. The model produced an accuracy of 98%. Furthermore, we identified and highlighted the Features importance to provide the ranking of the features used in the prediction of the CKD. The outcome revealed that two attributes; Creatinine and Bicarbonate have the highest influence on the CKD prediction.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-22T15:31:14Z</published>\n",
      "    <arxiv:comment>14 paages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Iliyas Ibrahim Iliyas</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Isah Rambo Saidu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ali Baba Dauda</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Suleiman Tasiu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.12123v1</id>\n",
      "    <title>Machine Learning Algorithm for NLOS Millimeter Wave in 5G V2X Communication</title>\n",
      "    <updated>2020-12-16T11:41:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.12123v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.12123v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The 5G vehicle-to-everything (V2X) communication for autonomous and semi-autonomous driving utilizes the wireless technology for communication and the Millimeter Wave bands are widely implemented in this kind of vehicular network application. The main purpose of this paper is to broadcast the messages from the mmWave Base Station to vehicles at LOS (Line-of-sight) and NLOS (Non-LOS). Relay using Machine Learning (RML) algorithm is formulated to train the mmBS for identifying the blockages within its coverage area and broadcast the messages to the vehicles at NLOS using a LOS nodes as a relay. The transmission of information is faster with higher throughput and it covers a wider bandwidth which is reused, therefore when performing machine learning within the coverage area of mmBS most of the vehicles in NLOS can be benefited. A unique method of relay mechanism combined with machine learning is proposed to communicate with mobile nodes at NLOS.</summary>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-16T11:41:09Z</published>\n",
      "    <arxiv:comment>14 pages, 9 figures, conference 7th International conference on Computer Networks and Communications (CCNET 2020), AIRCC Publishing Corporation</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.NI\"/>\n",
      "    <author>\n",
      "      <name>Deepika Mohan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>G. G. Md. Nawaz Ali</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peter Han Joo Chong</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.12137v1</id>\n",
      "    <title>Projected Stochastic Gradient Langevin Algorithms for Constrained Sampling and Non-Convex Learning</title>\n",
      "    <updated>2020-12-22T16:19:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.12137v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.12137v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Langevin algorithms are gradient descent methods with additive noise. They have been used for decades in Markov chain Monte Carlo (MCMC) sampling, optimization, and learning. Their convergence properties for unconstrained non-convex optimization and learning problems have been studied widely in the last few years. Other work has examined projected Langevin algorithms for sampling from log-concave distributions restricted to convex compact sets. For learning and optimization, log-concave distributions correspond to convex losses. In this paper, we analyze the case of non-convex losses with compact convex constraint sets and IID external data variables. We term the resulting method the projected stochastic gradient Langevin algorithm (PSGLA). We show the algorithm achieves a deviation of $O(T^{-1/4}(\\log T)^{1/2})$ from its target distribution in 1-Wasserstein distance. For optimization and learning, we show that the algorithm achieves $ε$-suboptimal solutions, on average, provided that it is run for a time that is polynomial in $ε^{-1}$ and slightly super-exponential in the problem dimension.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.PR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-22T16:19:20Z</published>\n",
      "    <arxiv:comment>45 pages. Under Review for COLT 2021</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Andrew Lamperski</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.12139v1</id>\n",
      "    <title>Image to Bengali Caption Generation Using Deep CNN and Bidirectional Gated Recurrent Unit</title>\n",
      "    <updated>2020-12-22T16:22:02Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.12139v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.12139v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>There is very little notable research on generating descriptions of the Bengali language. About 243 million people speak in Bengali, and it is the 7th most spoken language on the planet. The purpose of this research is to propose a CNN and Bidirectional GRU based architecture model that generates natural language captions in the Bengali language from an image. Bengali people can use this research to break the language barrier and better understand each other's perspectives. It will also help many blind people with their everyday lives. This paper used an encoder-decoder approach to generate captions. We used a pre-trained Deep convolutional neural network (DCNN) called InceptonV3image embedding model as the encoder for analysis, classification, and annotation of the dataset's images Bidirectional Gated Recurrent unit (BGRU) layer as the decoder to generate captions. Argmax and Beam search is used to produce the highest possible quality of the captions. A new dataset called BNATURE is used, which comprises 8000 images with five captions per image. It is used for training and testing the proposed model. We obtained BLEU-1, BLEU-2, BLEU-3, BLEU-4 and Meteor is 42.6, 27.95, 23, 66, 16.41, 28.7 respectively.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-22T16:22:02Z</published>\n",
      "    <arxiv:comment>Accepted at ICCIT2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Al Momin Faruk</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hasan Al Faraby</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Md. Muzahidul Azad</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Md. Riduyan Fedous</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Md. Kishor Morol</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.12142v1</id>\n",
      "    <title>High-Speed Robot Navigation using Predicted Occupancy Maps</title>\n",
      "    <updated>2020-12-22T16:25:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.12142v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.12142v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Safe and high-speed navigation is a key enabling capability for real world deployment of robotic systems. A significant limitation of existing approaches is the computational bottleneck associated with explicit mapping and the limited field of view (FOV) of existing sensor technologies. In this paper, we study algorithmic approaches that allow the robot to predict spaces extending beyond the sensor horizon for robust planning at high speeds. We accomplish this using a generative neural network trained from real-world data without requiring human annotated labels. Further, we extend our existing control algorithms to support leveraging the predicted spaces to improve collision-free planning and navigation at high speeds. Our experiments are conducted on a physical robot based on the MIT race car using an RGBD sensor where were able to demonstrate improved performance at 4 m/s compared to a controller not operating on predicted regions of the map.</summary>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-22T16:25:12Z</published>\n",
      "    <arxiv:primary_category term=\"cs.RO\"/>\n",
      "    <author>\n",
      "      <name>Kapil D. Katyal</name>\n",
      "      <arxiv:affiliation>Johns Hopkins University Applied Physics Lab</arxiv:affiliation>\n",
      "      <arxiv:affiliation>Dept. of Comp. Sci., Johns Hopkins University</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Adam Polevoy</name>\n",
      "      <arxiv:affiliation>Johns Hopkins University Applied Physics Lab</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joseph Moore</name>\n",
      "      <arxiv:affiliation>Johns Hopkins University Applied Physics Lab</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Craig Knuth</name>\n",
      "      <arxiv:affiliation>Johns Hopkins University Applied Physics Lab</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Katie M. Popek</name>\n",
      "      <arxiv:affiliation>Johns Hopkins University Applied Physics Lab</arxiv:affiliation>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.13028v1</id>\n",
      "    <title>General Domain Adaptation Through Proportional Progressive Pseudo Labeling</title>\n",
      "    <updated>2020-12-23T23:57:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.13028v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.13028v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Domain adaptation helps transfer the knowledge gained from a labeled source domain to an unlabeled target domain. During the past few years, different domain adaptation techniques have been published. One common flaw of these approaches is that while they might work well on one input type, such as images, their performance drops when applied to others, such as text or time-series. In this paper, we introduce Proportional Progressive Pseudo Labeling (PPPL), a simple, yet effective technique that can be implemented in a few lines of code to build a more general domain adaptation technique that can be applied on several different input types. At the beginning of the training phase, PPPL progressively reduces target domain classification error, by training the model directly with pseudo-labeled target domain samples, while excluding samples with more likely wrong pseudo-labels from the training set and also postponing training on such samples. Experiments on 6 different datasets that include tasks such as anomaly detection, text sentiment analysis and image classification demonstrate that PPPL can beat other baselines and generalize better.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-23T23:57:00Z</published>\n",
      "    <arxiv:comment>Published at 2020 IEEE International Conference on Big Data (Big Data)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Mohammad J. Hashemi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eric Keller</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.13096v1</id>\n",
      "    <title>Wheel-Rail Interface Condition Estimation (W-RICE)</title>\n",
      "    <updated>2020-12-24T04:40:27Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.13096v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.13096v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The surface roughness between the wheel and rail has a huge influence on rolling noise level. The presence of the third body such as frost or grease at wheel-rail interface contributes towards change in adhesion coefficient resulting in the generation of noise at various levels. Therefore, it is possible to estimate adhesion conditions between the wheel and rail from the analysis of noise patterns originating from wheel-rail interaction. In this study, a new approach to estimate adhesion condition is proposed which takes rolling noise as input.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-24T04:40:27Z</published>\n",
      "    <arxiv:comment>8 pages, 3 figures, JRC2020 conference</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Sundar Shrestha</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anand Koirala</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maksym Spiryagin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qing Wu</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1115/JRC2020-8037</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1115/JRC2020-8037\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.13115v1</id>\n",
      "    <title>Upper Confidence Bounds for Combining Stochastic Bandits</title>\n",
      "    <updated>2020-12-24T05:36:29Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.13115v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.13115v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We provide a simple method to combine stochastic bandit algorithms. Our approach is based on a \"meta-UCB\" procedure that treats each of $N$ individual bandit algorithms as arms in a higher-level $N$-armed bandit problem that we solve with a variant of the classic UCB algorithm. Our final regret depends only on the regret of the base algorithm with the best regret in hindsight. This approach provides an easy and intuitive alternative strategy to the CORRAL algorithm for adversarial bandits, without requiring the stability conditions imposed by CORRAL on the base algorithms. Our results match lower bounds in several settings, and we provide empirical validation of our algorithm on misspecified linear bandit and model selection problems.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-24T05:36:29Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Ashok Cutkosky</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abhimanyu Das</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Manish Purohit</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.12308v1</id>\n",
      "    <title>Randomized RX for target detection</title>\n",
      "    <updated>2020-12-08T19:18:49Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.12308v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.12308v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This work tackles the target detection problem through the well-known global RX method. The RX method models the clutter as a multivariate Gaussian distribution, and has been extended to nonlinear distributions using kernel methods. While the kernel RX can cope with complex clutters, it requires a considerable amount of computational resources as the number of clutter pixels gets larger. Here we propose random Fourier features to approximate the Gaussian kernel in kernel RX and consequently our development keep the accuracy of the nonlinearity while reducing the computational cost which is now controlled by an hyperparameter. Results over both synthetic and real-world image target detection problems show space and time efficiency of the proposed method while providing high detection performance.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-08T19:18:49Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Fatih Nar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Adrián Pérez-Suay</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>José Antonio Padrón</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gustau Camps-Valls</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.12373v1</id>\n",
      "    <title>The Life and Death of SSDs and HDDs: Similarities, Differences, and Prediction Models</title>\n",
      "    <updated>2020-12-22T21:50:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.12373v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.12373v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Data center downtime typically centers around IT equipment failure. Storage devices are the most frequently failing components in data centers. We present a comparative study of hard disk drives (HDDs) and solid state drives (SSDs) that constitute the typical storage in data centers. Using a six-year field data of 100,000 HDDs of different models from the same manufacturer from the BackBlaze dataset and a six-year field data of 30,000 SSDs of three models from a Google data center, we characterize the workload conditions that lead to failures and illustrate that their root causes differ from common expectation but remain difficult to discern. For the case of HDDs we observe that young and old drives do not present many differences in their failures. Instead, failures may be distinguished by discriminating drives based on the time spent for head positioning. For SSDs, we observe high levels of infant mortality and characterize the differences between infant and non-infant failures. We develop several machine learning failure prediction models that are shown to be surprisingly accurate, achieving high recall and low false positive rates. These models are used beyond simple prediction as they aid us to untangle the complex interaction of workload characteristics that lead to failures and identify failure root causes from monitored symptoms.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-22T21:50:32Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Riccardo Pinciroli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lishan Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jacob Alter</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Evgenia Smirni</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.13391v2</id>\n",
      "    <title>I like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling</title>\n",
      "    <updated>2020-12-28T18:32:21Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.13391v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.13391v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>To quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues. We then compare a structured utterance-based approach of using pre-trained Transformer models for contradiction detection with the typical unstructured approach. Results reveal that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain; (ii) the structured utterance-based approach is more robust and transferable on both analysis and out-of-distribution dialogues than its unstructured counterpart. We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-24T18:47:49Z</published>\n",
      "    <arxiv:comment>15 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Yixin Nie</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mary Williamson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mohit Bansal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Douwe Kiela</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jason Weston</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.11066v2</id>\n",
      "    <title>Fairness, Welfare, and Equity in Personalized Pricing</title>\n",
      "    <updated>2020-12-27T17:21:29Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.11066v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.11066v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study the interplay of fairness, welfare, and equity considerations in personalized pricing based on customer features. Sellers are increasingly able to conduct price personalization based on predictive modeling of demand conditional on covariates: setting customized interest rates, targeted discounts of consumer goods, and personalized subsidies of scarce resources with positive externalities like vaccines and bed nets. These different application areas may lead to different concerns around fairness, welfare, and equity on different objectives: price burdens on consumers, price envy, firm revenue, access to a good, equal access, and distributional consequences when the good in question further impacts downstream outcomes of interest. We conduct a comprehensive literature review in order to disentangle these different normative considerations and propose a taxonomy of different objectives with mathematical definitions. We focus on observational metrics that do not assume access to an underlying valuation distribution which is either unobserved due to binary feedback or ill-defined due to overriding behavioral concerns regarding interpreting revealed preferences. In the setting of personalized pricing for the provision of goods with positive benefits, we discuss how price optimization may provide unambiguous benefit by achieving a \"triple bottom line\": personalized pricing enables expanding access, which in turn may lead to gains in welfare due to heterogeneous utility, and improve revenue or budget utilization. We empirically demonstrate the potential benefits of personalized pricing in two settings: pricing subsidies for an elective vaccine, and the effects of personalized interest rates on downstream outcomes in microcredit.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-21T01:01:56Z</published>\n",
      "    <arxiv:comment>Accepted at FAccT 2021</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Nathan Kallus</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Angela Zhou</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.11147v1</id>\n",
      "    <title>Hop-Hop Relation-aware Graph Neural Networks</title>\n",
      "    <updated>2020-12-21T06:58:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.11147v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.11147v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Graph Neural Networks (GNNs) are widely used in graph representation learning. However, most GNN methods are designed for either homogeneous or heterogeneous graphs. In this paper, we propose a new model, Hop-Hop Relation-aware Graph Neural Network (HHR-GNN), to unify representation learning for these two types of graphs. HHR-GNN learns a personalized receptive field for each node by leveraging knowledge graph embedding to learn relation scores between the central node's representations at different hops. In neighborhood aggregation, our model simultaneously allows for hop-aware projection and aggregation. This mechanism enables the central node to learn a hop-wise neighborhood mixing that can be applied to both homogeneous and heterogeneous graphs. Experimental results on five benchmarks show the competitive performance of our model compared to state-of-the-art GNNs, e.g., up to 13K faster in terms of time cost per training epoch on large heterogeneous graphs.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-21T06:58:38Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Li Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yan Ge</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Haiping Lu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.11195v1</id>\n",
      "    <title>Personalized fall detection monitoring system based on learning from the user movements</title>\n",
      "    <updated>2020-12-21T09:19:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.11195v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.11195v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Personalized fall detection system is shown to provide added and more benefits compare to the current fall detection system. The personalized model can also be applied to anything where one class of data is hard to gather. The results show that adapting to the user needs, improve the overall accuracy of the system. Future work includes detection of the smartphone on the user so that the user can place the system anywhere on the body and make sure it detects. Even though the accuracy is not 100% the proof of concept of personalization can be used to achieve greater accuracy. The concept of personalization used in this paper can also be extended to other research in the medical field or where data is hard to come by for a particular class. More research into the feature extraction and feature selection module should be investigated. For the feature selection module, more research into selecting features based on one class data.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-21T09:19:12Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Pranesh Vallabh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nazanin Malekian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Reza Malekian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ting-Mei Li</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.11197v1</id>\n",
      "    <title>Neural Joint Entropy Estimation</title>\n",
      "    <updated>2020-12-21T09:23:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.11197v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.11197v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Estimating the entropy of a discrete random variable is a fundamental problem in information theory and related fields. This problem has many applications in various domains, including machine learning, statistics and data compression. Over the years, a variety of estimation schemes have been suggested. However, despite significant progress, most methods still struggle when the sample is small, compared to the variable's alphabet size. In this work, we introduce a practical solution to this problem, which extends the work of McAllester and Statos (2020). The proposed scheme uses the generalization abilities of cross-entropy estimation in deep neural networks (DNNs) to introduce improved entropy estimation accuracy. Furthermore, we introduce a family of estimators for related information-theoretic measures, such as conditional entropy and mutual information. We show that these estimators are strongly consistent and demonstrate their performance in a variety of use-cases. First, we consider large alphabet entropy estimation. Then, we extend the scope to mutual information estimation. Next, we apply the proposed scheme to conditional mutual information estimation, as we focus on independence testing tasks. Finally, we study a transfer entropy estimation problem. The proposed estimators demonstrate improved performance compared to existing methods in all tested setups.</summary>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-21T09:23:39Z</published>\n",
      "    <arxiv:primary_category term=\"cs.IT\"/>\n",
      "    <author>\n",
      "      <name>Yuval Shalev</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amichai Painsky</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Irad Ben-Gal</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.12718v1</id>\n",
      "    <title>Compliance Generation for Privacy Documents under GDPR: A Roadmap for Implementing Automation and Machine Learning</title>\n",
      "    <updated>2020-12-23T14:46:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.12718v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.12718v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Most prominent research today addresses compliance with data protection laws through consumer-centric and public-regulatory approaches. We shift this perspective with the Privatech project to focus on corporations and law firms as agents of compliance. To comply with data protection laws, data processors must implement accountability measures to assess and document compliance in relation to both privacy documents and privacy practices. In this paper, we survey, on the one hand, current research on GDPR automation, and on the other hand, the operational challenges corporations face to comply with GDPR, and that may benefit from new forms of automation. We attempt to bridge the gap. We provide a roadmap for compliance assessment and generation by identifying compliance issues, breaking them down into tasks that can be addressed through machine learning and automation, and providing notes about related developments in the Privatech project.</summary>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-23T14:46:51Z</published>\n",
      "    <arxiv:comment>14 pages, The paper was presented at GDPR Compliance Theories, Techniques, Tools a Workshop of JURIX 2019. Universidad Politécnica de Madrid, Madrid, Spain, 11 December 2019</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.AI\"/>\n",
      "    <author>\n",
      "      <name>David Restrepo Amariles</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aurore Clément Troussel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rajaa El Hamdani</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.12725v1</id>\n",
      "    <title>Learning-based Prediction and Uplink Retransmission for Wireless Virtual Reality (VR) Network</title>\n",
      "    <updated>2020-12-16T18:31:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.12725v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.12725v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Wireless Virtual Reality (VR) users are able to enjoy immersive experience from anywhere at anytime. However, providing full spherical VR video with high quality under limited VR interaction latency is challenging. If the viewpoint of the VR user can be predicted in advance, only the required viewpoint is needed to be rendered and delivered, which can reduce the VR interaction latency. Therefore, in this paper, we use offline and online learning algorithms to predict viewpoint of the VR user using real VR dataset. For the offline learning algorithm, the trained learning model is directly used to predict the viewpoint of VR users in continuous time slots. While for the online learning algorithm, based on the VR user's actual viewpoint delivered through uplink transmission, we compare it with the predicted viewpoint and update the parameters of the online learning algorithm to further improve the prediction accuracy. To guarantee the reliability of the uplink transmission, we integrate the Proactive retransmission scheme into our proposed online learning algorithm. Simulation results show that our proposed online learning algorithm for uplink wireless VR network with the proactive retransmission scheme only exhibits about 5% prediction error.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-16T18:31:05Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Xiaonan Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xinyu Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yansha Deng</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.12775v1</id>\n",
      "    <title>Adaptive Precision Training for Resource Constrained Devices</title>\n",
      "    <updated>2020-12-23T16:28:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.12775v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.12775v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Learn in-situ is a growing trend for Edge AI. Training deep neural network (DNN) on edge devices is challenging because both energy and memory are constrained. Low precision training helps to reduce the energy cost of a single training iteration, but that does not necessarily translate to energy savings for the whole training process, because low precision could slows down the convergence rate. One evidence is that most works for low precision training keep an fp32 copy of the model during training, which in turn imposes memory requirements on edge devices. In this work we propose Adaptive Precision Training. It is able to save both total training energy cost and memory usage at the same time. We use model of the same precision for both forward and backward pass in order to reduce memory usage for training. Through evaluating the progress of training, APT allocates layer-wise precision dynamically so that the model learns quicker for longer time. APT provides an application specific hyper-parameter for users to play trade-off between training energy cost, memory usage and accuracy. Experiment shows that APT achieves more than 50% saving on training energy and memory usage with limited accuracy loss. 20% more savings of training energy and memory usage can be achieved in return for a 1% sacrifice in accuracy loss.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-23T16:28:41Z</published>\n",
      "    <arxiv:comment>6 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Tian Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tao Luo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joey Tianyi Zhou</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.14106v1</id>\n",
      "    <title>Diagnosis/Prognosis of COVID-19 Images: Challenges, Opportunities, and Applications</title>\n",
      "    <updated>2020-12-28T05:38:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.14106v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.14106v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The novel Coronavirus disease, COVID-19, has rapidly and abruptly changed the world as we knew in 2020. It becomes the most unprecedent challenge to analytic epidemiology in general and signal processing theories in specific. Given its high contingency nature and adverse effects across the world, it is important to develop efficient processing/learning models to overcome this pandemic and be prepared for potential future ones. In this regard, medical imaging plays an important role for the management of COVID-19. Human-centered interpretation of medical images is, however, tedious and can be subjective. This has resulted in a surge of interest to develop Radiomics models for analysis and interpretation of medical images. Signal Processing (SP) and Deep Learning (DL) models can assist in development of robust Radiomics solutions for diagnosis/prognosis, severity assessment, treatment response, and monitoring of COVID-19 patients. In this article, we aim to present an overview of the current state, challenges, and opportunities of developing SP/DL-empowered models for diagnosis (screening/monitoring) and prognosis (outcome prediction and severity assessment) of COVID-19 infection. More specifically, the article starts by elaborating the latest development on the theoretical framework of analytic epidemiology and hypersignal processing for COVID-19. Afterwards, imaging modalities and Radiological characteristics of COVID-19 are discussed. SL/DL-based Radiomic models specific to the analysis of COVID-19 infection are then described covering the following four domains: Segmentation of COVID-19 lesions; Predictive models for outcome prediction; Severity assessment, and; Diagnosis/classification models. Finally, open problems and opportunities are presented in detail.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-28T05:38:44Z</published>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Arash Mohammadi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yingxu Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nastaran Enshaei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Parnian Afshar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Farnoosh Naderkhani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anastasia Oikonomou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Moezedin Javad Rafiee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Helder C. R. Oliveira</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Svetlana Yanushkevich</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Konstantinos N. Plataniotis</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.14285v1</id>\n",
      "    <title>Affirmative Algorithms: The Legal Grounds for Fairness as Awareness</title>\n",
      "    <updated>2020-12-18T22:53:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.14285v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.14285v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>While there has been a flurry of research in algorithmic fairness, what is less recognized is that modern antidiscrimination law may prohibit the adoption of such techniques. We make three contributions. First, we discuss how such approaches will likely be deemed \"algorithmic affirmative action,\" posing serious legal risks of violating equal protection, particularly under the higher education jurisprudence. Such cases have increasingly turned toward anticlassification, demanding \"individualized consideration\" and barring formal, quantitative weights for race regardless of purpose. This case law is hence fundamentally incompatible with fairness in machine learning. Second, we argue that the government-contracting cases offer an alternative grounding for algorithmic fairness, as these cases permit explicit and quantitative race-based remedies based on historical discrimination by the actor. Third, while limited, this doctrinal approach also guides the future of algorithmic fairness, mandating that adjustments be calibrated to the entity's responsibility for historical discrimination causing present-day disparities. The contractor cases provide a legally viable path for algorithmic fairness under current constitutional doctrine but call for more research at the intersection of algorithmic fairness and causal inference to ensure that bias mitigation is tailored to specific causes and mechanisms of bias.</summary>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-18T22:53:20Z</published>\n",
      "    <arxiv:comment>12 pages, 3 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CY\"/>\n",
      "    <arxiv:journal_ref>10/30/20 U. Chi. L. Rev. Online 143, https://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang/</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Daniel E. Ho</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alice Xiang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.14338v1</id>\n",
      "    <title>Low-Cost Maximum Entropy Covariance Matrix Reconstruction Algorithm for Robust Adaptive Beamforming</title>\n",
      "    <updated>2020-12-28T16:26:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.14338v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.14338v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this letter, we present a novel low-complexity adaptive beamforming technique using a stochastic gradient algorithm to avoid matrix inversions. The proposed method exploits algorithms based on the maximum entropy power spectrum (MEPS) to estimate the noise-plus-interference covariance matrix (MEPS-NPIC) so that the beamforming weights are updated adaptively, thus greatly reducing the computational complexity. MEPS is further used to reconstruct the desired signal covariance matrix and to improve the estimate of the desired signals's steering vector (SV). Simulations show the superiority of the proposed MEPS-NPIC approach over previously proposed beamformers.</summary>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-28T16:26:55Z</published>\n",
      "    <arxiv:comment>6 pages, 4 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IT\"/>\n",
      "    <author>\n",
      "      <name>S. Mohammadzadeh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>V. H. Nascimento</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>R. C. de Lamare</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.13677v1</id>\n",
      "    <title>Toward Compact Data from Big Data</title>\n",
      "    <updated>2020-12-26T04:45:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.13677v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.13677v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Bigdata is a dataset of which size is beyond the ability of handling a valuable raw material that can be refined and distilled into valuable specific insights. Compact data is a method that optimizes the big dataset that gives best assets without handling complex bigdata. The compact dataset contains the maximum knowledge patterns at fine grained level for effective and personalized utilization of bigdata systems without bigdata. The compact data method is a tailor-made design which depends on problem situations. Various compact data techniques have been demonstrated into various data-driven research area in the paper.</summary>\n",
      "    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-26T04:45:40Z</published>\n",
      "    <arxiv:comment>This paper has been accepted in the 2020 IEEE-ICITIS Conference</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.DB\"/>\n",
      "    <author>\n",
      "      <name> Song-Kyoo</name>\n",
      "      <arxiv:affiliation>Amang</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name> Kim</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.13690v1</id>\n",
      "    <title>One-Shot Object Localization Using Learnt Visual Cues via Siamese Networks</title>\n",
      "    <updated>2020-12-26T07:40:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.13690v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.13690v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A robot that can operate in novel and unstructured environments must be capable of recognizing new, previously unseen, objects. In this work, a visual cue is used to specify a novel object of interest which must be localized in new environments. An end-to-end neural network equipped with a Siamese network is used to learn the cue, infer the object of interest, and then to localize it in new environments. We show that a simulated robot can pick-and-place novel objects pointed to by a laser pointer. We also evaluate the performance of the proposed approach on a dataset derived from the Omniglot handwritten character dataset and on a small dataset of toys.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-26T07:40:00Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Sagar Gubbi Venkatesh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bharadwaj Amrutur</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/IROS40897.2019.8967881</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/IROS40897.2019.8967881\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.14005v1</id>\n",
      "    <title>Neural document expansion for ad-hoc information retrieval</title>\n",
      "    <updated>2020-12-27T20:00:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.14005v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.14005v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recently, Nogueira et al. [2019] proposed a new approach to document expansion based on a neural Seq2Seq model, showing significant improvement on short text retrieval task. However, this approach needs a large amount of in-domain training data. In this paper, we show that this neural document expansion approach can be effectively adapted to standard IR tasks, where labels are scarce and many long documents are present.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-27T20:00:08Z</published>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Cheng Tang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrew Arnold</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.14043v1</id>\n",
      "    <title>Blackwell Online Learning for Markov Decision Processes</title>\n",
      "    <updated>2020-12-28T00:40:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.14043v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.14043v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This work provides a novel interpretation of Markov Decision Processes (MDP) from the online optimization viewpoint. In such an online optimization context, the policy of the MDP is viewed as the decision variable while the corresponding value function is treated as payoff feedback from the environment. Based on this interpretation, we construct a Blackwell game induced by MDP, which bridges the gap among regret minimization, Blackwell approachability theory, and learning theory for MDP. Specifically, from the approachability theory, we propose 1) Blackwell value iteration for offline planning and 2) Blackwell $Q-$learning for online learning in MDP, both of which are shown to converge to the optimal solution. Our theoretical guarantees are corroborated by numerical experiments.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-28T00:40:01Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Tao Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Guanze Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Quanyan Zhu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2012.14065v1</id>\n",
      "    <title>Deep Learning with Heterogeneous Graph Embeddings for Mortality Prediction from Electronic Health Records</title>\n",
      "    <updated>2020-12-28T02:27:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2012.14065v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2012.14065v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Computational prediction of in-hospital mortality in the setting of an intensive care unit can help clinical practitioners to guide care and make early decisions for interventions. As clinical data are complex and varied in their structure and components, continued innovation of modeling strategies is required to identify architectures that can best model outcomes. In this work, we train a Heterogeneous Graph Model (HGM) on Electronic Health Record data and use the resulting embedding vector as additional information added to a Convolutional Neural Network (CNN) model for predicting in-hospital mortality. We show that the additional information provided by including time as a vector in the embedding captures the relationships between medical concepts, lab tests, and diagnoses, which enhances predictive performance. We find that adding HGM to a CNN model increases the mortality prediction accuracy up to 4\\%. This framework serves as a foundation for future experiments involving different EHR data types on important healthcare prediction tasks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-12-28T02:27:09Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Tingyi Wanyan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hossein Honarvar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ariful Azad</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ying Ding</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Benjamin S. Glicksberg</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.07916v1</id>\n",
      "    <title>IAN: Combining Generative Adversarial Networks for Imaginative Face Generation</title>\n",
      "    <updated>2019-04-16T18:45:33Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.07916v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.07916v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Generative Adversarial Networks (GANs) have gained momentum for their ability to model image distributions. They learn to emulate the training set and that enables sampling from that domain and using the knowledge learned for useful applications. Several methods proposed enhancing GANs, including regularizing the loss with some feature matching. We seek to push GANs beyond the data in the training and try to explore unseen territory in the image manifold. We first propose a new regularizer for GAN based on K-nearest neighbor (K-NN) selective feature matching to a target set Y in high-level feature space, during the adversarial training of GAN on the base set X, and we call this novel model K-GAN. We show that minimizing the added term follows from cross-entropy minimization between the distributions of GAN and the set Y. Then, We introduce a cascaded framework for GANs that try to address the task of imagining a new distribution that combines the base set X and target set Y by cascading sampling GANs with translation GANs, and we dub the cascade of such GANs as the Imaginative Adversarial Network (IAN). We conduct an objective and subjective evaluation for different IAN setups in the addressed task and show some useful applications for these IANs, like manifold traversing and creative face generation for characters' design in movies or video games.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-16T18:45:33Z</published>\n",
      "    <arxiv:comment>preprint</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Abdullah Hamdi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bernard Ghanem</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.07964v1</id>\n",
      "    <title>3D Shape Synthesis for Conceptual Design and Optimization Using Variational Autoencoders</title>\n",
      "    <updated>2019-04-16T20:26:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.07964v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.07964v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose a data-driven 3D shape design method that can learn a generative model from a corpus of existing designs, and use this model to produce a wide range of new designs. The approach learns an encoding of the samples in the training corpus using an unsupervised variational autoencoder-decoder architecture, without the need for an explicit parametric representation of the original designs. To facilitate the generation of smooth final surfaces, we develop a 3D shape representation based on a distance transformation of the original 3D data, rather than using the commonly utilized binary voxel representation. Once established, the generator maps the latent space representations to the high-dimensional distance transformation fields, which are then automatically surfaced to produce 3D representations amenable to physics simulations or other objective function evaluation modules. We demonstrate our approach for the computational design of gliders that are optimized to attain prescribed performance scores. Our results show that when combined with genetic optimization, the proposed approach can generate a rich set of candidate concept designs that achieve prescribed functional goals, even when the original dataset has only a few or no solutions that achieve these goals.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-16T20:26:53Z</published>\n",
      "    <arxiv:comment>Preprint accepted by ASME IDETC/CIE 2019</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Wentai Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhangsihao Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Haoliang Jiang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Suyash Nigam</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Soji Yamakawa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tomotake Furuhata</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kenji Shimada</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Levent Burak Kara</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.07969v1</id>\n",
      "    <title>DNN Architecture for High Performance Prediction on Natural Videos Loses Submodule's Ability to Learn Discrete-World Dataset</title>\n",
      "    <updated>2019-04-16T20:35:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.07969v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.07969v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Is cognition a collection of loosely connected functions tuned to different tasks, or can there be a general learning algorithm? If such an hypothetical general algorithm did exist, tuned to our world, could it adapt seamlessly to a world with different laws of nature? We consider the theory that predictive coding is such a general rule, and falsify it for one specific neural architecture known for high-performance predictions on natural videos and replication of human visual illusions: PredNet. Our results show that PredNet's high performance generalizes without retraining on a completely different natural video dataset. Yet PredNet cannot be trained to reach even mediocre accuracy on an artificial video dataset created with the rules of the Game of Life (GoL). We also find that a submodule of PredNet, a Convolutional Neural Network trained alone, reaches perfect accuracy on the GoL while being mediocre for natural videos, showing that PredNet's architecture itself is responsible for both the high performance on natural videos and the loss of performance on the GoL. Just as humans cannot predict the dynamics of the GoL, our results suggest that there might be a trade-off between high performance on sensory inputs with different sets of rules.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-16T20:35:09Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Lana Sinapayen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Atsushi Noda</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.07998v2</id>\n",
      "    <title>SynC: A Unified Framework for Generating Synthetic Population with Gaussian Copula</title>\n",
      "    <updated>2019-11-11T01:48:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.07998v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.07998v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Synthetic population generation is the process of combining multiple socioeconomic and demographic datasets from different sources and/or granularity levels, and downscaling them to an individual level. Although it is a fundamental step for many data science tasks, an efficient and standard framework is absent. In this study, we propose a multi-stage framework called SynC (Synthetic Population via Gaussian Copula) to fill the gap. SynC first removes potential outliers in the data and then fits the filtered data with a Gaussian copula model to correctly capture dependencies and marginal distributions of sampled survey data. Finally, SynC leverages predictive models to merge datasets into one and then scales them accordingly to match the marginal constraints. We make three key contributions in this work: 1) propose a novel framework for generating individual level data from aggregated data sources by combining state-of-the-art machine learning and statistical techniques, 2) demonstrate its value as a feature engineering tool, as well as an alternative to data collection in situations where gathering is difficult through two real-world datasets, 3) release an easy-to-use framework implementation for reproducibility, and 4) ensure the methodology is scalable at the production level and can easily incorporate new data.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-16T22:10:19Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Colin Wan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zheng Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alicia Guo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yue Zhao</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.08034v2</id>\n",
      "    <title>People infer recursive visual concepts from just a few examples</title>\n",
      "    <updated>2019-07-29T15:26:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.08034v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.08034v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Machine learning has made major advances in categorizing objects in images, yet the best algorithms miss important aspects of how people learn and think about categories. People can learn richer concepts from fewer examples, including causal models that explain how members of a category are formed. Here, we explore the limits of this human ability to infer causal \"programs\" -- latent generating processes with nontrivial algorithmic properties -- from one, two, or three visual examples. People were asked to extrapolate the programs in several ways, for both classifying and generating new examples. As a theory of these inductive abilities, we present a Bayesian program learning model that searches the space of programs for the best explanation of the observations. Although variable, people's judgments are broadly consistent with the model and inconsistent with several alternatives, including a pre-trained deep neural network for object recognition, indicating that people can learn and reason with rich algorithmic abstractions from sparse input data.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-17T00:45:05Z</published>\n",
      "    <arxiv:comment>In press at \"Computational Brain &amp; Behavior\"</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Brenden M. Lake</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Steven T. Piantadosi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.08049v1</id>\n",
      "    <title>Neural Message Passing for Multi-Label Classification</title>\n",
      "    <updated>2019-04-17T01:58:17Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.08049v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.08049v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Multi-label classification (MLC) is the task of assigning a set of target labels for a given sample. Modeling the combinatorial label interactions in MLC has been a long-haul challenge. We propose Label Message Passing (LaMP) Neural Networks to efficiently model the joint prediction of multiple labels. LaMP treats labels as nodes on a label-interaction graph and computes the hidden representation of each label node conditioned on the input using attention-based neural message passing. Attention enables LaMP to assign different importance to neighbor nodes per label, learning how labels interact (implicitly). The proposed models are simple, accurate, interpretable, structure-agnostic, and applicable for predicting dense labels since LaMP is incredibly parallelizable. We validate the benefits of LaMP on seven real-world MLC datasets, covering a broad spectrum of input/output types and outperforming the state-of-the-art results. Notably, LaMP enables intuitive interpretation of how classifying each label depends on the elements of a sample and at the same time rely on its interaction with other labels. We provide our code and datasets at https://github.com/QData/LaMP</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-17T01:58:17Z</published>\n",
      "    <arxiv:comment>19pages. We provide our code and datasets at https://github.com/QData/LaMP</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jack Lanchantin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arshdeep Sekhon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yanjun Qi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.08050v1</id>\n",
      "    <title>Sparseout: Controlling Sparsity in Deep Networks</title>\n",
      "    <updated>2019-04-17T02:10:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.08050v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.08050v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Dropout is commonly used to help reduce overfitting in deep neural networks. Sparsity is a potentially important property of neural networks, but is not explicitly controlled by Dropout-based regularization. In this work, we propose Sparseout a simple and efficient variant of Dropout that can be used to control the sparsity of the activations in a neural network. We theoretically prove that Sparseout is equivalent to an $L_q$ penalty on the features of a generalized linear model and that Dropout is a special case of Sparseout for neural networks. We empirically demonstrate that Sparseout is computationally inexpensive and is able to control the desired level of sparsity in the activations. We evaluated Sparseout on image classification and language modelling tasks to see the effect of sparsity on these tasks. We found that sparsity of the activations is favorable for language modelling performance while image classification benefits from denser activations. Sparseout provides a way to investigate sparsity in state-of-the-art deep learning models. Source code for Sparseout could be found at \\url{https://github.com/najeebkhan/sparseout}.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-17T02:10:25Z</published>\n",
      "    <arxiv:comment>Code: https://github.com/najeebkhan/sparseout</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Najeeb Khan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ian Stavness</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1007/978-3-030-18305-9_24</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1007/978-3-030-18305-9_24\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.08051v1</id>\n",
      "    <title>Posterior-regularized REINFORCE for Instance Selection in Distant Supervision</title>\n",
      "    <updated>2019-04-17T02:21:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.08051v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.08051v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper provides a new way to improve the efficiency of the REINFORCE training process. We apply it to the task of instance selection in distant supervision. Modeling the instance selection in one bag as a sequential decision process, a reinforcement learning agent is trained to determine whether an instance is valuable or not and construct a new bag with less noisy instances. However unbiased methods, such as REINFORCE, could usually take much time to train. This paper adopts posterior regularization (PR) to integrate some domain-specific rules in instance selection using REINFORCE. As the experiment results show, this method remarkably improves the performance of the relation classifier trained on cleaned distant supervision dataset as well as the efficiency of the REINFORCE training.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-17T02:21:51Z</published>\n",
      "    <arxiv:comment>Five pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <arxiv:journal_ref>naacl 2019</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Qi Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Siliang Tang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiang Ren</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fei Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shiliang Pu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yueting Zhuang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.00261v1</id>\n",
      "    <title>Automatic Dataset Augmentation Using Virtual Human Simulation</title>\n",
      "    <updated>2019-05-01T11:01:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.00261v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.00261v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Virtual Human Simulation has been widely used for different purposes, such as comfort or accessibility analysis. In this paper, we investigate the possibility of using this type of technique to extend the training datasets of pedestrians to be used with machine learning techniques. Our main goal is to verify if Computer Graphics (CG) images of virtual humans with a simplistic rendering can be efficient in order to augment datasets used for training machine learning methods. In fact, from a machine learning point of view, there is a need to collect and label large datasets for ground truth, which sometimes demands manual annotation. In addition, find out images and videos with real people and also provide ground truth of people detection and counting is not trivial. If CG images, which can have a ground truth automatically generated, can also be used as training in machine learning techniques for pedestrian detection and counting, it can certainly facilitate and optimize the whole process of event detection. In particular, we propose to parametrize virtual humans using a data-driven approach. Results demonstrated that using the extended datasets with CG images outperforms the results when compared to only real images sequences.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-01T11:01:39Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Marcelo C. Ghilardi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Leandro Dihl</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Estevão Testa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pedro Braga</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>João P. Pianta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Isabel H. Manssour</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Soraia R. Musse</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.02796v1</id>\n",
      "    <title>Collaborative and Privacy-Preserving Machine Teaching via Consensus Optimization</title>\n",
      "    <updated>2019-05-07T20:15:31Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.02796v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.02796v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this work, we define a collaborative and privacy-preserving machine teaching paradigm with multiple distributed teachers. We focus on consensus super teaching. It aims at organizing distributed teachers to jointly select a compact while informative training subset from data hosted by the teachers to make a learner learn better. The challenges arise from three perspectives. First, the state-of-the-art pool-based super teaching method applies mixed-integer non-linear programming (MINLP) which does not scale well to very large data sets. Second, it is desirable to restrict data access of the teachers to only their own data during the collaboration stage to mitigate privacy leaks. Finally, the teaching collaboration should be communication-efficient since large communication overheads can cause synchronization delays between teachers.\n",
      "  To address these challenges, we formulate collaborative teaching as a consensus and privacy-preserving optimization process to minimize teaching risk. We theoretically demonstrate the necessity of collaboration between teachers for improving the learner's learning. Furthermore, we show that the proposed method enjoys a similar property as the Oracle property of adaptive Lasso. The empirical study illustrates that our teaching method can deliver significantly more accurate teaching results with high speed, while the non-collaborative MINLP-based super teaching becomes prohibitively expensive to compute.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-07T20:15:31Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>IJCNN 2019</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Yufei Han</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuzhe Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christopher Gates</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kevin Roundy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yun Shen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.02845v1</id>\n",
      "    <title>Feature Selection and Feature Extraction in Pattern Analysis: A Literature Review</title>\n",
      "    <updated>2019-05-07T23:41:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.02845v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.02845v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Pattern analysis often requires a pre-processing stage for extracting or selecting features in order to help the classification, prediction, or clustering stage discriminate or represent the data in a better way. The reason for this requirement is that the raw data are complex and difficult to process without extracting or selecting appropriate features beforehand. This paper reviews theory and motivation of different common methods of feature selection and extraction and introduces some of their applications. Some numerical implementations are also shown for these methods. Finally, the methods in feature selection and extraction are compared.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-07T23:41:34Z</published>\n",
      "    <arxiv:comment>14 pages, 1 figure, 2 tables, survey (literature review) paper</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Benyamin Ghojogh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maria N. Samad</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sayema Asif Mashhadi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tania Kapoor</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wahab Ali</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fakhri Karray</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mark Crowley</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.02898v1</id>\n",
      "    <title>A Generative Model for Sampling High-Performance and Diverse Weights for Neural Networks</title>\n",
      "    <updated>2019-05-07T04:28:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.02898v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.02898v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent work on mode connectivity in the loss landscape of deep neural networks has demonstrated that the locus of (sub-)optimal weight vectors lies on continuous paths. In this work, we train a neural network that serves as a hypernetwork, mapping a latent vector into high-performance (low-loss) weight vectors, generalizing recent findings of mode connectivity to higher dimensional manifolds. We formulate the training objective as a compromise between accuracy and diversity, where the diversity takes into account trivial symmetry transformations of the target network. We demonstrate how to reduce the number of parameters in the hypernetwork by parameter sharing. Once learned, the hypernetwork allows for a computationally efficient, ancestral sampling of neural network weights, which we recruit to form large ensembles. The improvement in classification accuracy obtained by this ensembling indicates that the generated manifold extends in dimensions other than directions implied by trivial symmetries. For computational efficiency, we distill an ensemble into a single classifier while retaining generalization.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-07T04:28:46Z</published>\n",
      "    <arxiv:comment>arXiv admin note: substantial text overlap with arXiv:1801.01952</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Lior Deutsch</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Erik Nijkamp</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu Yang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.01958v1</id>\n",
      "    <title>Text2Node: a Cross-Domain System for Mapping Arbitrary Phrases to a Taxonomy</title>\n",
      "    <updated>2019-04-11T17:31:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.01958v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.01958v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Electronic health record (EHR) systems are used extensively throughout the healthcare domain. However, data interchangeability between EHR systems is limited due to the use of different coding standards across systems. Existing methods of mapping coding standards based on manual human experts mapping, dictionary mapping, symbolic NLP and classification are unscalable and cannot accommodate large scale EHR datasets.\n",
      "  In this work, we present Text2Node, a cross-domain mapping system capable of mapping medical phrases to concepts in a large taxonomy (such as SNOMED CT). The system is designed to generalize from a limited set of training samples and map phrases to elements of the taxonomy that are not covered by training data. As a result, our system is scalable, robust to wording variants between coding systems and can output highly relevant concepts when no exact concept exists in the target taxonomy. Text2Node operates in three main stages: first, the lexicon is mapped to word embeddings; second, the taxonomy is vectorized using node embeddings; and finally, the mapping function is trained to connect the two embedding spaces. We compared multiple algorithms and architectures for each stage of the training, including GloVe and FastText word embeddings, CNN and Bi-LSTM mapping functions, and node2vec for node embeddings. We confirmed the robustness and generalisation properties of Text2Node by mapping ICD-9-CM Diagnosis phrases to SNOMED CT and by zero-shot training at comparable accuracy.\n",
      "  This system is a novel methodological contribution to the task of normalizing and linking phrases to a taxonomy, advancing data interchangeability in healthcare. When applied, the system can use electronic health records to generate an embedding that incorporates taxonomical medical knowledge to improve clinical predictive models.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-11T17:31:23Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Rohollah Soltani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexandre Tomberg</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.01973v1</id>\n",
      "    <title>Who wrote this book? A challenge for e-commerce</title>\n",
      "    <updated>2019-04-19T10:13:07Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.01973v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.01973v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Modern e-commerce catalogs contain millions of references, associated with textual and visual information that is of paramount importance for the products to be found via search or browsing. Of particular significance is the book category, where the author name(s) field poses a significant challenge. Indeed, books written by a given author (such as F. Scott Fitzgerald) might be listed with different authors' names in a catalog due to abbreviations and spelling variants and mistakes, among others. To solve this problem at scale, we design a composite system involving open data sources for books as well as machine learning components leveraging deep learning-based techniques for natural language processing. In particular, we use Siamese neural networks for an approximate match with known author names, and direct correction of the provided author's name using sequence-to-sequence learning with neural networks. We evaluate this approach on product data from the e-commerce website Rakuten France, and find that the top proposal of the system is the normalized author name with 72% accuracy.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-19T10:13:07Z</published>\n",
      "    <arxiv:comment>8 pages, 1 figure</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Béranger Dumont</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Simona Maggio</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ghiles Sidi Said</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Quoc-Tien Au</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.01991v1</id>\n",
      "    <title>A Content-Based Approach to Email Triage Action Prediction: Exploration and Evaluation</title>\n",
      "    <updated>2019-04-30T01:52:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.01991v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.01991v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Email has remained a principal form of communication among people, both in enterprise and social settings. With a deluge of emails crowding our mailboxes daily, there is a dire need of smart email systems that can recover important emails and make personalized recommendations. In this work, we study the problem of predicting user triage actions to incoming emails where we take the reply prediction as a working example. Different from existing methods, we formulate the triage action prediction as a recommendation problem and focus on the content-based approach, where the users are represented using the content of current and past emails. We also introduce additional similarity features to further explore the affinities between users and emails. Experiments on the publicly available Avocado email collection demonstrate the advantages of our proposed recommendation framework and our method is able to achieve better performance compared to the state-of-the-art deep recommendation methods. More importantly, we provide valuable insight into the effectiveness of different textual and user representations and show that traditional bag-of-words approaches, with the help from the similarity features, compete favorably with the more advanced neural embedding methods.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-30T01:52:57Z</published>\n",
      "    <arxiv:comment>User representations, Personalization, Email response prediction, Similarity features</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Sudipto Mukherjee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ke Jiang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.01998v1</id>\n",
      "    <title>A Persona-based Multi-turn Conversation Model in an Adversarial Learning Framework</title>\n",
      "    <updated>2019-04-29T15:09:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.01998v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.01998v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to multi-turn dialogue by modifying the state-of-the-art hredGAN architecture. To achieve this, we introduce an additional input modality into the encoder and decoder of hredGAN to capture other attributes such as speaker identity, location, sub-topics, and other external attributes that might be available from the corpus of human-to-human interactions. The resulting persona hredGAN ($phredGAN$) shows better performance than both the existing persona-based Seq2Seq and hredGAN models when those external attributes are available in a multi-turn dialogue corpus. This superiority is demonstrated on TV drama series with character consistency (such as Big Bang Theory and Friends) and customer service interaction datasets such as Ubuntu dialogue corpus in terms of perplexity, BLEU, ROUGE, and Distinct n-gram scores.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-29T15:09:34Z</published>\n",
      "    <arxiv:comment>2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA). arXiv admin note: substantial text overlap with arXiv:1905.01992</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Oluwatobi O. Olabiyi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anish Khazane</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Erik T. Mueller</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.02019v1</id>\n",
      "    <title>Conditioning LSTM Decoder and Bi-directional Attention Based Question Answering System</title>\n",
      "    <updated>2019-05-02T01:07:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.02019v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.02019v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Applying neural-networks on Question Answering has gained increasing popularity in recent years. In this paper, I implemented a model with Bi-directional attention flow layer, connected with a Multi-layer LSTM encoder, connected with one start-index decoder and one conditioning end-index decoder. I introduce a new end-index decoder layer, conditioning on start-index output. The Experiment shows this has increased model performance by 15.16%. For prediction, I proposed a new smart-span equation, rewarding both short answer length and high probability in start-index and end-index, which further improved the prediction accuracy. The best single model achieves an F1 score of 73.97% and EM score of 64.95% on test set.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-02T01:07:20Z</published>\n",
      "    <arxiv:comment>7 pages, 7 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Heguang Liu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.03421v3</id>\n",
      "    <title>Adversarial Image Translation: Unrestricted Adversarial Examples in Face Recognition Systems</title>\n",
      "    <updated>2020-01-28T06:36:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.03421v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.03421v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Thanks to recent advances in deep neural networks (DNNs), face recognition systems have become highly accurate in classifying a large number of face images. However, recent studies have found that DNNs could be vulnerable to adversarial examples, raising concerns about the robustness of such systems. Adversarial examples that are not restricted to small perturbations could be more serious since conventional certified defenses might be ineffective against them. To shed light on the vulnerability to such adversarial examples, we propose a flexible and efficient method for generating unrestricted adversarial examples using image translation techniques. Our method enables us to translate a source image into any desired facial appearance with large perturbations to deceive target face recognition systems. Our experimental results indicate that our method achieved about $90$ and $80\\%$ attack success rates under white- and black-box settings, respectively, and that the translated images are perceptually realistic and maintain the identifiability of the individual while the perturbations are large enough to bypass certified defenses.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-09T02:58:45Z</published>\n",
      "    <arxiv:comment>Kazuya Kakizaki and Kosuke Yoshida share equal contributions. Accepted at AAAI Workshop on Artificial Intelligence Safety (2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Kazuya Kakizaki</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kosuke Yoshida</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.02941v1</id>\n",
      "    <title>Robust Federated Training via Collaborative Machine Teaching using Trusted Instances</title>\n",
      "    <updated>2019-05-08T07:27:04Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.02941v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.02941v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Federated learning performs distributed model training using local data hosted by agents. It shares only model parameter updates for iterative aggregation at the server. Although it is privacy-preserving by design, federated learning is vulnerable to noise corruption of local agents, as demonstrated in the previous study on adversarial data poisoning threat against federated learning systems. Even a single noise-corrupted agent can bias the model training. In our work, we propose a collaborative and privacy-preserving machine teaching paradigm with multiple distributed teachers, to improve robustness of the federated training process against local data corruption. We assume that each local agent (teacher) have the resources to verify a small portions of trusted instances, which may not by itself be adequate for learning. In the proposed collaborative machine teaching method, these trusted instances guide the distributed agents to jointly select a compact while informative training subset from data hosted by their own. Simultaneously, the agents learn to add changes of limited magnitudes into the selected data instances, in order to improve the testing performances of the federally trained model despite of the training data corruption. Experiments on toy and real data demonstrate that our approach can identify training set bugs effectively and suggest appropriate changes to the labels. Our algorithm is a step toward trustworthy machine learning.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-08T07:27:04Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yufei Han</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiangliang Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.03028v2</id>\n",
      "    <title>Deep Landscape Forecasting for Real-time Bidding Advertising</title>\n",
      "    <updated>2019-05-12T16:27:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.03028v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.03028v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The emergence of real-time auction in online advertising has drawn huge attention of modeling the market competition, i.e., bid landscape forecasting. The problem is formulated as to forecast the probability distribution of market price for each ad auction. With the consideration of the censorship issue which is caused by the second-price auction mechanism, many researchers have devoted their efforts on bid landscape forecasting by incorporating survival analysis from medical research field. However, most existing solutions mainly focus on either counting-based statistics of the segmented sample clusters, or learning a parameterized model based on some heuristic assumptions of distribution forms. Moreover, they neither consider the sequential patterns of the feature over the price space. In order to capture more sophisticated yet flexible patterns at fine-grained level of the data, we propose a Deep Landscape Forecasting (DLF) model which combines deep learning for probability distribution forecasting and survival analysis for censorship handling. Specifically, we utilize a recurrent neural network to flexibly model the conditional winning probability w.r.t. each bid price. Then we conduct the bid landscape forecasting through probability chain rule with strict mathematical derivations. And, in an end-to-end manner, we optimize the model by minimizing two negative likelihood losses with comprehensive motivations. Without any specific assumption for the distribution form of bid landscape, our model shows great advantages over previous works on fitting various sophisticated market price distributions. In the experiments over two large-scale real-world datasets, our model significantly outperforms the state-of-the-art solutions under various metrics.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-07T15:22:02Z</published>\n",
      "    <arxiv:comment>KDD 2019. The reproducible code and dataset link is https://github.com/rk2900/DLF</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Kan Ren</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiarui Qin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lei Zheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhengyu Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Weinan Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yong Yu</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3292500.3330870</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3292500.3330870\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.03046v1</id>\n",
      "    <title>PiNet: A Permutation Invariant Graph Neural Network for Graph Classification</title>\n",
      "    <updated>2019-05-08T12:51:52Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.03046v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.03046v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose an end-to-end deep learning learning model for graph classification and representation learning that is invariant to permutation of the nodes of the input graphs. We address the challenge of learning a fixed size graph representation for graphs of varying dimensions through a differentiable node attention pooling mechanism. In addition to a theoretical proof of its invariance to permutation, we provide empirical evidence demonstrating the statistically significant gain in accuracy when faced with an isomorphic graph classification task given only a small number of training examples. We analyse the effect of four different matrices to facilitate the local message passing mechanism by which graph convolutions are performed vs. a matrix parametrised by a learned parameter pair able to transition smoothly between the former. Finally, we show that our model achieves competitive classification performance with existing techniques on a set of molecule datasets.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-08T12:51:52Z</published>\n",
      "    <arxiv:comment>7 pages, 4 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Peter Meltzer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marcelo Daniel Gutierrez Mallea</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peter J. Bentley</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.03066v3</id>\n",
      "    <title>Training a Fast Object Detector for LiDAR Range Images Using Labeled Data from Sensors with Higher Resolution</title>\n",
      "    <updated>2019-12-05T14:03:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.03066v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.03066v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we describe a strategy for training neural networks for object detection in range images obtained from one type of LiDAR sensor using labeled data from a different type of LiDAR sensor. Additionally, an efficient model for object detection in range images for use in self-driving cars is presented. Currently, the highest performing algorithms for object detection from LiDAR measurements are based on neural networks. Training these networks using supervised learning requires large annotated datasets. Therefore, most research using neural networks for object detection from LiDAR point clouds is conducted on a very small number of publicly available datasets. Consequently, only a small number of sensor types are used. We use an existing annotated dataset to train a neural network that can be used with a LiDAR sensor that has a lower resolution than the one used for recording the annotated dataset. This is done by simulating data from the lower resolution LiDAR sensor based on the higher resolution dataset. Furthermore, improvements to models that use LiDAR range images for object detection are presented. The results are validated using both simulated sensor data and data from an actual lower resolution sensor mounted to a research vehicle. It is shown that the model can detect objects from 360° range images in real time.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-08T13:43:03Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <arxiv:journal_ref>2019 IEEE Intelligent Transportation Systems Conference (ITSC), Auckland, New Zealand, 2019, pp. 2707-2713</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Manuel Herzog</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Klaus Dietmayer</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/ITSC.2019.8917011</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/ITSC.2019.8917011\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.03218v1</id>\n",
      "    <title>MetaPred: Meta-Learning for Clinical Risk Prediction with Limited Patient Electronic Health Records</title>\n",
      "    <updated>2019-05-08T17:07:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.03218v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.03218v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In recent years, increasingly augmentation of health data, such as patient Electronic Health Records (EHR), are becoming readily available. This provides an unprecedented opportunity for knowledge discovery and data mining algorithms to dig insights from them, which can, later on, be helpful to the improvement of the quality of care delivery. Predictive modeling of clinical risk, including in-hospital mortality, hospital readmission, chronic disease onset, condition exacerbation, etc., from patient EHR, is one of the health data analytic problems that attract most of the interests. The reason is not only because the problem is important in clinical settings, but also there are challenges working with EHR such as sparsity, irregularity, temporality, etc. Different from applications in other domains such as computer vision and natural language processing, the labeled data samples in medicine (patients) are relatively limited, which creates lots of troubles for effective predictive model learning, especially for complicated models such as deep learning. In this paper, we propose MetaPred, a meta-learning for clinical risk prediction from longitudinal patient EHRs. In particular, in order to predict the target risk where there are limited data samples, we train a meta-learner from a set of related risk prediction tasks which learns how a good predictor is learned. The meta-learned can then be directly used in target risk prediction, and the limited available samples can be used for further fine-tuning the model performance. The effectiveness of MetaPred is tested on a real patient EHR repository from Oregon Health &amp; Science University. We are able to demonstrate that with CNN and RNN as base predictors, MetaPred can achieve much better performance for predicting target risk with low resources comparing with the predictor trained on the limited samples available for this risk.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-08T17:07:51Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xi Sheryl Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fengyi Tang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hiroko Dodge</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiayu Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fei Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.03297v3</id>\n",
      "    <title>Interpretable Subgroup Discovery in Treatment Effect Estimation with Application to Opioid Prescribing Guidelines</title>\n",
      "    <updated>2020-03-04T19:48:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.03297v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.03297v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The dearth of prescribing guidelines for physicians is one key driver of the current opioid epidemic in the United States. In this work, we analyze medical and pharmaceutical claims data to draw insights on characteristics of patients who are more prone to adverse outcomes after an initial synthetic opioid prescription. Toward this end, we propose a generative model that allows discovery from observational data of subgroups that demonstrate an enhanced or diminished causal effect due to treatment. Our approach models these sub-populations as a mixture distribution, using sparsity to enhance interpretability, while jointly learning nonlinear predictors of the potential outcomes to better adjust for confounding. The approach leads to human-interpretable insights on discovered subgroups, improving the practical utility for decision support</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-08T19:00:09Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>First ACM Conference on Health, Inference and Learning (CHIL) 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Chirag Nagpal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dennis Wei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bhanukiran Vinzamuri</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Monica Shekhar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sara E. Berger</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Subhro Das</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kush R. Varshney</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3368555.3384456</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3368555.3384456\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.03330v2</id>\n",
      "    <title>Universal Sound Separation</title>\n",
      "    <updated>2019-08-02T20:44:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.03330v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.03330v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent deep learning approaches have achieved impressive performance on speech enhancement and separation tasks. However, these approaches have not been investigated for separating mixtures of arbitrary sounds of different types, a task we refer to as universal sound separation, and it is unknown how performance on speech tasks carries over to non-speech tasks. To study this question, we develop a dataset of mixtures containing arbitrary sounds, and use it to investigate the space of mask-based separation architectures, varying both the overall network architecture and the framewise analysis-synthesis basis for signal transformations. These network architectures include convolutional long short-term memory networks and time-dilated convolution stacks inspired by the recent success of time-domain enhancement networks like ConvTasNet. For the latter architecture, we also propose novel modifications that further improve separation performance. In terms of the framewise analysis-synthesis basis, we explore both a short-time Fourier transform (STFT) and a learnable basis, as used in ConvTasNet. For both of these bases, we also examine the effect of window size. In particular, for STFTs, we find that longer windows (25-50 ms) work best for speech/non-speech separation, while shorter windows (2.5 ms) work best for arbitrary sounds. For learnable bases, shorter windows (2.5 ms) work best on all tasks. Surprisingly, for universal sound separation, STFTs outperform learnable bases. Our best methods produce an improvement in scale-invariant signal-to-distortion ratio of over 13 dB for speech/non-speech separation and close to 10 dB for universal sound separation.</summary>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-08T20:48:49Z</published>\n",
      "    <arxiv:comment>5 pages, accepted to WASPAA 2019</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.SD\"/>\n",
      "    <author>\n",
      "      <name>Ilya Kavalerov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Scott Wisdom</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hakan Erdogan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Brian Patton</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kevin Wilson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jonathan Le Roux</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>John R. Hershey</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.02576v1</id>\n",
      "    <title>Regression Equilibrium</title>\n",
      "    <updated>2019-05-04T13:00:42Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.02576v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.02576v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Prediction is a well-studied machine learning task, and prediction algorithms are core ingredients in online products and services. Despite their centrality in the competition between online companies who offer prediction-based products, the \\textit{strategic} use of prediction algorithms remains unexplored. The goal of this paper is to examine strategic use of prediction algorithms. We introduce a novel game-theoretic setting that is based on the PAC learning framework, where each player (aka a prediction algorithm aimed at competition) seeks to maximize the sum of points for which it produces an accurate prediction and the others do not. We show that algorithms aiming at generalization may wittingly mispredict some points to perform better than others on expectation. We analyze the empirical game, i.e., the game induced on a given sample, prove that it always possesses a pure Nash equilibrium, and show that every better-response learning process converges. Moreover, our learning-theoretic analysis suggests that players can, with high probability, learn an approximate pure Nash equilibrium for the whole population using a small number of samples.</summary>\n",
      "    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-04T13:00:42Z</published>\n",
      "    <arxiv:comment>This paper was published in the twentieth ACM conference on Economics and Computation (ACM EC 19). arXiv admin note: substantial text overlap with arXiv:1806.01703</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.GT\"/>\n",
      "    <author>\n",
      "      <name>Omer Ben-Porat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Moshe Tennenholtz</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.02706v2</id>\n",
      "    <title>Learning Unsupervised Multi-View Stereopsis via Robust Photometric Consistency</title>\n",
      "    <updated>2019-06-06T17:30:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.02706v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.02706v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present a learning based approach for multi-view stereopsis (MVS). While current deep MVS methods achieve impressive results, they crucially rely on ground-truth 3D training data, and acquisition of such precise 3D geometry for supervision is a major hurdle. Our framework instead leverages photometric consistency between multiple views as supervisory signal for learning depth prediction in a wide baseline MVS setup. However, naively applying photo consistency constraints is undesirable due to occlusion and lighting changes across views. To overcome this, we propose a robust loss formulation that: a) enforces first order consistency and b) for each point, selectively enforces consistency with some views, thus implicitly handling occlusions. We demonstrate our ability to learn MVS without 3D supervision using a real dataset, and show that each component of our proposed robust loss results in a significant improvement. We qualitatively observe that our reconstructions are often more complete than the acquired ground truth, further showing the merits of this approach. Lastly, our learned model generalizes to novel settings, and our approach allows adaptation of existing CNNs to datasets without ground-truth 3D by unsupervised finetuning. Project webpage: https://tejaskhot.github.io/unsup_mvs</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-07T17:45:22Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Tejas Khot</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shubham Agrawal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shubham Tulsiani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christoph Mertz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Simon Lucey</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Martial Hebert</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.02234v2</id>\n",
      "    <title>Image Matters: Scalable Detection of Offensive and Non-Compliant Content / Logo in Product Images</title>\n",
      "    <updated>2019-08-02T07:38:26Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.02234v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.02234v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In e-commerce, product content, especially product images have a significant influence on a customer's journey from product discovery to evaluation and finally, purchase decision. Since many e-commerce retailers sell items from other third-party marketplace sellers besides their own, the content published by both internal and external content creators needs to be monitored and enriched, wherever possible. Despite guidelines and warnings, product listings that contain offensive and non-compliant images continue to enter catalogs. Offensive and non-compliant content can include a wide range of objects, logos, and banners conveying violent, sexually explicit, racist, or promotional messages. Such images can severely damage the customer experience, lead to legal issues, and erode the company brand. In this paper, we present a computer vision driven offensive and non-compliant image detection system for extremely large image datasets. This paper delves into the unique challenges of applying deep learning to real-world product image data from retail world. We demonstrate how we resolve a number of technical challenges such as lack of training data, severe class imbalance, fine-grained class definitions etc. using a number of practical yet unique technical strategies. Our system combines state-of-the-art image classification and object detection techniques with budgeted crowdsourcing to develop a solution customized for a massive, diverse, and constantly evolving product catalog.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-06T18:35:28Z</published>\n",
      "    <arxiv:comment>10 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Shreyansh Gandhi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Samrat Kokkula</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abon Chaudhuri</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alessandro Magnani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Theban Stanley</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Behzad Ahmadi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Venkatesh Kandaswamy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Omer Ovenc</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shie Mannor</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.02259v2</id>\n",
      "    <title>Source Generator Attribution via Inversion</title>\n",
      "    <updated>2019-06-13T19:01:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.02259v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.02259v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>With advances in Generative Adversarial Networks (GANs) leading to dramatically-improved synthetic images and video, there is an increased need for algorithms which extend traditional forensics to this new category of imagery. While GANs have been shown to be helpful in a number of computer vision applications, there are other problematic uses such as `deep fakes' which necessitate such forensics. Source camera attribution algorithms using various cues have addressed this need for imagery captured by a camera, but there are fewer options for synthetic imagery. We address the problem of attributing a synthetic image to a specific generator in a white box setting, by inverting the process of generation. This enables us to simultaneously determine whether the generator produced the image and recover an input which produces a close match to the synthetic image.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-06T20:47:28Z</published>\n",
      "    <arxiv:comment>Updated with new experiments</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Michael Albright</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Scott McCloskey</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.02304v1</id>\n",
      "    <title>CrossTrainer: Practical Domain Adaptation with Loss Reweighting</title>\n",
      "    <updated>2019-05-07T00:48:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.02304v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.02304v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Domain adaptation provides a powerful set of model training techniques given domain-specific training data and supplemental data with unknown relevance. The techniques are useful when users need to develop models with data from varying sources, of varying quality, or from different time ranges. We build CrossTrainer, a system for practical domain adaptation. CrossTrainer utilizes loss reweighting, which provides consistently high model accuracy across a variety of datasets in our empirical analysis. However, loss reweighting is sensitive to the choice of a weight hyperparameter that is expensive to tune. We develop optimizations leveraging unique properties of loss reweighting that allow CrossTrainer to output accurate models while improving training time compared to naive hyperparameter search.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-07T00:48:48Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Justin Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Edward Gan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kexin Rong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sahaana Suri</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peter Bailis</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3329486.3329491</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3329486.3329491\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.02138v1</id>\n",
      "    <title>Is a Single Embedding Enough? Learning Node Representations that Capture Multiple Social Contexts</title>\n",
      "    <updated>2019-05-06T16:46:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.02138v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.02138v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent interest in graph embedding methods has focused on learning a single representation for each node in the graph. But can nodes really be best described by a single vector representation? In this work, we propose a method for learning multiple representations of the nodes in a graph (e.g., the users of a social network). Based on a principled decomposition of the ego-network, each representation encodes the role of the node in a different local community in which the nodes participate. These representations allow for improved reconstruction of the nuanced relationships that occur in the graph -- a phenomenon that we illustrate through state-of-the-art results on link prediction tasks on a variety of graphs, reducing the error by up to $90\\%$. In addition, we show that these embeddings allow for effective visual analysis of the learned community structure.</summary>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-06T16:46:38Z</published>\n",
      "    <arxiv:primary_category term=\"cs.SI\"/>\n",
      "    <arxiv:journal_ref>In Proceedings of \"The Web Conference\" 2019, WWW, 2019</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Alessandro Epasto</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bryan Perozzi</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3308558.3313660</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3308558.3313660\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.02197v1</id>\n",
      "    <title>Back to the Future: Predicting Traffic Shockwave Formation and Propagation Using a Convolutional Encoder-Decoder Network</title>\n",
      "    <updated>2019-05-04T18:33:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.02197v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.02197v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This study proposes a deep learning methodology to predict the propagation of traffic shockwaves. The input to the deep neural network is time-space diagram of the study segment, and the output of the network is the predicted (future) propagation of the shockwave on the study segment in the form of time-space diagram. The main feature of the proposed methodology is the ability to extract the features embedded in the time-space diagram to predict the propagation of traffic shockwaves.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-04T18:33:55Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Mohammadreza Khajeh-Hosseini</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alireza Talebpour</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.02199v1</id>\n",
      "    <title>Nonlinear Approximation and (Deep) ReLU Networks</title>\n",
      "    <updated>2019-05-05T12:54:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.02199v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.02199v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This article is concerned with the approximation and expressive powers of deep neural networks. This is an active research area currently producing many interesting papers. The results most commonly found in the literature prove that neural networks approximate functions with classical smoothness to the same accuracy as classical linear methods of approximation, e.g. approximation by polynomials or by piecewise polynomials on prescribed partitions. However, approximation by neural networks depending on n parameters is a form of nonlinear approximation and as such should be compared with other nonlinear methods such as variable knot splines or n-term approximation from dictionaries. The performance of neural networks in targeted applications such as machine learning indicate that they actually possess even greater approximation power than these traditional methods of nonlinear approximation. The main results of this article prove that this is indeed the case. This is done by exhibiting large classes of functions which can be efficiently captured by neural networks where classical nonlinear methods fall short of the task. The present article purposefully limits itself to studying the approximation of univariate functions by ReLU networks. Many generalizations to functions of several variables and other activation functions can be envisioned. However, even in this simplest of settings considered here, a theory that completely quantifies the approximation power of neural networks is still lacking.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-05T12:54:35Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>I. Daubechies</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>R. DeVore</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>S. Foucart</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>B. Hanin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>G. Petrova</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.02219v2</id>\n",
      "    <title>Lessons from Contextual Bandit Learning in a Customer Support Bot</title>\n",
      "    <updated>2019-06-18T17:59:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.02219v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.02219v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this work, we describe practical lessons we have learned from successfully using contextual bandits (CBs) to improve key business metrics of the Microsoft Virtual Agent for customer support. While our current use cases focus on single step einforcement learning (RL) and mostly in the domain of natural language processing and information retrieval we believe many of our findings are generally applicable. Through this article, we highlight certain issues that RL practitioners may encounter in similar types of applications as well as offer practical solutions to these challenges.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-06T18:00:46Z</published>\n",
      "    <arxiv:comment>Reinforcement Learning for Real Life Workshop</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Nikos Karampatziakis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sebastian Kochman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jade Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paul Mineiro</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kathy Osborne</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Weizhu Chen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.12072v3</id>\n",
      "    <title>Flow-based generative models for Markov chain Monte Carlo in lattice field theory</title>\n",
      "    <updated>2019-09-09T17:53:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.12072v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.12072v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A Markov chain update scheme using a machine-learned flow-based generative model is proposed for Monte Carlo sampling in lattice field theories. The generative model may be optimized (trained) to produce samples from a distribution approximating the desired Boltzmann distribution determined by the lattice action of the theory being studied. Training the model systematically improves autocorrelation times in the Markov chain, even in regions of parameter space where standard Markov chain Monte Carlo algorithms exhibit critical slowing down in producing decorrelated updates. Moreover, the model may be trained without existing samples from the desired distribution. The algorithm is compared with HMC and local Metropolis sampling for $φ^4$ theory in two dimensions.</summary>\n",
      "    <category term=\"hep-lat\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cond-mat.stat-mech\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-26T23:30:09Z</published>\n",
      "    <arxiv:comment>13 pages, 7 figures; corrected normalization conventions in eqns. 20 and 23</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"hep-lat\"/>\n",
      "    <arxiv:journal_ref>Phys. Rev. D 100, 034515 (2019)</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>M. S. Albergo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>G. Kanwar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>P. E. Shanahan</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1103/PhysRevD.100.034515</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1103/PhysRevD.100.034515\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.13142v3</id>\n",
      "    <title>Incorporating Symbolic Sequential Modeling for Speech Enhancement</title>\n",
      "    <updated>2019-07-02T02:38:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.13142v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.13142v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In a noisy environment, a lossy speech signal can be automatically restored by a listener if he/she knows the language well. That is, with the built-in knowledge of a \"language model\", a listener may effectively suppress noise interference and retrieve the target speech signals. Accordingly, we argue that familiarity with the underlying linguistic content of spoken utterances benefits speech enhancement (SE) in noisy environments. In this study, in addition to the conventional modeling for learning the acoustic noisy-clean speech mapping, an abstract symbolic sequential modeling is incorporated into the SE framework. This symbolic sequential modeling can be regarded as a \"linguistic constraint\" in learning the acoustic noisy-clean speech mapping function. In this study, the symbolic sequences for acoustic signals are obtained as discrete representations with a Vector Quantized Variational Autoencoder algorithm. The obtained symbols are able to capture high-level phoneme-like content from speech signals. The experimental results demonstrate that the proposed framework can obtain notable performance improvement in terms of perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI) on the TIMIT dataset.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-30T10:31:22Z</published>\n",
      "    <arxiv:comment>Accepted to Interspeech 2019</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Chien-Feng Liao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu Tsao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xugang Lu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hisashi Kawai</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.13179v1</id>\n",
      "    <title>Weakly Supervised Open-set Domain Adaptation by Dual-domain Collaboration</title>\n",
      "    <updated>2019-04-30T11:54:19Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.13179v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.13179v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In conventional domain adaptation, a critical assumption is that there exists a fully labeled domain (source) that contains the same label space as another unlabeled or scarcely labeled domain (target). However, in the real world, there often exist application scenarios in which both domains are partially labeled and not all classes are shared between these two domains. Thus, it is meaningful to let partially labeled domains learn from each other to classify all the unlabeled samples in each domain under an open-set setting. We consider this problem as weakly supervised open-set domain adaptation. To address this practical setting, we propose the Collaborative Distribution Alignment (CDA) method, which performs knowledge transfer bilaterally and works collaboratively to classify unlabeled data and identify outlier samples. Extensive experiments on the Office benchmark and an application on person reidentification show that our method achieves state-of-the-art performance.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-30T11:54:19Z</published>\n",
      "    <arxiv:comment>CVPR 2019</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Shuhan Tan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiening Jiao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wei-Shi Zheng</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.13195v1</id>\n",
      "    <title>Test Selection for Deep Learning Systems</title>\n",
      "    <updated>2019-04-30T12:44:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.13195v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.13195v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Testing of deep learning models is challenging due to the excessive number and complexity of computations involved. As a result, test data selection is performed manually and in an ad hoc way. This raises the question of how we can automatically select candidate test data to test deep learning models. Recent research has focused on adapting test selection metrics from code-based software testing (such as coverage) to deep learning. However, deep learning models have different attributes from code such as spread of computations across the entire network reflecting training data properties, balance of neuron weights and redundancy (use of many more neurons than needed). Such differences make code-based metrics inappropriate to select data that can challenge the models (can trigger misclassification). We thus propose a set of test selection metrics based on the notion of model uncertainty (model confidence on specific inputs). Intuitively, the more uncertain we are about a candidate sample, the more likely it is that this sample triggers a misclassification. Similarly, the samples for which we are the most uncertain, are the most informative and should be used to improve the model by retraining. We evaluate these metrics on two widely-used image classification problems involving real and artificial (adversarial) data. We show that uncertainty-based metrics have a strong ability to select data that are misclassified and lead to major improvement in classification accuracy during retraining: up to 80% more gain than random selection and other state-of-the-art metrics on one dataset and up to 29% on the other.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-30T12:44:10Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Wei Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mike Papadakis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anestis Tsakmalis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maxime Cordy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yves Le Traon</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.13196v1</id>\n",
      "    <title>Semantic Referee: A Neural-Symbolic Framework for Enhancing Geospatial Semantic Segmentation</title>\n",
      "    <updated>2019-04-30T12:44:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.13196v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.13196v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Understanding why machine learning algorithms may fail is usually the task of the human expert that uses domain knowledge and contextual information to discover systematic shortcomings in either the data or the algorithm. In this paper, we propose a semantic referee, which is able to extract qualitative features of the errors emerging from deep machine learning frameworks and suggest corrections. The semantic referee relies on ontological reasoning about spatial knowledge in order to characterize errors in terms of their spatial relations with the environment. Using semantics, the reasoner interacts with the learning algorithm as a supervisor. In this paper, the proposed method of the interaction between a neural network classifier and a semantic referee shows how to improve the performance of semantic segmentation for satellite imagery data.</summary>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-30T12:44:22Z</published>\n",
      "    <arxiv:primary_category term=\"cs.AI\"/>\n",
      "    <author>\n",
      "      <name>Marjan Alirezaie</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Martin Längkvist</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michael Sioutis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amy Loutfi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.13197v1</id>\n",
      "    <title>Investigation of Initialization Strategies for the Multiple Instance Adaptive Cosine Estimator</title>\n",
      "    <updated>2019-04-30T12:45:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.13197v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.13197v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Sensors which use electromagnetic induction (EMI) to excite a response in conducting bodies have long been investigated for subsurface explosive hazard detection. In particular, EMI sensors have been used to discriminate between different types of objects, and to detect objects with low metal content. One successful, previously investigated approach is the Multiple Instance Adaptive Cosine Estimator (MI-ACE). In this paper, a number of new initialization techniques for MI-ACE are proposed and evaluated using their respective performance and speed. The cross validated learned signatures, as well as learned background statistics, are used with Adaptive Cosine Estimator (ACE) to generate confidence maps, which are clustered into alarms. Alarms are scored against a ground truth and the initialization approaches are compared.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-30T12:45:18Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>James Bocinsky</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Connor McCurley</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniel Shats</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alina Zare</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.07640v1</id>\n",
      "    <title>Medical device surveillance with electronic health records</title>\n",
      "    <updated>2019-04-03T18:48:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.07640v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.07640v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Post-market medical device surveillance is a challenge facing manufacturers, regulatory agencies, and health care providers. Electronic health records are valuable sources of real world evidence to assess device safety and track device-related patient outcomes over time. However, distilling this evidence remains challenging, as information is fractured across clinical notes and structured records. Modern machine learning methods for machine reading promise to unlock increasingly complex information from text, but face barriers due to their reliance on large and expensive hand-labeled training sets. To address these challenges, we developed and validated state-of-the-art deep learning methods that identify patient outcomes from clinical notes without requiring hand-labeled training data. Using hip replacements as a test case, our methods accurately extracted implant details and reports of complications and pain from electronic health records with up to 96.3% precision, 98.5% recall, and 97.4% F1, improved classification performance by 12.7- 53.0% over rule-based methods, and detected over 6 times as many complication events compared to using structured data alone. Using these events to assess complication-free survivorship of different implant systems, we found significant variation between implants, including for risk of revision surgery, which could not be detected using coded data alone. Patients with revision surgeries had more hip pain mentions in the post-hip replacement, pre-revision period compared to patients with no evidence of revision surgery (mean hip pain mentions 4.97 vs. 3.23; t = 5.14; p &lt; 0.001). Some implant models were associated with higher or lower rates of hip pain mentions. Our methods complement existing surveillance mechanisms by requiring orders of magnitude less hand-labeled training data, offering a scalable solution for national medical device surveillance.</summary>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-03T18:48:20Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CY\"/>\n",
      "    <author>\n",
      "      <name>Alison Callahan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jason A Fries</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christopher Ré</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>James I Huddleston</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nicholas J Giori</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Scott Delp</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nigam H Shah</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.07687v4</id>\n",
      "    <title>Advanced Customer Activity Prediction based on Deep Hierarchic Encoder-Decoders</title>\n",
      "    <updated>2019-06-21T17:03:21Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.07687v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.07687v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Product recommender systems and customer profiling techniques have always been a priority in online retail. Recent machine learning research advances and also wide availability of massive parallel numerical computing has enabled various approaches and directions of recommender systems advancement. Worth to mention is the fact that in past years multiple traditional \"offline\" retail business are gearing more and more towards employing inferential and even predictive analytics both to stock-related problems such as predictive replenishment but also to enrich customer interaction experience. One of the most important areas of recommender systems research and development is that of Deep Learning based models which employ representational learning to model consumer behavioral patterns. Current state of the art in Deep Learning based recommender systems uses multiple approaches ranging from already classical methods such as the ones based on learning product representation vector, to recurrent analysis of customer transactional time-series and up to generative models based on adversarial training. Each of these methods has multiple advantages and inherent weaknesses such as inability of understanding the actual user-journey, ability to propose only single product recommendation or top-k product recommendations without prediction of actual next-best-offer. In our work we will present a new and innovative architectural approach of applying state-of-the-art hierarchical multi-module encoder-decoder architecture in order to solve several of current state-of-the-art recommender systems issues. Our approach will also produce by-products such as product need-based segmentation and customer behavioral segmentation - all in an end-to-end trainable approach. Finally, we will present a couple methods that solve known retail &amp; distribution pain-points based on the proposed architecture.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-11T18:15:33Z</published>\n",
      "    <arxiv:comment>2019 22nd International Conference on Control Systems and Computer Science (CSCS)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Andrei Damian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Laurentiu Piciu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sergiu Turlea</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nicolae Tapus</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.07698v2</id>\n",
      "    <title>Multimodal Subspace Support Vector Data Description</title>\n",
      "    <updated>2020-09-14T13:31:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.07698v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.07698v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we propose a novel method for projecting data from multiple modalities to a new subspace optimized for one-class classification. The proposed method iteratively transforms the data from the original feature space of each modality to a new common feature space along with finding a joint compact description of data coming from all the modalities. For data in each modality, we define a separate transformation to map the data from the corresponding feature space to the new optimized subspace by exploiting the available information from the class of interest only. We also propose different regularization strategies for the proposed method and provide both linear and non-linear formulations. The proposed Multimodal Subspace Support Vector Data Description outperforms all the competing methods using data from a single modality or fusing data from all modalities in four out of five datasets.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-16T14:16:09Z</published>\n",
      "    <arxiv:comment>26 pages manuscript (6 tables, 2 figures), 24 pages supplementary material (27 tables, 10 figures). The manuscript and supplementary material are combined as a single .pdf (50 pages) file</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>Pattern Recognition, 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Fahad Sohrab</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jenni Raitoharju</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexandros Iosifidis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Moncef Gabbouj</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1016/j.patcog.2020.107648</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1016/j.patcog.2020.107648\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.08129v2</id>\n",
      "    <title>Rogue-Gym: A New Challenge for Generalization in Reinforcement Learning</title>\n",
      "    <updated>2019-06-01T03:39:07Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.08129v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.08129v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we propose Rogue-Gym, a simple and classic style roguelike game built for evaluating generalization in reinforcement learning (RL). Combined with the recent progress of deep neural networks, RL has successfully trained human-level agents without human knowledge in many games such as those for Atari 2600. However, it has been pointed out that agents trained with RL methods often overfit the training environment, and they work poorly in slightly different environments. To investigate this problem, some research environments with procedural content generation have been proposed. Following these studies, we propose the use of roguelikes as a benchmark for evaluating the generalization ability of RL agents. In our Rogue-Gym, agents need to explore dungeons that are structured differently each time they start a new game. Thanks to the very diverse structures of the dungeons, we believe that the generalization benchmark of Rogue-Gym is sufficiently fair. In our experiments, we evaluate a standard reinforcement learning method, PPO, with and without enhancements for generalization. The results show that some enhancements believed to be effective fail to mitigate the overfitting in Rogue-Gym, although others slightly improve the generalization ability.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-17T08:31:06Z</published>\n",
      "    <arxiv:comment>8 pages, 14 figures, 4 tables, accepted to IEEE COG 2019</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yuji Kanagawa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tomoyuki Kaneko</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.08144v1</id>\n",
      "    <title>Predicting drug-target interaction using 3D structure-embedded graph representations from graph neural networks</title>\n",
      "    <updated>2019-04-17T09:03:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.08144v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.08144v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Accurate prediction of drug-target interaction (DTI) is essential for in silico drug design. For the purpose, we propose a novel approach for predicting DTI using a GNN that directly incorporates the 3D structure of a protein-ligand complex. We also apply a distance-aware graph attention algorithm with gate augmentation to increase the performance of our model. As a result, our model shows better performance than docking and other deep learning methods for both virtual screening and pose prediction. In addition, our model can reproduce the natural population distribution of active molecules and inactive molecules.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-17T09:03:54Z</published>\n",
      "    <arxiv:comment>20 pages, 2 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jaechang Lim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Seongok Ryu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kyubyong Park</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yo Joong Choe</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiyeon Ham</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Woo Youn Kim</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.08149v2</id>\n",
      "    <title>Bayesian policy selection using active inference</title>\n",
      "    <updated>2019-04-25T14:28:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.08149v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.08149v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Learning to take actions based on observations is a core requirement for artificial agents to be able to be successful and robust at their task. Reinforcement Learning (RL) is a well-known technique for learning such policies. However, current RL algorithms often have to deal with reward shaping, have difficulties generalizing to other environments and are most often sample inefficient. In this paper, we explore active inference and the free energy principle, a normative theory from neuroscience that explains how self-organizing biological systems operate by maintaining a model of the world and casting action selection as an inference problem. We apply this concept to a typical problem known to the RL community, the mountain car problem, and show how active inference encompasses both RL and learning from demonstrations.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-17T09:18:07Z</published>\n",
      "    <arxiv:comment>ICLR 2019 Workshop on Structure &amp; priors in reinforcement learning</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Ozan Çatal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Johannes Nauta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tim Verbelen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pieter Simoens</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bart Dhoedt</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.08269v1</id>\n",
      "    <title>BS-Nets: An End-to-End Framework For Band Selection of Hyperspectral Image</title>\n",
      "    <updated>2019-04-17T13:41:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.08269v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.08269v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Hyperspectral image (HSI) consists of hundreds of continuous narrow bands with high spectral correlation, which would lead to the so-called Hughes phenomenon and the high computational cost in processing. Band selection has been proven effective in avoiding such problems by removing the redundant bands. However, many of existing band selection methods separately estimate the significance for every single band and cannot fully consider the nonlinear and global interaction between spectral bands. In this paper, by assuming that a complete HSI can be reconstructed from its few informative bands, we propose a general band selection framework, Band Selection Network (termed as BS-Net). The framework consists of a band attention module (BAM), which aims to explicitly model the nonlinear inter-dependencies between spectral bands, and a reconstruction network (RecNet), which is used to restore the original HSI cube from the learned informative bands, resulting in a flexible architecture. The resulting framework is end-to-end trainable, making it easier to train from scratch and to combine with existing networks. We implement two BS-Nets respectively using fully connected networks (BS-Net-FC) and convolutional neural networks (BS-Net-Conv), and compare the results with many existing band selection approaches for three real hyperspectral images, demonstrating that the proposed BS-Nets can accurately select informative band subset with less redundancy and achieve significantly better classification performance with an acceptable time cost.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-17T13:41:38Z</published>\n",
      "    <arxiv:comment>The paper has been submitted to IEEE TGRS</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Yaoming Cai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaobo Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhihua Cai</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/TGRS.2019.2951433</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/TGRS.2019.2951433\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.00424v5</id>\n",
      "    <title>An ADMM Based Framework for AutoML Pipeline Configuration</title>\n",
      "    <updated>2019-12-06T19:27:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.00424v5\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.00424v5\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study the AutoML problem of automatically configuring machine learning pipelines by jointly selecting algorithms and their appropriate hyper-parameters for all steps in supervised learning pipelines. This black-box (gradient-free) optimization with mixed integer &amp; continuous variables is a challenging problem. We propose a novel AutoML scheme by leveraging the alternating direction method of multipliers (ADMM). The proposed framework is able to (i) decompose the optimization problem into easier sub-problems that have a reduced number of variables and circumvent the challenge of mixed variable categories, and (ii) incorporate black-box constraints along-side the black-box optimization objective. We empirically evaluate the flexibility (in utilizing existing AutoML techniques), effectiveness (against open source AutoML toolkits),and unique capability (of executing AutoML with practically motivated black-box constraints) of our proposed scheme on a collection of binary classification data sets from UCI ML&amp; OpenML repositories. We observe that on an average our framework provides significant gains in comparison to other AutoML frameworks (Auto-sklearn &amp; TPOT), highlighting the practical advantages of this framework.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-01T16:51:52Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>published at AAAI 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Sijia Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Parikshit Ram</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Deepak Vijaykeerthy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Djallel Bouneffouf</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gregory Bramble</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Horst Samulowitz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dakuo Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrew Conn</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexander Gray</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.00448v1</id>\n",
      "    <title>On Expected Accuracy</title>\n",
      "    <updated>2019-05-01T18:53:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.00448v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.00448v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We empirically investigate the (negative) expected accuracy as an alternative loss function to cross entropy (negative log likelihood) for classification tasks. Coupled with softmax activation, it has small derivatives over most of its domain, and is therefore hard to optimize. A modified, leaky version is evaluated on a variety of classification tasks, including digit recognition, image classification, sequence tagging and tree tagging, using a variety of neural architectures such as logistic regression, multilayer perceptron, CNN, LSTM and Tree-LSTM. We show that it yields comparable or better accuracy compared to cross entropy. Furthermore, the proposed objective is shown to be more robust to label noise.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-01T18:53:48Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Ozan İrsoy</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.12769v1</id>\n",
      "    <title>Localization, Detection and Tracking of Multiple Moving Sound Sources with a Convolutional Recurrent Neural Network</title>\n",
      "    <updated>2019-04-29T15:26:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.12769v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.12769v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper investigates the joint localization, detection, and tracking of sound events using a convolutional recurrent neural network (CRNN). We use a CRNN previously proposed for the localization and detection of stationary sources, and show that the recurrent layers enable the spatial tracking of moving sources when trained with dynamic scenes. The tracking performance of the CRNN is compared with a stand-alone tracking method that combines a multi-source (DOA) estimator and a particle filter. Their respective performance is evaluated in various acoustic conditions such as anechoic and reverberant scenarios, stationary and moving sources at several angular velocities, and with a varying number of overlapping sources. The results show that the CRNN manages to track multiple sources more consistently than the parametric method across acoustic scenarios, but at the cost of higher localization error.</summary>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-29T15:26:47Z</published>\n",
      "    <arxiv:primary_category term=\"cs.SD\"/>\n",
      "    <author>\n",
      "      <name>Sharath Adavanne</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Archontis Politis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tuomas Virtanen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.12848v6</id>\n",
      "    <title>Unsupervised Data Augmentation for Consistency Training</title>\n",
      "    <updated>2020-11-05T15:11:02Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.12848v6\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.12848v6\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-29T17:56:59Z</published>\n",
      "    <arxiv:comment>NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Qizhe Xie</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zihang Dai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eduard Hovy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Minh-Thang Luong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Quoc V. Le</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.12901v1</id>\n",
      "    <title>Challenges of Real-World Reinforcement Learning</title>\n",
      "    <updated>2019-04-29T18:40:15Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.12901v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.12901v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical RL research.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-29T18:40:15Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Gabriel Dulac-Arnold</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniel Mankowitz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Todd Hester</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.12904v1</id>\n",
      "    <title>Neuromorphic Acceleration for Approximate Bayesian Inference on Neural Networks via Permanent Dropout</title>\n",
      "    <updated>2019-04-29T18:43:07Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.12904v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.12904v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>As neural networks have begun performing increasingly critical tasks for society, ranging from driving cars to identifying candidates for drug development, the value of their ability to perform uncertainty quantification (UQ) in their predictions has risen commensurately. Permanent dropout, a popular method for neural network UQ, involves injecting stochasticity into the inference phase of the model and creating many predictions for each of the test data. This shifts the computational and energy burden of deep neural networks from the training phase to the inference phase. Recent work has demonstrated near-lossless conversion of classical deep neural networks to their spiking counterparts. We use these results to demonstrate the feasibility of conducting the inference phase with permanent dropout on spiking neural networks, mitigating the technique's computational and energy burden, which is essential for its use at scale or on edge platforms. We demonstrate the proposed approach via the Nengo spiking neural simulator on a combination drug therapy dataset for cancer treatment, where UQ is critical. Our results indicate that the spiking approximation gives a predictive distribution practically indistinguishable from that given by the classical network.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-29T18:43:07Z</published>\n",
      "    <arxiv:comment>4 pages, 4 figures. Submitted to International Conference on Neuromorphic Systems (ICONS) 2019</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Nathan Wycoff</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Prasanna Balaprakash</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fangfang Xia</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.13016v4</id>\n",
      "    <title>On Stationary-Point Hitting Time and Ergodicity of Stochastic Gradient Langevin Dynamics</title>\n",
      "    <updated>2020-03-15T22:42:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.13016v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.13016v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Stochastic gradient Langevin dynamics (SGLD) is a fundamental algorithm in stochastic optimization. Recent work by Zhang et al. [2017] presents an analysis for the hitting time of SGLD for the first and second order stationary points. The proof in Zhang et al. [2017] is a two-stage procedure through bounding the Cheeger's constant, which is rather complicated and leads to loose bounds. In this paper, using intuitions from stochastic differential equations, we provide a direct analysis for the hitting times of SGLD to the first and second order stationary points. Our analysis is straightforward. It only relies on basic linear algebra and probability theory tools. Our direct analysis also leads to tighter bounds comparing to Zhang et al. [2017] and shows the explicit dependence of the hitting time on different factors, including dimensionality, smoothness, noise strength, and step size effects. Under suitable conditions, we show that the hitting time of SGLD to first-order stationary points can be dimension-independent. Moreover, we apply our analysis to study several important online estimation problems in machine learning, including linear regression, matrix factorization, and online PCA.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-30T02:03:11Z</published>\n",
      "    <arxiv:comment>41 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Xi Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Simon S. Du</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xin T. Tong</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.12562v1</id>\n",
      "    <title>Soft edit distance for differentiable comparison of symbolic sequences</title>\n",
      "    <updated>2019-04-29T11:31:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.12562v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.12562v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Edit distance, also known as Levenshtein distance, is an essential way to compare two strings that proved to be particularly useful in the analysis of genetic sequences and natural language processing. However, edit distance is a discrete function that is known to be hard to optimize. This fact hampers the use of this metric in Machine Learning. Even as simple algorithm as K-means fails to cluster a set of sequences using edit distance if they are of variable length and abundance. In this paper we propose a novel metric - soft edit distance (SED), which is a smooth approximation of edit distance. It is differentiable and therefore it is possible to optimize it with gradient methods. Similar to original edit distance, SED as well as its derivatives can be calculated with recurrent formulas at polynomial time. We prove usefulness of the proposed metric on synthetic datasets and clustering of biological sequences.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-29T11:31:54Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Evgenii Ofitserov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vasily Tsvetkov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vadim Nazarov</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1904.13373v1</id>\n",
      "    <title>Gradient Coding Based on Block Designs for Mitigating Adversarial Stragglers</title>\n",
      "    <updated>2019-04-30T17:13:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1904.13373v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1904.13373v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Distributed implementations of gradient-based methods, wherein a server distributes gradient computations across worker machines, suffer from slow running machines, called 'stragglers'. Gradient coding is a coding-theoretic framework to mitigate stragglers by enabling the server to recover the gradient sum in the presence of stragglers. 'Approximate gradient codes' are variants of gradient codes that reduce computation and storage overhead per worker by allowing the server to approximately reconstruct the gradient sum.\n",
      "  In this work, our goal is to construct approximate gradient codes that are resilient to stragglers selected by a computationally unbounded adversary. Our motivation for constructing codes to mitigate adversarial stragglers stems from the challenge of tackling stragglers in massive-scale elastic and serverless systems, wherein it is difficult to statistically model stragglers. Towards this end, we propose a class of approximate gradient codes based on balanced incomplete block designs (BIBDs). We show that the approximation error for these codes depends only on the number of stragglers, and thus, adversarial straggler selection has no advantage over random selection. In addition, the proposed codes admit computationally efficient decoding at the server. Next, to characterize fundamental limits of adversarial straggling, we consider the notion of 'adversarial threshold' -- the smallest number of workers that an adversary must straggle to inflict certain approximation error. We compute a lower bound on the adversarial threshold, and show that codes based on symmetric BIBDs maximize this lower bound among a wide class of codes, making them excellent candidates for mitigating adversarial stragglers.</summary>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-30T17:13:32Z</published>\n",
      "    <arxiv:comment>Shorter version accepted in 2019 IEEE International Symposium on Information Theory (ISIT)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IT\"/>\n",
      "    <author>\n",
      "      <name>Swanand Kadhe</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>O. Ozan Koyluoglu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kannan Ramchandran</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1905.00076v3</id>\n",
      "    <title>Ensemble Distribution Distillation</title>\n",
      "    <updated>2019-11-25T21:27:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1905.00076v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1905.00076v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Ensembles of models often yield improvements in system performance. These ensemble approaches have also been empirically shown to yield robust measures of uncertainty, and are capable of distinguishing between different \\emph{forms} of uncertainty. However, ensembles come at a computational and memory cost which may be prohibitive for many applications. There has been significant work done on the distillation of an ensemble into a single model. Such approaches decrease computational cost and allow a single model to achieve an accuracy comparable to that of an ensemble. However, information about the \\emph{diversity} of the ensemble, which can yield estimates of different forms of uncertainty, is lost. This work considers the novel task of \\emph{Ensemble Distribution Distillation} (EnD$^2$) --- distilling the distribution of the predictions from an ensemble, rather than just the average prediction, into a single model. EnD$^2$ enables a single model to retain both the improved classification performance of ensemble distillation as well as information about the diversity of the ensemble, which is useful for uncertainty estimation. A solution for EnD$^2$ based on Prior Networks, a class of models which allow a single neural network to explicitly model a distribution over output distributions, is proposed in this work. The properties of EnD$^2$ are investigated on both an artificial dataset, and on the CIFAR-10, CIFAR-100 and TinyImageNet datasets, where it is shown that EnD$^2$ can approach the classification performance of an ensemble, and outperforms both standard DNNs and Ensemble Distillation on the tasks of misclassification and out-of-distribution input detection.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-04-30T19:46:28Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Andrey Malinin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bruno Mlodozeniec</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mark Gales</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.00825v1</id>\n",
      "    <title>Self-supervised Body Image Acquisition Using a Deep Neural Network for Sensorimotor Prediction</title>\n",
      "    <updated>2019-06-03T14:10:17Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.00825v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.00825v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This work investigates how a naive agent can acquire its own body image in a self-supervised way, based on the predictability of its sensorimotor experience. Our working hypothesis is that, due to its temporal stability, an agent's body produces more consistent sensory experiences than the environment, which exhibits a greater variability. Given its motor experience, an agent can thus reliably predict what appearance its body should have. This intrinsic predictability can be used to automatically isolate the body image from the rest of the environment. We propose a two-branches deconvolutional neural network to predict the visual sensory state associated with an input motor state, as well as the prediction error associated with this input. We train the network on a dataset of first-person images collected with a simulated Pepper robot, and show how the network outputs can be used to automatically isolate its visible arm from the rest of the environment. Finally, the quality of the body image produced by the network is evaluated.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T14:10:17Z</published>\n",
      "    <arxiv:comment>6 pages, 7 figures, submitted to ICDL-Epirob 2019</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Alban Laflaquière</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Verena V. Hafner</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.00910v2</id>\n",
      "    <title>Learning Representations by Maximizing Mutual Information Across Views</title>\n",
      "    <updated>2019-07-08T16:41:31Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.00910v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.00910v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views -- e.g., presence of certain objects or occurrence of certain events.\n",
      "  Following our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1% accuracy on ImageNet using standard linear evaluation. This beats prior results by over 12% and concurrent results by 7%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: https://github.com/Philip-Bachman/amdim-public.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T16:24:57Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Philip Bachman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>R Devon Hjelm</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>William Buchwalter</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.00912v1</id>\n",
      "    <title>Temporal Density Extrapolation using a Dynamic Basis Approach</title>\n",
      "    <updated>2019-06-03T16:29:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.00912v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.00912v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Density estimation is a versatile technique underlying many data mining tasks and techniques,ranging from exploration and presentation of static data, to probabilistic classification, or identifying changes or irregularities in streaming data. With the pervasiveness of embedded systems and digitisation, this latter type of streaming and evolving data becomes more important. Nevertheless, research in density estimation has so far focused on stationary data, leaving the task of of extrapolating and predicting density at time points outside a training window an open problem. For this task, Temporal Density Extrapolation (TDX) is proposed. This novel method models and predicts gradual monotonous changes in a distribution. It is based on the expansion of basis functions, whose weights are modelled as functions of compositional data over time by using an isometric log-ratio transformation. Extrapolated density estimates are then obtained by extrapolating the weights to the requested time point, and querying the density from the basis functions with back-transformed weights. Our approach aims for broad applicability by neither being restricted to a specific parametric distribution, nor relying on cluster structure in the data.It requires only two additional extrapolation-specific parameters, for which reasonable defaults exist. Experimental evaluation on various data streams, synthetic as well as from the real-world domains of credit scoring and environmental health, shows that the model manages to capture monotonous drift patterns accurately and better than existing methods. Thereby, it requires not more than 1.5-times the run time of a corresponding static density estimation approach.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T16:29:25Z</published>\n",
      "    <arxiv:comment>Accepted for publication in Data Mining and Knowledge Discovery, Special issue of the ECML/PKDD 2019 Journal Track, Springer, 2019</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>Data Mining and Knowledge Discovery 2019</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Georg Krempl</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dominik Lang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vera Hofer</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.00951v1</id>\n",
      "    <title>User Traffic Prediction for Proactive Resource Management: Learning-Powered Approaches</title>\n",
      "    <updated>2019-05-09T13:56:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.00951v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.00951v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Traffic prediction plays a vital role in efficient planning and usage of network resources in wireless networks. While traffic prediction in wired networks is an established field, there is a lack of research on the analysis of traffic in cellular networks, especially in a content-blind manner at the user level. Here, we shed light into this problem by designing traffic prediction tools that employ either statistical, rule-based, or deep machine learning methods. First, we present an extensive experimental evaluation of the designed tools over a real traffic dataset. Within this analysis, the impact of different parameters, such as length of prediction, feature set used in analyses, and granularity of data, on accuracy of prediction are investigated. Second, regarding the coupling observed between behavior of traffic and its generating application, we extend our analysis to the blind classification of applications generating the traffic based on the statistics of traffic arrival/departure. The results demonstrate presence of a threshold number of previous observations, beyond which, deep machine learning can outperform linear statistical learning, and before which, statistical learning outperforms deep learning approaches. Further analysis of this threshold value represents a strong coupling between this threshold, the length of future prediction, and the feature set in use. Finally, through a case study, we present how the experienced delay could be decreased by traffic arrival prediction.</summary>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-09T13:56:51Z</published>\n",
      "    <arxiv:comment>arXiv admin note: text overlap with arXiv:1906.00939</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.NI\"/>\n",
      "    <author>\n",
      "      <name>Amin Azari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Panagiotis Papapetrou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stojan Denic</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gunnar Peters</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01026v2</id>\n",
      "    <title>NodeDrop: A Condition for Reducing Network Size without Effect on Output</title>\n",
      "    <updated>2019-06-12T13:22:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01026v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01026v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Determining an appropriate number of features for each layer in a neural network is an important and difficult task. This task is especially important in applications on systems with limited memory or processing power. Many current approaches to reduce network size either utilize iterative procedures, which can extend training time significantly, or require very careful tuning of algorithm parameters to achieve reasonable results. In this paper we propose NodeDrop, a new method for eliminating features in a network. With NodeDrop, we define a condition to identify and guarantee which nodes carry no information, and then use regularization to encourage nodes to meet this condition. We find that NodeDrop drastically reduces the number of features in a network while maintaining high performance, reducing the number of parameters by a factor of 114x for a VGG like network on CIFAR10 without a drop in accuracy.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T19:03:46Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Louis Jensen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jacob Harer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sang Chin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01040v3</id>\n",
      "    <title>A Surprising Density of Illusionable Natural Speech</title>\n",
      "    <updated>2019-08-19T05:52:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01040v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01040v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent work on adversarial examples has demonstrated that most natural inputs can be perturbed to fool even state-of-the-art machine learning systems. But does this happen for humans as well? In this work, we investigate: what fraction of natural instances of speech can be turned into \"illusions\" which either alter humans' perception or result in different people having significantly different perceptions? We first consider the McGurk effect, the phenomenon by which adding a carefully chosen video clip to the audio channel affects the viewer's perception of what is said (McGurk and MacDonald, 1976). We obtain empirical estimates that a significant fraction of both words and sentences occurring in natural speech have some susceptibility to this effect. We also learn models for predicting McGurk illusionability. Finally we demonstrate that the Yanny or Laurel auditory illusion (Pressnitzer et al., 2018) is not an isolated occurrence by generating several very different new instances. We believe that the surprising density of illusionable natural speech warrants further investigation, from the perspectives of both security and cognitive science. Supplementary videos are available at: https://www.youtube.com/playlist?list=PLaX7t1K-e_fF2iaenoKznCatm0RC37B_k.</summary>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T19:33:43Z</published>\n",
      "    <arxiv:comment>CogSci 2019</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.SD\"/>\n",
      "    <author>\n",
      "      <name>Melody Y. Guan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gregory Valiant</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01078v2</id>\n",
      "    <title>Increasing Compactness Of Deep Learning Based Speech Enhancement Models With Parameter Pruning And Quantization Techniques</title>\n",
      "    <updated>2019-07-31T18:22:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01078v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01078v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Most recent studies on deep learning based speech enhancement (SE) focused on improving denoising performance. However, successful SE applications require striking a desirable balance between denoising performance and computational cost in real scenarios. In this study, we propose a novel parameter pruning (PP) technique, which removes redundant channels in a neural network. In addition, a parameter quantization (PQ) technique was applied to reduce the size of a neural network by representing weights with fewer cluster centroids. Because the techniques are derived based on different concepts, the PP and PQ can be integrated to provide even more compact SE models. The experimental results show that the PP and PQ techniques produce a compacted SE model with a size of only 10.03% compared to that of the original model, resulting in minor performance losses of 1.43% (from 0.70 to 0.69) for STOI and 3.24% (from 1.85 to 1.79) for PESQ. The promising results suggest that the PP and PQ techniques can be used in a SE system in devices with limited storage and computation resources.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-31T04:07:20Z</published>\n",
      "    <arxiv:comment>4pages, 6 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Jyun-Yi Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cheng Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Szu-Wei Fu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chih-Ting Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shao-Yi Chien</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu Tsao</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/LSP.2019.2951950</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/LSP.2019.2951950\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01103v2</id>\n",
      "    <title>Attractive versus truncated repulsive supercooled liquids: The dynamics is encoded in the pair correlation function</title>\n",
      "    <updated>2020-01-14T11:37:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01103v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01103v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We compare glassy dynamics in two liquids that differ in the form of their interaction potentials. Both systems have the same repulsive interactions but one has also an attractive part in the potential. These two systems exhibit very different dynamics despite having nearly identical pair correlation functions. We demonstrate that a properly weighted integral of the pair correlation function, which amplifies the subtle differences between the two systems, correctly captures their dynamical differences. The weights are obtained from a standard machine learning algorithm.</summary>\n",
      "    <category term=\"cond-mat.stat-mech\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cond-mat.soft\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T22:23:16Z</published>\n",
      "    <arxiv:comment>6 pages, 5 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cond-mat.stat-mech\"/>\n",
      "    <arxiv:journal_ref>Phys. Rev. E 101, 010602 (2020)</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>François P. Landes</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Giulio Biroli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Olivier Dauchot</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrea J. Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David R. Reichman</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1103/PhysRevE.101.010602</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1103/PhysRevE.101.010602\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01115v3</id>\n",
      "    <title>Convergence Rate of $\\mathcal{O}(1/k)$ for Optimistic Gradient and Extra-gradient Methods in Smooth Convex-Concave Saddle Point Problems</title>\n",
      "    <updated>2020-09-29T17:43:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01115v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01115v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study the iteration complexity of the optimistic gradient descent-ascent (OGDA) method and the extra-gradient (EG) method for finding a saddle point of a convex-concave unconstrained min-max problem. To do so, we first show that both OGDA and EG can be interpreted as approximate variants of the proximal point method. This is similar to the approach taken in [Nemirovski, 2004] which analyzes EG as an approximation of the `conceptual mirror prox'. In this paper, we highlight how gradients used in OGDA and EG try to approximate the gradient of the Proximal Point method. We then exploit this interpretation to show that both algorithms produce iterates that remain within a bounded set. We further show that the primal dual gap of the averaged iterates generated by both of these algorithms converge with a rate of $\\mathcal{O}(1/k)$. Our theoretical analysis is of interest as it provides a the first convergence rate estimate for OGDA in the general convex-concave setting. Moreover, it provides a simple convergence analysis for the EG algorithm in terms of function value without using compactness assumption.</summary>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T22:54:41Z</published>\n",
      "    <arxiv:comment>19 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"math.OC\"/>\n",
      "    <author>\n",
      "      <name>Aryan Mokhtari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Asuman Ozdaglar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sarath Pattathil</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01119v1</id>\n",
      "    <title>Analysis and Improvement of Adversarial Training in DQN Agents With Adversarially-Guided Exploration (AGE)</title>\n",
      "    <updated>2019-06-03T23:31:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01119v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01119v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper investigates the effectiveness of adversarial training in enhancing the robustness of Deep Q-Network (DQN) policies to state-space perturbations. We first present a formal analysis of adversarial training in DQN agents and its performance with respect to the proportion of adversarial perturbations to nominal observations used for training. Next, we consider the sample-inefficiency of current adversarial training techniques, and propose a novel Adversarially-Guided Exploration (AGE) mechanism based on a modified hybrid of the $ε$-greedy algorithm and Boltzmann exploration. We verify the feasibility of this exploration mechanism through experimental evaluation of its performance in comparison with the traditional decaying $ε$-greedy and parameter-space noise exploration algorithms.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T23:31:25Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Vahid Behzadan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>William Hsu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01121v1</id>\n",
      "    <title>Adversarial Exploitation of Policy Imitation</title>\n",
      "    <updated>2019-06-03T23:38:33Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01121v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01121v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper investigates a class of attacks targeting the confidentiality aspect of security in Deep Reinforcement Learning (DRL) policies. Recent research have established the vulnerability of supervised machine learning models (e.g., classifiers) to model extraction attacks. Such attacks leverage the loosely-restricted ability of the attacker to iteratively query the model for labels, thereby allowing for the forging of a labeled dataset which can be used to train a replica of the original model. In this work, we demonstrate the feasibility of exploiting imitation learning techniques in launching model extraction attacks on DRL agents. Furthermore, we develop proof-of-concept attacks that leverage such techniques for black-box attacks against the integrity of DRL policies. We also present a discussion on potential solution concepts for mitigation techniques.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T23:38:33Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Vahid Behzadan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>William Hsu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01126v1</id>\n",
      "    <title>Sequential Triggers for Watermarking of Deep Reinforcement Learning Policies</title>\n",
      "    <updated>2019-06-03T23:42:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01126v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01126v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper proposes a novel scheme for the watermarking of Deep Reinforcement Learning (DRL) policies. This scheme provides a mechanism for the integration of a unique identifier within the policy in the form of its response to a designated sequence of state transitions, while incurring minimal impact on the nominal performance of the policy. The applications of this watermarking scheme include detection of unauthorized replications of proprietary policies, as well as enabling the graceful interruption or termination of DRL activities by authorized entities. We demonstrate the feasibility of our proposal via experimental evaluation of watermarking a DQN policy trained in the Cartpole environment.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T23:42:44Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Vahid Behzadan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>William Hsu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01131v1</id>\n",
      "    <title>Hybrid Machine Learning Forecasts for the FIFA Women's World Cup 2019</title>\n",
      "    <updated>2019-06-03T23:48:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01131v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01131v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this work, we combine two different ranking methods together with several other predictors in a joint random forest approach for the scores of soccer matches. The first ranking method is based on the bookmaker consensus, the second ranking method estimates adequate ability parameters that reflect the current strength of the teams best. The proposed combined approach is then applied to the data from the two previous FIFA Women's World Cups 2011 and 2015. Finally, based on the resulting estimates, the FIFA Women's World Cup 2019 is simulated repeatedly and winning probabilities are obtained for all teams. The model clearly favors the defending champion USA before the host France.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T23:48:30Z</published>\n",
      "    <arxiv:comment>arXiv admin note: substantial text overlap with arXiv:1806.03208</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Andreas Groll</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christophe Ley</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gunther Schauberger</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hans Van Eetvelde</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Achim Zeileis</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01139v2</id>\n",
      "    <title>On the number of variables to use in principal component regression</title>\n",
      "    <updated>2019-10-03T13:10:42Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01139v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01139v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study least squares linear regression over $N$ uncorrelated Gaussian features that are selected in order of decreasing variance. When the number of selected features $p$ is at most the sample size $n$, the estimator under consideration coincides with the principal component regression estimator; when $p&gt;n$, the estimator is the least $\\ell_2$ norm solution over the selected features. We give an average-case analysis of the out-of-sample prediction error as $p,n,N \\to \\infty$ with $p/N \\to α$ and $n/N \\to β$, for some constants $α\\in [0,1]$ and $β\\in (0,1)$. In this average-case setting, the prediction error exhibits a \"double descent\" shape as a function of $p$. We also establish conditions under which the minimum risk is achieved in the interpolating ($p&gt;n$) regime.</summary>\n",
      "    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-04T00:22:10Z</published>\n",
      "    <arxiv:primary_category term=\"math.ST\"/>\n",
      "    <author>\n",
      "      <name>Ji Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniel Hsu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01160v1</id>\n",
      "    <title>Transfer Learning with intelligent training data selection for prediction of Alzheimer's Disease</title>\n",
      "    <updated>2019-06-04T02:23:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01160v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01160v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Detection of Alzheimer's Disease (AD) from neuroimaging data such as MRI through machine learning has been a subject of intense research in recent years. Recent success of deep learning in computer vision has progressed such research further. However, common limitations with such algorithms are reliance on a large number of training images, and requirement of careful optimization of the architecture of deep networks. In this paper, we attempt solving these issues with transfer learning, where the state-of-the-art VGG architecture is initialized with pre-trained weights from large benchmark datasets consisting of natural images. The network is then fine-tuned with layer-wise tuning, where only a pre-defined group of layers are trained on MRI images. To shrink the training data size, we employ image entropy to select the most informative slices. Through experimentation on the ADNI dataset, we show that with training size of 10 to 20 times smaller than the other contemporary methods, we reach state-of-the-art performance in AD vs. NC, AD vs. MCI, and MCI vs. NC classification problems, with a 4% and a 7% increase in accuracy over the state-of-the-art for AD vs. MCI and MCI vs. NC, respectively. We also provide detailed analysis of the effect of the intelligent training data selection method, changing the training size, and changing the number of layers to be fine-tuned. Finally, we provide Class Activation Maps (CAM) that demonstrate how the proposed model focuses on discriminative image regions that are neuropathologically relevant, and can help the healthcare practitioner in interpreting the model's decision making process.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-04T02:23:59Z</published>\n",
      "    <arxiv:comment>Accepted to IEEE Access</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Naimul Mefraz Khan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marcia Hon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nabila Abraham</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01161v2</id>\n",
      "    <title>Resolving Gendered Ambiguous Pronouns with BERT</title>\n",
      "    <updated>2019-06-13T11:26:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01161v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01161v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Pronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding and a necessary component of machine translation systems, chat bots and assistants. Neural machine learning systems perform far from ideally in this task, reaching as low as 73% F1 scores on modern benchmark datasets. Moreover, they tend to perform better for masculine pronouns than for feminine ones. Thus, the problem is both challenging and important for NLP researchers and practitioners. In this project, we describe our BERT-based approach to solving the problem of gender-balanced pronoun resolution. We are able to reach 92% F1 score and a much lower gender bias on the benchmark dataset shared by Google AI Language team.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T11:10:10Z</published>\n",
      "    <arxiv:comment>accepted to 1st ACL Workshop on Gender Bias for Natural Language Processing</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Matei Ionita</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yury Kashnitsky</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ken Krige</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vladimir Larin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Denis Logvinenko</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Atanas Atanasov</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01195v1</id>\n",
      "    <title>Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</title>\n",
      "    <updated>2019-06-04T04:59:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01195v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01195v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The recent proliferation of knowledge graphs (KGs) coupled with incomplete or partial information, in the form of missing relations (links) between entities, has fueled a lot of research on knowledge base completion (also known as relation prediction). Several recent works suggest that convolutional neural network (CNN) based models generate richer and more expressive feature embeddings and hence also perform well on relation prediction. However, we observe that these KG embeddings treat triples independently and thus fail to cover the complex and hidden information that is inherently implicit in the local neighborhood surrounding a triple. To this effect, our paper proposes a novel attention based feature embedding that captures both entity and relation features in any given entity's neighborhood. Additionally, we also encapsulate relation clusters and multihop relations in our model. Our empirical study offers insights into the efficacy of our attention based model and we show marked performance gains in comparison to state of the art methods on all datasets.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-04T04:59:08Z</published>\n",
      "    <arxiv:comment>accepted as long paper in ACL 2019</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Deepak Nathani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jatin Chauhan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Charu Sharma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Manohar Kaul</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01277v2</id>\n",
      "    <title>Wasserstein Weisfeiler-Lehman Graph Kernels</title>\n",
      "    <updated>2019-10-30T14:25:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01277v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01277v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Most graph kernels are an instance of the class of $\\mathcal{R}$-Convolution kernels, which measure the similarity of objects by comparing their substructures. Despite their empirical success, most graph kernels use a naive aggregation of the final set of substructures, usually a sum or average, thereby potentially discarding valuable information about the distribution of individual components. Furthermore, only a limited instance of these approaches can be extended to continuously attributed graphs. We propose a novel method that relies on the Wasserstein distance between the node feature vector distributions of two graphs, which allows to find subtler differences in data sets by considering graphs as high-dimensional objects, rather than simple means. We further propose a Weisfeiler-Lehman inspired embedding scheme for graphs with continuous node attributes and weighted edges, enhance it with the computed Wasserstein distance, and thus improve the state-of-the-art prediction performance on several graph classification tasks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.MN\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-04T08:52:47Z</published>\n",
      "    <arxiv:comment>Accepted as a Spotlight talk at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Matteo Togninalli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Elisabetta Ghisu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Felipe Llinares-López</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bastian Rieck</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Karsten Borgwardt</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01288v3</id>\n",
      "    <title>Information Competing Process for Learning Diversified Representations</title>\n",
      "    <updated>2019-11-28T04:17:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01288v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01288v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Learning representations with diversified information remains as an open problem. Towards learning diversified representations, a new approach, termed Information Competing Process (ICP), is proposed in this paper. Aiming to enrich the information carried by feature representations, ICP separates a representation into two parts with different mutual information constraints. The separated parts are forced to accomplish the downstream task independently in a competitive environment which prevents the two parts from learning what each other learned for the downstream task. Such competing parts are then combined synergistically to complete the task. By fusing representation parts learned competitively under different conditions, ICP facilitates obtaining diversified representations which contain rich information. Experiments on image classification and image reconstruction tasks demonstrate the great potential of ICP to learn discriminative and disentangled representations in both supervised and self-supervised learning settings.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-04T09:10:43Z</published>\n",
      "    <arxiv:comment>Accept as a NeurIPS 2019 paper</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jie Hu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rongrong Ji</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>ShengChuan Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaoshuai Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qixiang Ye</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chia-Wen Lin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qi Tian</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01292v1</id>\n",
      "    <title>Optimal Unsupervised Domain Translation</title>\n",
      "    <updated>2019-06-04T09:19:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01292v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01292v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Domain Translation is the problem of finding a meaningful correspondence between two domains. Since in a majority of settings paired supervision is not available, much work focuses on Unsupervised Domain Translation (UDT) where data samples from each domain are unpaired. Following the seminal work of CycleGAN for UDT, many variants and extensions of this model have been proposed. However, there is still little theoretical understanding behind their success. We observe that these methods yield solutions which are approximately minimal w.r.t. a given transportation cost, leading us to reformulate the problem in the Optimal Transport (OT) framework. This viewpoint gives us a new perspective on Unsupervised Domain Translation and allows us to prove the existence and uniqueness of the retrieved mapping, given a large family of transport costs. We then propose a novel framework to efficiently compute optimal mappings in a dynamical setting. We show that it generalizes previous methods and enables a more explicit control over the computed optimal mapping. It also provides smooth interpolations between the two domains. Experiments on toy and real world datasets illustrate the behavior of our method.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-04T09:19:47Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Emmanuel de Bézenac</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ibrahim Ayed</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Patrick Gallinari</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01374v1</id>\n",
      "    <title>Autonomous Reinforcement Learning of Multiple Interrelated Tasks</title>\n",
      "    <updated>2019-06-04T12:30:04Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01374v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01374v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Autonomous multiple tasks learning is a fundamental capability to develop versatile artificial agents that can act in complex environments. In real-world scenarios, tasks may be interrelated (or \"hierarchical\") so that a robot has to first learn to achieve some of them to set the preconditions for learning other ones. Even though different strategies have been used in robotics to tackle the acquisition of interrelated tasks, in particular within the developmental robotics framework, autonomous learning in this kind of scenarios is still an open question. Building on previous research in the framework of intrinsically motivated open-ended learning, in this work we describe how this question can be addressed working on the level of task selection, in particular considering the multiple interrelated tasks scenario as an MDP where the system is trying to maximise its competence over all the tasks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-04T12:30:04Z</published>\n",
      "    <arxiv:comment>Accepted to \"The 9th Joint IEEE International Conference on Development and Learning and on Epigenetic Robotics\" (ICDL-EpiRob2019). arXiv admin note: substantial text overlap with arXiv:1905.02690</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Vieri Giuliano Santucci</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gianluca Baldassarre</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Emilio Cartoni</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01600v2</id>\n",
      "    <title>Nemesyst: A Hybrid Parallelism Deep Learning-Based Framework Applied for Internet of Things Enabled Food Retailing Refrigeration Systems</title>\n",
      "    <updated>2019-09-03T22:55:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01600v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01600v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep Learning has attracted considerable attention across multiple application domains, including computer vision, signal processing and natural language processing. Although quite a few single node deep learning frameworks exist, such as tensorflow, pytorch and keras, we still lack a complete processing structure that can accommodate large scale data processing, version control, and deployment, all while staying agnostic of any specific single node framework. To bridge this gap, this paper proposes a new, higher level framework, i.e. Nemesyst, which uses databases along with model sequentialisation to allow processes to be fed unique and transformed data at the point of need. This facilitates near real-time application and makes models available for further training or use at any node that has access to the database simultaneously. Nemesyst is well suited as an application framework for internet of things aggregated control systems, deploying deep learning techniques to optimise individual machines in massive networks. To demonstrate this framework, we adopted a case study in a novel domain; deploying deep learning to optimise the high speed control of electrical power consumed by a massive internet of things network of retail refrigeration systems in proportion to load available on the UK National Grid (a demand side response). The case study demonstrated for the first time in such a setting how deep learning models, such as Recurrent Neural Networks (vanilla and Long-Short-Term Memory) and Generative Adversarial Networks paired with Nemesyst, achieve compelling performance, whilst still being malleable to future adjustments as both the data and requirements inevitably change over time.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-04T17:23:09Z</published>\n",
      "    <arxiv:comment>25 pages, 13 figures, 4 tables, 2 appendices</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>Computers in Industry, 2019</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>George Onoufriou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ronald Bickerton</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Simon Pearson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Georgios Leontidis</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1016/j.compind.2019.103133</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1016/j.compind.2019.103133\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01629v3</id>\n",
      "    <title>Exact Combinatorial Optimization with Graph Convolutional Neural Networks</title>\n",
      "    <updated>2019-10-30T15:21:45Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01629v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01629v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Combinatorial optimization problems are typically tackled by the branch-and-bound paradigm. We propose a new graph convolutional neural network model for learning branch-and-bound variable selection policies, which leverages the natural variable-constraint bipartite graph representation of mixed-integer linear programs. We train our model via imitation learning from the strong branching expert rule, and demonstrate on a series of hard problems that our approach produces policies that improve upon state-of-the-art machine-learning methods for branching and generalize to instances significantly larger than seen during training. Moreover, we improve for the first time over expert-designed branching rules implemented in a state-of-the-art solver on large problems. Code for reproducing all the experiments can be found at https://github.com/ds4dm/learn2branch.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-04T17:59:40Z</published>\n",
      "    <arxiv:comment>Accepted paper at the NeurIPS 2019 conference</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Maxime Gasse</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Didier Chételat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nicola Ferroni</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Laurent Charlin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrea Lodi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01668v1</id>\n",
      "    <title>Neuromorphic Architecture Optimization for Task-Specific Dynamic Learning</title>\n",
      "    <updated>2019-06-04T18:20:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01668v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01668v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The ability to learn and adapt in real time is a central feature of biological systems. Neuromorphic architectures demonstrating such versatility can greatly enhance our ability to efficiently process information at the edge. A key challenge, however, is to understand which learning rules are best suited for specific tasks and how the relevant hyperparameters can be fine-tuned. In this work, we introduce a conceptual framework in which the learning process is integrated into the network itself. This allows us to cast meta-learning as a mathematical optimization problem. We employ DeepHyper, a scalable, asynchronous model-based search, to simultaneously optimize the choice of meta-learning rules and their hyperparameters. We demonstrate our approach with two different datasets, MNIST and FashionMNIST, using a network architecture inspired by the learning center of the insect brain. Our results show that optimal learning rules can be dataset-dependent even within similar tasks. This dependency demonstrates the importance of introducing versatility and flexibility in the learning algorithms. It also illuminates experimental findings in insect neuroscience that have shown a heterogeneity of learning rules within the insect mushroom body.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-04T18:20:23Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>Proceedings of the International Conference on Neuromorphic Systems 2019. ACM, New York, NY, USA, Article 5, 5 pages</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Sandeep Madireddy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Angel Yanguas-Gil</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Prasanna Balaprakash</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3354265.3354270</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3354265.3354270\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01726v1</id>\n",
      "    <title>An Introduction to a New Text Classification and Visualization for Natural Language Processing Using Topological Data Analysis</title>\n",
      "    <updated>2019-06-03T08:06:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01726v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01726v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Topological Data Analysis (TDA) is a novel new and fast growing field of data science providing a set of new topological and geometric tools to derive relevant features out of complex high-dimensional data. In this paper we apply two of best methods in topological data analysis, \"Persistent Homology\" and \"Mapper\", in order to classify persian poems which has been composed by two of the best Iranian poets namely \"Ferdowsi\" and \"Hafez\". This article has two main parts, in the first part we explain the mathematics behind these two methods which is easy to understand for general audience and in the second part we describe our models and the results of applying TDA tools to NLP.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.AT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T08:06:39Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Naiereh Elyasi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mehdi Hosseini Moghadam</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.00495v1</id>\n",
      "    <title>Truncated Cauchy Non-negative Matrix Factorization</title>\n",
      "    <updated>2019-06-02T22:21:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.00495v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.00495v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Non-negative matrix factorization (NMF) minimizes the Euclidean distance between the data matrix and its low rank approximation, and it fails when applied to corrupted data because the loss function is sensitive to outliers. In this paper, we propose a Truncated CauchyNMF loss that handle outliers by truncating large errors, and develop a Truncated CauchyNMF to robustly learn the subspace on noisy datasets contaminated by outliers. We theoretically analyze the robustness of Truncated CauchyNMF comparing with the competing models and theoretically prove that Truncated CauchyNMF has a generalization bound which converges at a rate of order $O(\\sqrt{{\\ln n}/{n}})$, where $n$ is the sample size. We evaluate Truncated CauchyNMF by image clustering on both simulated and real datasets. The experimental results on the datasets containing gross corruptions validate the effectiveness and robustness of Truncated CauchyNMF for learning robust subspaces.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-02T22:21:30Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE T-PAMI), vol. 41, no. 1, pp. 246-259, Jan. 2019</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Naiyang Guan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tongliang Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yangmuzi Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dacheng Tao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Larry S. Davis</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/TPAMI.2017.2777841</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/TPAMI.2017.2777841\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.00532v2</id>\n",
      "    <title>Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model</title>\n",
      "    <updated>2019-06-07T16:27:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.00532v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.00532v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this work, we quantize a trained Transformer machine language translation model leveraging INT8/VNNI instructions in the latest Intel$^\\circledR$ Xeon$^\\circledR$ Cascade Lake processors to improve inference performance while maintaining less than 0.5$\\%$ drop in accuracy. To the best of our knowledge, this is the first attempt in the industry to quantize the Transformer model. This has high impact as it clearly demonstrates the various complexities of quantizing the language translation model. We present novel quantization techniques directly in TensorFlow to opportunistically replace 32-bit floating point (FP32) computations with 8-bit integers (INT8) and transform the FP32 computational graph. We also present a bin-packing parallel batching technique to maximize CPU utilization. Overall, our optimizations with INT8/VNNI deliver 1.5X improvement over the best FP32 performance. Furthermore, it reveals the opportunities and challenges to boost performance of quantized deep learning inference and establishes best practices to run inference with high efficiency on Intel CPUs.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T02:29:22Z</published>\n",
      "    <arxiv:comment>To appear at the Joint Workshop on On-Device Machine Learning &amp; Compact Deep Neural Network Representations, 36th International Conference on Machine Learning, Long Beach, California, 2019</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Aishwarya Bhandare</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vamsi Sripathi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Deepthi Karkada</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vivek Menon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sun Choi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kushal Datta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vikram Saletore</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.00537v1</id>\n",
      "    <title>Incorporating Biological Knowledge with Factor Graph Neural Network for Interpretable Deep Learning</title>\n",
      "    <updated>2019-06-03T02:51:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.00537v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.00537v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>While deep learning has achieved great success in many fields, one common criticism about deep learning is its lack of interpretability. In most cases, the hidden units in a deep neural network do not have a clear semantic meaning or correspond to any physical entities. However, model interpretability and explainability are crucial in many biomedical applications. To address this challenge, we developed the Factor Graph Neural Network model that is interpretable and predictable by combining probabilistic graphical models with deep learning. We directly encode biological knowledge such as Gene Ontology as a factor graph into the model architecture, making the model transparent and interpretable. Furthermore, we devised an attention mechanism that can capture multi-scale hierarchical interactions among biological entities such as genes and Gene Ontology terms. With parameter sharing mechanism, the unrolled Factor Graph Neural Network model can be trained with stochastic depth and generalize well. We applied our model to two cancer genomic datasets to predict target clinical variables and achieved better results than other traditional machine learning and deep learning models. Our model can also be used for gene set enrichment analysis and selecting Gene Ontology terms that are important to target clinical variables.</summary>\n",
      "    <category term=\"q-bio.GN\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T02:51:40Z</published>\n",
      "    <arxiv:comment>12 pages, 4 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"q-bio.GN\"/>\n",
      "    <author>\n",
      "      <name>Tianle Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aidong Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.00555v2</id>\n",
      "    <title>Adversarially Robust Generalization Just Requires More Unlabeled Data</title>\n",
      "    <updated>2019-09-26T01:44:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.00555v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.00555v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Neural network robustness has recently been highlighted by the existence of adversarial examples. Many previous works show that the learned networks do not perform well on perturbed test data, and significantly more labeled data is required to achieve adversarially robust generalization. In this paper, we theoretically and empirically show that with just more unlabeled data, we can learn a model with better adversarially robust generalization. The key insight of our results is based on a risk decomposition theorem, in which the expected robust risk is separated into two parts: the stability part which measures the prediction stability in the presence of perturbations, and the accuracy part which evaluates the standard classification accuracy. As the stability part does not depend on any label information, we can optimize this part using unlabeled data. We further prove that for a specific Gaussian mixture problem, adversarially robust generalization can be almost as easy as the standard generalization in supervised learning if a sufficiently large amount of unlabeled data is provided. Inspired by the theoretical findings, we further show that a practical adversarial training algorithm that leverages unlabeled data can improve adversarial robust generalization on MNIST and Cifar-10.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T03:56:41Z</published>\n",
      "    <arxiv:comment>16 pages. Submitted to ICLR 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Runtian Zhai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tianle Cai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Di He</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chen Dan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kun He</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>John Hopcroft</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liwei Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.00616v4</id>\n",
      "    <title>Domain Adaptation with Optimal Transport on the Manifold of SPD matrices</title>\n",
      "    <updated>2020-07-27T16:21:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.00616v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.00616v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we address the problem of Domain Adaptation (DA) using Optimal Transport (OT) on Riemannian manifolds. We model the difference between two domains by a diffeomorphism and use the polar factorization theorem to claim that OT is indeed optimal for DA in a well-defined sense, up to a volume preserving map. We then focus on the manifold of Symmetric and Positive-Definite (SPD) matrices, whose structure provided a useful context in recent applications. We demonstrate the polar factorization theorem on this manifold. Due to the uniqueness of the weighted Riemannian mean, and by exploiting existing regularized OT algorithms, we formulate a simple algorithm that maps the source domain to the target domain. We test our algorithm on two Brain-Computer Interface (BCI) data sets and observe state of the art performance.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T07:54:54Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Or Yair</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Felix Dietrich</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ronen Talmon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ioannis G. Kevrekidis</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.00618v1</id>\n",
      "    <title>A Direct $\\tilde{O}(1/ε)$ Iteration Parallel Algorithm for Optimal Transport</title>\n",
      "    <updated>2019-06-03T07:58:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.00618v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.00618v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Optimal transportation, or computing the Wasserstein or ``earth mover's'' distance between two distributions, is a fundamental primitive which arises in many learning and statistical settings. We give an algorithm which solves this problem to additive $ε$ with $\\tilde{O}(1/ε)$ parallel depth, and $\\tilde{O}\\left(n^2/ε\\right)$ work. Barring a breakthrough on a long-standing algorithmic open problem, this is optimal for first-order methods. Blanchet et. al. '18, Quanrud '19 obtained similar runtimes through reductions to positive linear programming and matrix scaling. However, these reduction-based algorithms use complicated subroutines which may be deemed impractical due to requiring solvers for second-order iterations (matrix scaling) or non-parallelizability (positive LP). The fastest practical algorithms run in time $\\tilde{O}(\\min(n^2 / ε^2, n^{2.5} / ε))$ (Dvurechensky et. al. '18, Lin et. al. '19). We bridge this gap by providing a parallel, first-order, $\\tilde{O}(1/ε)$ iteration algorithm without worse dependence on dimension, and provide preliminary experimental evidence that our algorithm may enjoy improved practical performance. We obtain this runtime via a primal-dual extragradient method, motivated by recent theoretical improvements to maximum flow (Sherman '17).</summary>\n",
      "    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T07:58:09Z</published>\n",
      "    <arxiv:comment>23 pages, 2 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.DS\"/>\n",
      "    <author>\n",
      "      <name>Arun Jambulapati</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aaron Sidford</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kevin Tian</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.00625v1</id>\n",
      "    <title>Decentralized Deep Reinforcement Learning for Delay-Power Tradeoff in Vehicular Communications</title>\n",
      "    <updated>2019-06-03T08:21:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.00625v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.00625v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper targets at the problem of radio resource management for expected long-term delay-power tradeoff in vehicular communications. At each decision epoch, the road side unit observes the global network state, allocates channels and schedules data packets for all vehicle user equipment-pairs (VUE-pairs). The decision-making procedure is modelled as a discrete-time Markov decision process (MDP). The technical challenges in solving an optimal control policy originate from highly spatial mobility of vehicles and temporal variations in data traffic. To simplify the decision-making process, we first decompose the MDP into a series of per-VUE-pair MDPs. We then propose an online long short-term memory based deep reinforcement learning algorithm to break the curse of high dimensionality in state space faced by each per-VUE-pair MDP. With the proposed algorithm, the optimal channel allocation and packet scheduling decision at each epoch can be made in a decentralized way in accordance with the partial observations of the global network state at the VUE-pairs. Numerical simulations validate the theoretical analysis and show the effectiveness of the proposed online learning algorithm.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T08:21:01Z</published>\n",
      "    <arxiv:comment>Accepted to Proc. IEEE ICC, Shanghai, China, May 2019</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Xianfu Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Celimuge Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Honggang Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yan Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mehdi Bennis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Heli Vuojala</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.00628v2</id>\n",
      "    <title>Fast and Stable Interval Bounds Propagation for Training Verifiably Robust Models</title>\n",
      "    <updated>2019-07-03T06:28:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.00628v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.00628v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present an efficient technique, which allows to train classification networks which are verifiably robust against norm-bounded adversarial attacks. This framework is built upon the work of Gowal et al., who applies the interval arithmetic to bound the activations at each layer and keeps the prediction invariant to the input perturbation. While that method is faster than competitive approaches, it requires careful tuning of hyper-parameters and a large number of epochs to converge. To speed up and stabilize training, we supply the cost function with an additional term, which encourages the model to keep the interval bounds at hidden layers small. Experimental results demonstrate that we can achieve comparable (or even better) results using a smaller number of training iterations, in a more stable fashion. Moreover, the proposed model is not so sensitive to the exact specification of the training process, which makes it easier to use by practitioners.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T08:25:47Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Paweł Morawiecki</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Przemysław Spurek</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marek Śmieja</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jacek Tabor</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.00629v2</id>\n",
      "    <title>Computing Valid p-values for Image Segmentation by Selective Inference</title>\n",
      "    <updated>2019-12-09T10:23:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.00629v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.00629v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Image segmentation is one of the most fundamental tasks of computer vision. In many practical applications, it is essential to properly evaluate the reliability of individual segmentation results. In this study, we propose a novel framework to provide the statistical significance of segmentation results in the form of p-values. Specifically, we consider a statistical hypothesis test for determining the difference between the object and the background regions. This problem is challenging because the difference can be deceptively large (called segmentation bias) due to the adaptation of the segmentation algorithm to the data. To overcome this difficulty, we introduce a statistical approach called selective inference, and develop a framework to compute valid p-values in which the segmentation bias is properly accounted for. Although the proposed framework is potentially applicable to various segmentation algorithms, we focus in this paper on graph cut-based and threshold-based segmentation algorithms, and develop two specific methods to compute valid p-values for the segmentation results obtained by these algorithms. We prove the theoretical validity of these two methods and demonstrate their practicality by applying them to segmentation problems for medical images.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T08:27:27Z</published>\n",
      "    <arxiv:comment>20 pages, 8 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Kosuke Tanizaki</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Noriaki Hashimoto</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu Inatsu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hidekata Hontani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ichiro Takeuchi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.00679v1</id>\n",
      "    <title>The Adversarial Machine Learning Conundrum: Can The Insecurity of ML Become The Achilles' Heel of Cognitive Networks?</title>\n",
      "    <updated>2019-06-03T10:03:31Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.00679v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.00679v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The holy grail of networking is to create \\textit{cognitive networks} that organize, manage, and drive themselves. Such a vision now seems attainable thanks in large part to the progress in the field of machine learning (ML), which has now already disrupted a number of industries and revolutionized practically all fields of research. But are the ML models foolproof and robust to security attacks to be in charge of managing the network? Unfortunately, many modern ML models are easily misled by simple and easily-crafted adversarial perturbations, which does not bode well for the future of ML-based cognitive networks unless ML vulnerabilities for the cognitive networking environment are identified, addressed, and fixed. The purpose of this article is to highlight the problem of insecure ML and to sensitize the readers to the danger of adversarial ML by showing how an easily-crafted adversarial ML example can compromise the operations of the cognitive self-driving network. In this paper, we demonstrate adversarial attacks on two simple yet representative cognitive networking applications (namely, intrusion detection and network traffic classification). We also provide some guidelines to design secure ML models for cognitive networks that are robust to adversarial attacks on the ML pipeline of cognitive networks.</summary>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-03T10:03:31Z</published>\n",
      "    <arxiv:primary_category term=\"cs.NI\"/>\n",
      "    <author>\n",
      "      <name>Muhammad Usama</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Junaid Qadir</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ala Al-Fuqaha</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mounir Hamdi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01407v1</id>\n",
      "    <title>RL4health: Crowdsourcing Reinforcement Learning for Knee Replacement Pathway Optimization</title>\n",
      "    <updated>2019-05-24T22:12:02Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01407v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01407v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Joint replacement is the most common inpatient surgical treatment in the US. We investigate the clinical pathway optimization for knee replacement, which is a sequential decision process from onset to recovery. Based on episodic claims from previous cases, we view the pathway optimization as an intelligence crowdsourcing problem and learn the optimal decision policy from data by imitating the best expert at every intermediate state. We develop a reinforcement learning-based pipeline that uses value iteration, state compression and aggregation learning, kernel representation and cross validation to predict the best treatment policy. It also provides forecast of the clinical pathway under the optimized policy. Empirical validation shows that the optimized policy reduces the overall cost by 7 percent and reduces the excessive cost premium by 33 percent.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-24T22:12:02Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Hao Lu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mengdi Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01432v1</id>\n",
      "    <title>Knowledge-augmented Column Networks: Guiding Deep Learning with Advice</title>\n",
      "    <updated>2019-05-31T21:09:21Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01432v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01432v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recently, deep models have had considerable success in several tasks, especially with low-level representations. However, effective learning from sparse noisy samples is a major challenge in most deep models, especially in domains with structured representations. Inspired by the proven success of human guided machine learning, we propose Knowledge-augmented Column Networks, a relational deep learning framework that leverages human advice/knowledge to learn better models in presence of sparsity and systematic noise.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-31T21:09:21Z</published>\n",
      "    <arxiv:comment>Presented at 2019 ICML Workshop on Human in the Loop Learning (HILL 2019), Long Beach, USA. arXiv admin note: substantial text overlap with arXiv:1904.06950</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Mayukh Das</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Devendra Singh Dhami</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yang Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gautam Kunapuli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sriraam Natarajan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01470v3</id>\n",
      "    <title>Options as responses: Grounding behavioural hierarchies in multi-agent RL</title>\n",
      "    <updated>2020-07-10T13:31:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01470v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01470v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper investigates generalisation in multi-agent games, where the generality of the agent can be evaluated by playing against opponents it hasn't seen during training. We propose two new games with concealed information and complex, non-transitive reward structure (think rock/paper/scissors). It turns out that most current deep reinforcement learning methods fail to efficiently explore the strategy space, thus learning policies that generalise poorly to unseen opponents. We then propose a novel hierarchical agent architecture, where the hierarchy is grounded in the game-theoretic structure of the game -- the top level chooses strategic responses to opponents, while the low level implements them into policy over primitive actions. This grounding facilitates credit assignment across the levels of hierarchy. Our experiments show that the proposed hierarchical agent is capable of generalisation to unseen opponents, while conventional baselines fail to generalise whatsoever.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-04T14:18:47Z</published>\n",
      "    <arxiv:comment>First two authors contributed equally</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>International Conference on Machine Learning 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Alexander Sasha Vezhnevets</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuhuai Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Remi Leblond</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joel Z. Leibo</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01504v1</id>\n",
      "    <title>Embedded hyper-parameter tuning by Simulated Annealing</title>\n",
      "    <updated>2019-06-04T15:14:36Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01504v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01504v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose a new metaheuristic training scheme that combines Stochastic Gradient Descent (SGD) and Discrete Optimization in an unconventional way. Our idea is to define a discrete neighborhood of the current SGD point containing a number of \"potentially good moves\" that exploit gradient information, and to search this neighborhood by using a classical metaheuristic scheme borrowed from Discrete Optimization.\n",
      "  In the present paper we investigate the use of a simple Simulated Annealing (SA) metaheuristic that accepts/rejects a candidate new solution in the neighborhood with a probability that depends both on the new solution quality and on a parameter (the temperature) which is modified over time to lower the probability of accepting worsening moves. We use this scheme as an automatic way to perform hyper-parameter tuning, hence the title of the paper. A distinctive feature of our scheme is that hyper-parameters are modified within a single SGD execution (and not in an external loop, as customary) and evaluated on the fly on the current minibatch, i.e., their tuning is fully embedded within the SGD algorithm.\n",
      "  The use of SA for training is not new, but previous proposals were mainly intended for non-differentiable objective functions for which SGD is not applied due to the lack of gradients. On the contrary, our SA method requires differentiability of (a proxy of) the loss function, and leverages on the availability of a gradient direction to define local moves that have a large probability to improve the current solution.\n",
      "  Computational results on image classification (CIFAR-10) are reported, showing that the proposed approach leads to an improvement of the final validation accuracy for modern Deep Neural Networks such as ResNet34 and VGG16.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-04T15:14:36Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Matteo Fischetti</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matteo Stringher</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01510v1</id>\n",
      "    <title>Accelerating Physics-Based Simulations Using Neural Network Proxies: An Application in Oil Reservoir Modeling</title>\n",
      "    <updated>2019-05-23T20:09:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01510v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01510v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We develop a proxy model based on deep learning methods to accelerate the simulations of oil reservoirs--by three orders of magnitude--compared to industry-strength physics-based PDE solvers. This paper describes a new architectural approach to this task, accompanied by a thorough experimental evaluation on a publicly available reservoir model. We demonstrate that in a practical setting a speedup of more than 2000X can be achieved with an average sequence error of about 10\\% relative to the oil-field simulator. The proxy model is contrasted with a high-quality physics-based acceleration baseline and is shown to outperform it by several orders of magnitude. We believe the outcomes presented here are extremely promising and offer a valuable benchmark for continuing research in oil field development optimization. Due to its domain-agnostic architecture, the presented approach can be extended to many applications beyond the field of oil and gas exploration.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-05-23T20:09:13Z</published>\n",
      "    <arxiv:comment>9 pages, submitted to FEED-2019 KDD Workshop &amp; Frontiers in Big Data</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>Front. Big Data, 20 September 2019</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Jiri Navratil</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alan King</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jesus Rios</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Georgios Kollias</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruben Torrado</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andres Codas</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.3389/fdata.2019.00033</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.3389/fdata.2019.00033\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01524v1</id>\n",
      "    <title>Text-based Editing of Talking-head Video</title>\n",
      "    <updated>2019-06-04T15:35:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01524v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01524v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Editing talking-head video to change the speech content or to remove filler words is challenging. We propose a novel method to edit talking-head video based on its transcript to produce a realistic output video in which the dialogue of the speaker has been modified, while maintaining a seamless audio-visual flow (i.e. no jump cuts). Our method automatically annotates an input talking-head video with phonemes, visemes, 3D face pose and geometry, reflectance, expression and scene illumination per frame. To edit a video, the user has to only edit the transcript, and an optimization strategy then chooses segments of the input corpus as base material. The annotated parameters corresponding to the selected segments are seamlessly stitched together and used to produce an intermediate video representation in which the lower half of the face is rendered with a parametric face model. Finally, a recurrent video generation network transforms this representation to a photorealistic video that matches the edited transcript. We demonstrate a large variety of edits, such as the addition, removal, and alteration of words, as well as convincing language translation and full sentence synthesis.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-04T15:35:16Z</published>\n",
      "    <arxiv:comment>A version with higher resolution images can be downloaded from the authors' website</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Ohad Fried</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ayush Tewari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michael Zollhöfer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Adam Finkelstein</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eli Shechtman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dan B Goldman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kyle Genova</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zeyu Jin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christian Theobalt</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maneesh Agrawala</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1906.01552v1</id>\n",
      "    <title>Assessing Disparate Impacts of Personalized Interventions: Identifiability and Bounds</title>\n",
      "    <updated>2019-06-04T16:11:07Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/1906.01552v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/1906.01552v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Personalized interventions in social services, education, and healthcare leverage individual-level causal effect predictions in order to give the best treatment to each individual or to prioritize program interventions for the individuals most likely to benefit. While the sensitivity of these domains compels us to evaluate the fairness of such policies, we show that actually auditing their disparate impacts per standard observational metrics, such as true positive rates, is impossible since ground truths are unknown. Whether our data is experimental or observational, an individual's actual outcome under an intervention different than that received can never be known, only predicted based on features. We prove how we can nonetheless point-identify these quantities under the additional assumption of monotone treatment response, which may be reasonable in many applications. We further provide a sensitivity analysis for this assumption by means of sharp partial-identification bounds under violations of monotonicity of varying strengths. We show how to use our results to audit personalized interventions using partially-identified ROC and xROC curves and demonstrate this in a case study of a French job training dataset.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"econ.EM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2019-06-04T16:11:07Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Nathan Kallus</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Angela Zhou</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.13420v2</id>\n",
      "    <title>Discretize-Optimize vs. Optimize-Discretize for Time-Series Regression and Continuous Normalizing Flows</title>\n",
      "    <updated>2020-07-31T00:28:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.13420v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.13420v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We compare the discretize-optimize (Disc-Opt) and optimize-discretize (Opt-Disc) approaches for time-series regression and continuous normalizing flows (CNFs) using neural ODEs. Neural ODEs are ordinary differential equations (ODEs) with neural network components. Training a neural ODE is an optimal control problem where the weights are the controls and the hidden features are the states. Every training iteration involves solving an ODE forward and another backward in time, which can require large amounts of computation, time, and memory. Comparing the Opt-Disc and Disc-Opt approaches in image classification tasks, Gholami et al. (2019) suggest that Disc-Opt is preferable due to the guaranteed accuracy of gradients. In this paper, we extend the comparison to neural ODEs for time-series regression and CNFs. Unlike in classification, meaningful models in these tasks must also satisfy additional requirements beyond accurate final-time output, e.g., the invertibility of the CNF. Through our numerical experiments, we demonstrate that with careful numerical treatment, Disc-Opt methods can achieve similar performance as Opt-Disc at inference with drastically reduced training costs. Disc-Opt reduced costs in six out of seven separate problems with training time reduction ranging from 39% to 97%, and in one case, Disc-Opt reduced training from nine days to less than one day.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-27T15:28:11Z</published>\n",
      "    <arxiv:comment>19 pages, 8 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Derek Onken</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lars Ruthotto</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.14621v1</id>\n",
      "    <title>Fair Classification via Unconstrained Optimization</title>\n",
      "    <updated>2020-05-21T11:29:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.14621v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.14621v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Achieving the Bayes optimal binary classification rule subject to group fairness constraints is known to be reducible, in some cases, to learning a group-wise thresholding rule over the Bayes regressor. In this paper, we extend this result by proving that, in a broader setting, the Bayes optimal fair learning rule remains a group-wise thresholding rule over the Bayes regressor but with a (possible) randomization at the thresholds. This provides a stronger justification to the post-processing approach in fair classification, in which (1) a predictor is learned first, after which (2) its output is adjusted to remove bias. We show how the post-processing rule in this two-stage approach can be learned quite efficiently by solving an unconstrained optimization problem. The proposed algorithm can be applied to any black-box machine learning model, such as deep neural networks, random forests and support vector machines. In addition, it can accommodate many fairness criteria that have been previously proposed in the literature, such as equalized odds and statistical parity. We prove that the algorithm is Bayes consistent and motivate it, furthermore, via an impossibility result that quantifies the tradeoff between accuracy and fairness across multiple demographic groups. Finally, we conclude by validating the algorithm on the Adult benchmark dataset.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-21T11:29:05Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Ibrahim Alabdulmohsin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.14627v1</id>\n",
      "    <title>Detection of Bangla Fake News using MNB and SVM Classifier</title>\n",
      "    <updated>2020-05-29T15:38:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.14627v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.14627v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Fake news has been coming into sight in significant numbers for numerous business and political reasons and has become frequent in the online world. People can get contaminated easily by these fake news for its fabricated words which have enormous effects on the offline community. Thus, interest in research in this area has risen. Significant research has been conducted on the detection of fake news from English texts and other languages but a few in Bangla Language. Our work reflects the experimental analysis on the detection of Bangla fake news from social media as this field still requires much focus. In this research work, we have used two supervised machine learning algorithms, Multinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to detect Bangla fake news with CountVectorizer and Term Frequency - Inverse Document Frequency Vectorizer as feature extraction. Our proposed framework detects fake news depending on the polarity of the corresponding article. Finally, our analysis shows SVM with the linear kernel with an accuracy of 96.64% outperform MNB with an accuracy of 93.32%.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-29T15:38:54Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Md Gulzar Hussain</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Md Rashidul Hasan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mahmuda Rahman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joy Protim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sakib Al Hasan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.14683v1</id>\n",
      "    <title>A Process for the Evaluation of Node Embedding Methods in the Context of Node Classification</title>\n",
      "    <updated>2020-05-29T17:20:19Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.14683v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.14683v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Node embedding methods find latent lower-dimensional representations which are used as features in machine learning models. In the last few years, these methods have become extremely popular as a replacement for manual feature engineering. Since authors use various approaches for the evaluation of node embedding methods, existing studies can rarely be efficiently and accurately compared. We address this issue by developing a process for a fair and objective evaluation of node embedding procedures w.r.t. node classification. This process supports researchers and practitioners to compare new and existing methods in a reproducible way. We apply this process to four popular node embedding methods and make valuable observations. With an appropriate combination of hyperparameters, good performance can be achieved even with embeddings of lower dimensions, which is positive for the run times of the downstream machine learning task and the embedding algorithm. Multiple hyperparameter combinations yield similar performance. Thus, no extensive, time-consuming search is required to achieve reasonable performance in most cases.</summary>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-29T17:20:19Z</published>\n",
      "    <arxiv:primary_category term=\"cs.SI\"/>\n",
      "    <author>\n",
      "      <name>Christoph Martin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Meike Riebeling</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.14230v1</id>\n",
      "    <title>Algorithm Selection Framework for Cyber Attack Detection</title>\n",
      "    <updated>2020-05-28T18:49:29Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.14230v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.14230v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The number of cyber threats against both wired and wireless computer systems and other components of the Internet of Things continues to increase annually. In this work, an algorithm selection framework is employed on the NSL-KDD data set and a novel paradigm of machine learning taxonomy is presented. The framework uses a combination of user input and meta-features to select the best algorithm to detect cyber attacks on a network. Performance is compared between a rule-of-thumb strategy and a meta-learning strategy. The framework removes the conjecture of the common trial-and-error algorithm selection method. The framework recommends five algorithms from the taxonomy. Both strategies recommend a high-performing algorithm, though not the best performing. The work demonstrates the close connectedness between algorithm selection and the taxonomy for which it is premised.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-28T18:49:29Z</published>\n",
      "    <arxiv:comment>6 pages, 7 figures, 1 table, accepted to WiseML '20</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Marc Chalé</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nathaniel D. Bastian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jeffery Weir</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3395352.3402623</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3395352.3402623\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.14256v1</id>\n",
      "    <title>Attention: to Better Stand on the Shoulders of Giants</title>\n",
      "    <updated>2020-05-27T00:25:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.14256v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.14256v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Science of science (SciSci) is an emerging discipline wherein science is used to study the structure and evolution of science itself using large data sets. The increasing availability of digital data on scholarly outcomes offers unprecedented opportunities to explore SciSci. In the progress of science, the previously discovered knowledge principally inspires new scientific ideas, and citation is a reasonably good reflection of this cumulative nature of scientific research. The researches that choose potentially influential references will have a lead over the emerging publications. Although the peer review process is the mainly reliable way of predicting a paper's future impact, the ability to foresee the lasting impact based on citation records is increasingly essential in the scientific impact analysis in the era of big data. This paper develops an attention mechanism for the long-term scientific impact prediction and validates the method based on a real large-scale citation data set. The results break conventional thinking. Instead of accurately simulating the original power-law distribution, emphasizing the limited attention can better stand on the shoulders of giants.</summary>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-27T00:25:51Z</published>\n",
      "    <arxiv:comment>arXiv admin note: text overlap with arXiv:1811.02117, arXiv:1811.02129</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.SI\"/>\n",
      "    <author>\n",
      "      <name>Sha Yuan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhou Shao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xingxing Wei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tong Xiao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yifan Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jie Tang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.00423v1</id>\n",
      "    <title>A New Accelerated Stochastic Gradient Method with Momentum</title>\n",
      "    <updated>2020-05-31T03:04:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.00423v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.00423v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we propose a novel accelerated stochastic gradient method with momentum, which momentum is the weighted average of previous gradients. The weights decays inverse proportionally with the iteration times. Stochastic gradient descent with momentum (Sgdm) use weights that decays exponentially with the iteration times to generate an momentum term. Using exponentially decaying weights, variants of Sgdm with well designed and complicated formats have been proposed to achieve better performance. The momentum update rules of our method is as simple as that of Sgdm. We provide theoretical convergence properties analyses for our method, which show both the exponentially decay weights and our inverse proportionally decay weights can limit the variance of the moving direction of parameters to be optimized to a region. Experimental results empirically show that our method works well with practical problems and outperforms Sgdm, and it outperforms Adam in convolutional neural networks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-31T03:04:32Z</published>\n",
      "    <arxiv:comment>10 pages, 6 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Liang Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaopeng Luo</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.00459v1</id>\n",
      "    <title>SANA : Sentiment Analysis on Newspapers comments in Algeria</title>\n",
      "    <updated>2020-05-31T08:02:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.00459v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.00459v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>It is very current in today life to seek for tracking the people opinion from their interaction with occurring events. A very common way to do that is comments in articles published in newspapers web sites dealing with contemporary events. Sentiment analysis or opinion mining is an emergent field who is the purpose is finding the behind phenomenon masked in opinionated texts. We are interested in our work by comments in Algerian newspaper websites. For this end, two corpora were used SANA and OCA. SANA corpus is created by collection of comments from three Algerian newspapers, and annotated by two Algerian Arabic native speakers, while OCA is a freely available corpus for sentiment analysis. For the classification we adopt Supports vector machines, naive Bayes and knearest neighbors. Obtained results are very promising and show the different effects of stemming in such domain, also knearest neighbors give important improvement comparing to other classifiers unlike similar works where SVM is the most dominant. From this study we observe the importance of dedicated resources and methods the newspaper comments sentiment analysis which we look forward in future works.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-31T08:02:23Z</published>\n",
      "    <arxiv:comment>9 pages, 2 figures, 12 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Hichem Rahab</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abdelhafid Zitouni</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mahieddine Djoudi</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1016/j.jksuci.2019.04.012</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1016/j.jksuci.2019.04.012\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.00475v3</id>\n",
      "    <title>Improved Regret for Zeroth-Order Adversarial Bandit Convex Optimisation</title>\n",
      "    <updated>2020-09-25T13:10:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.00475v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.00475v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We prove that the information-theoretic upper bound on the minimax regret for zeroth-order adversarial bandit convex optimisation is at most $O(d^{2.5} \\sqrt{n} \\log(n))$, where $d$ is the dimension and $n$ is the number of interactions. This improves on $O(d^{9.5} \\sqrt{n} \\log(n)^{7.5}$ by Bubeck et al. (2017). The proof is based on identifying an improved exploratory distribution for convex functions.</summary>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-31T09:22:10Z</published>\n",
      "    <arxiv:comment>To appear in Mathematical Statistics and Learning. 22 pages, 6 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"math.OC\"/>\n",
      "    <author>\n",
      "      <name>Tor Lattimore</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.00533v1</id>\n",
      "    <title>Benchmarking BioRelEx for Entity Tagging and Relation Extraction</title>\n",
      "    <updated>2020-05-31T14:45:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.00533v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.00533v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Extracting relationships and interactions between different biological entities is still an extremely challenging problem but has not received much attention as much as extraction in other generic domains. In addition to the lack of annotated data, low benchmarking is still a major reason for slow progress. In order to fill this gap, we compare multiple existing entity and relation extraction models over a recently introduced public dataset, BioRelEx of sentences annotated with biological entities and relations. Our straightforward benchmarking shows that span-based multi-task architectures like DYGIE show 4.9% and 6% absolute improvements in entity tagging and relation extraction respectively over the previous state-of-art and that incorporating domain-specific information like embeddings pre-trained over related domains boosts performance.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-31T14:45:28Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Abhinav Bhatt</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kaustubh D. Dhole</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.00727v1</id>\n",
      "    <title>Using Generative Models for Pediatric wbMRI</title>\n",
      "    <updated>2020-06-01T05:29:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.00727v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.00727v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Early detection of cancer is key to a good prognosis and requires frequent testing, especially in pediatrics. Whole-body magnetic resonance imaging (wbMRI) is an essential part of several well-established screening protocols, with screening starting in early childhood. To date, machine learning (ML) has been used on wbMRI images to stage adult cancer patients. It is not possible to use such tools in pediatrics due to the changing bone signal throughout growth, the difficulty of obtaining these images in young children due to movement and limited compliance, and the rarity of positive cases. We evaluate the quality of wbMRI images generated using generative adversarial networks (GANs) trained on wbMRI data from The Hospital for Sick Children in Toronto. We use the Frchet Inception Distance (FID) metric, Domain Frchet Distance (DFD), and blind tests with a radiology fellow for evaluation. We demonstrate that StyleGAN2 provides the best performance in generating wbMRI images with respect to all three metrics.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-01T05:29:18Z</published>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Alex Chang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vinith M. Suriyakumar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abhishek Moturu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nipaporn Tewattanarat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrea Doria</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anna Goldenberg</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.00027v1</id>\n",
      "    <title>Glaucoma Detection From Raw Circumapillary OCT Images Using Fully Convolutional Neural Networks</title>\n",
      "    <updated>2020-05-29T18:31:07Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.00027v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.00027v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Nowadays, glaucoma is the leading cause of blindness worldwide. We propose in this paper two different deep-learning-based approaches to address glaucoma detection just from raw circumpapillary OCT images. The first one is based on the development of convolutional neural networks (CNNs) trained from scratch. The second one lies in fine-tuning some of the most common state-of-the-art CNNs architectures. The experiments were performed on a private database composed of 93 glaucomatous and 156 normal B-scans around the optic nerve head of the retina, which were diagnosed by expert ophthalmologists. The validation results evidence that fine-tuned CNNs outperform the networks trained from scratch when small databases are addressed. Additionally, the VGG family of networks reports the most promising results, with an area under the ROC curve of 0.96 and an accuracy of 0.92, during the prediction of the independent test set.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-29T18:31:07Z</published>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Gabriel García</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rocío del Amor</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Adrián Colomer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Valery Naranjo</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.00052v1</id>\n",
      "    <title>Stance Prediction for Contemporary Issues: Data and Experiments</title>\n",
      "    <updated>2020-05-29T19:54:07Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.00052v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.00052v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We investigate whether pre-trained bidirectional transformers with sentiment and emotion information improve stance detection in long discussions of contemporary issues. As a part of this work, we create a novel stance detection dataset covering 419 different controversial issues and their related pros and cons collected by procon.org in nonpartisan format. Experimental results show that a shallow recurrent neural network with sentiment or emotion information can reach competitive results compared to fine-tuned BERT with 20x fewer parameters. We also use a simple approach that explains which input phrases contribute to stance detection.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-29T19:54:07Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Marjan Hosseinia</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eduard Dragut</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arjun Mukherjee</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.00067v2</id>\n",
      "    <title>Automated Measurements of Key Morphological Features of Human Embryos for IVF</title>\n",
      "    <updated>2020-07-20T21:34:27Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.00067v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.00067v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A major challenge in clinical In-Vitro Fertilization (IVF) is selecting the highest quality embryo to transfer to the patient in the hopes of achieving a pregnancy. Time-lapse microscopy provides clinicians with a wealth of information for selecting embryos. However, the resulting movies of embryos are currently analyzed manually, which is time consuming and subjective. Here, we automate feature extraction of time-lapse microscopy of human embryos with a machine-learning pipeline of five convolutional neural networks (CNNs). Our pipeline consists of (1) semantic segmentation of the regions of the embryo, (2) regression predictions of fragment severity, (3) classification of the developmental stage, and object instance segmentation of (4) cells and (5) pronuclei. Our approach greatly speeds up the measurement of quantitative, biologically relevant features that may aid in embryo selection.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.OT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-29T20:27:17Z</published>\n",
      "    <arxiv:comment>to be presented at MICCAI 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Brian D. Leahy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Won-Dong Jang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Helen Y. Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Robbert Struyven</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Donglai Wei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhe Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kylie R. Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Charlotte Royston</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liz Cam</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yael Kalma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Foad Azem</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dalit Ben-Yosef</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hanspeter Pfister</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniel Needleman</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.00112v2</id>\n",
      "    <title>Approximating the Ideal Observer for joint signal detection and localization tasks by use of supervised learning methods</title>\n",
      "    <updated>2020-07-15T02:01:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.00112v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.00112v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Medical imaging systems are commonly assessed and optimized by use of objective measures of image quality (IQ). The Ideal Observer (IO) performance has been advocated to provide a figure-of-merit for use in assessing and optimizing imaging systems because the IO sets an upper performance limit among all observers. When joint signal detection and localization tasks are considered, the IO that employs a modified generalized likelihood ratio test maximizes observer performance as characterized by the localization receiver operating characteristic (LROC) curve. Computations of likelihood ratios are analytically intractable in the majority of cases. Therefore, sampling-based methods that employ Markov-Chain Monte Carlo (MCMC) techniques have been developed to approximate the likelihood ratios. However, the applications of MCMC methods have been limited to relatively simple object models. Supervised learning-based methods that employ convolutional neural networks have been recently developed to approximate the IO for binary signal detection tasks. In this paper, the ability of supervised learning-based methods to approximate the IO for joint signal detection and localization tasks is explored. Both background-known-exactly and background-known-statistically signal detection and localization tasks are considered. The considered object models include a lumpy object model and a clustered lumpy model, and the considered measurement noise models include Laplacian noise, Gaussian noise, and mixed Poisson-Gaussian noise. The LROC curves produced by the supervised learning-based method are compared to those produced by the MCMC approach or analytical computation when feasible. The potential utility of the proposed method for computing objective measures of IQ for optimizing imaging system performance is explored.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-29T22:53:45Z</published>\n",
      "    <arxiv:comment>IEEE Transactions on Medical Imaging (Early Access), 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Weimin Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hua Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mark A. Anastasio</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/TMI.2020.3009022</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/TMI.2020.3009022\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.14419v2</id>\n",
      "    <title>Reinforcement Learning</title>\n",
      "    <updated>2020-06-13T05:19:26Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.14419v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.14419v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Reinforcement learning (RL) is a general framework for adaptive control, which has proven to be efficient in many domains, e.g., board games, video games or autonomous vehicles. In such problems, an agent faces a sequential decision-making problem where, at every time step, it observes its state, performs an action, receives a reward and moves to a new state. An RL agent learns by trial and error a good policy (or controller) based on observations and numeric reward feedback on the previously performed action. In this chapter, we present the basic framework of RL and recall the two main families of approaches that have been developed to learn a good policy. The first one, which is value-based, consists in estimating the value of an optimal policy, value from which a policy can be recovered, while the other, called policy search, directly works in a policy space. Actor-critic methods can be seen as a policy search technique where the policy value that is learned guides the policy improvement. Besides, we give an overview of some extensions of the standard RL framework, notably when risk-averse behavior needs to be taken into account or when rewards are not available or not known.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-29T06:53:29Z</published>\n",
      "    <arxiv:comment>Chapter in \"A Guided Tour of Artificial Intelligence Research\", Springer</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Olivier Buffet</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Olivier Pietquin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paul Weng</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.00217v1</id>\n",
      "    <title>Exploring Filterbank Learning for Keyword Spotting</title>\n",
      "    <updated>2020-05-30T08:11:58Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.00217v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.00217v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Despite their great performance over the years, handcrafted speech features are not necessarily optimal for any particular speech application. Consequently, with greater or lesser success, optimal filterbank learning has been studied for different speech processing tasks. In this paper, we fill in a gap by exploring filterbank learning for keyword spotting (KWS). Two approaches are examined: filterbank matrix learning in the power spectral domain and parameter learning of a psychoacoustically-motivated gammachirp filterbank. Filterbank parameters are optimized jointly with a modern deep residual neural network-based KWS back-end. Our experimental results reveal that, in general, there are no statistically significant differences, in terms of KWS accuracy, between using a learned filterbank and handcrafted speech features. Thus, while we conclude that the latter are still a wise choice when using modern KWS back-ends, we also hypothesize that this could be a symptom of information redundancy, which opens up new research possibilities in the field of small-footprint KWS.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-30T08:11:58Z</published>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Iván López-Espejo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zheng-Hua Tan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jesper Jensen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.00248v2</id>\n",
      "    <title>Modeling adult skeletal stem cell response to laser-machined topographies through deep learning</title>\n",
      "    <updated>2020-09-14T14:45:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.00248v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.00248v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The response of adult human bone marrow stromal stem cells to surface topographies generated through femtosecond laser machining can be predicted by a deep neural network. The network is capable of predicting cell response to a statistically significant level, including positioning predictions with a probability P &lt; 0.001, and therefore can be used as a model to determine the minimum line separation required for cell alignment, with implications for tissue structure development and tissue engineering. The application of a deep neural network, as a model, reduces the amount of experimental cell culture required to develop an enhanced understanding of cell behavior to topographical cues and, critically, provides rapid prediction of the effects of novel surface structures on tissue fabrication and cell signaling.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-30T12:21:17Z</published>\n",
      "    <arxiv:comment>Article accepted for publication in Tissue &amp; Cell (ISSN 0040-8166) 11th Sep 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <arxiv:journal_ref>Tissue and Cell: Volume 67, December 2020, 101442</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Benita S. Mackay</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matthew Praeger</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>James A. Grant-Jacob</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Janos Kanczler</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Robert W. Eason</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Richard O. C. Oreffo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ben Mills</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1016/j.tice.2020.101442</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1016/j.tice.2020.101442\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.00273v1</id>\n",
      "    <title>Positron Emission Tomography (PET) image enhancement using a gradient vector orientation based nonlinear diffusion filter (GVOF) for accurate quantitation of radioactivity concentration</title>\n",
      "    <updated>2020-05-30T13:57:02Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.00273v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.00273v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>To accurately quantify in vivo radiotracer uptake using Positron Emission Tomography (PET) is a challenging task due to low signal-to-noise ratio (SNR) and poor spatial resolution of PET camera along with the finite image sampling constraint. Furthermore, inter lesion variations of the SNR and contrast along with the variations in size of the lesion make the quantitation even more difficult. One of the ways to improve the quantitation is via post reconstruction filtering with Gaussian Filter (GF). Edge preserving Bilateral Filter (BF) and Nonlinear Diffusion Filter (NDF) are the alternatives to GF that can improve the SNR without degrading the image resolution. However, the performance of these edge preserving methods are only optimum for high count and low noise cases. A novel parameter free gradient vector orientation based nonlinear diffusion filter (GVOF) is proposed in this paper that is insensitive to statistical fluctuations (e. g., SNR, contrast, size etc.). GVOF method applied on the PET images collected with the NEMA phantom with varying levels of contrast and noise reveals that the GVOF method provides the highest SNR, CNR (contrast-to-noise ratio) and resolution compared to the original and other filtered images. The percentage bias in estimating the maximum activity representing SUVmax (Maximum Standardized Uptake Value) for the spheres with diameter &gt; 2cm where the partial volume effects (PVE) is negligible is the lowest for the GVOF method. The GVOF method also improves the maximum intensity reproducibility. Robustness of the GVOF against variation in sizes, contrast levels and SNR makes it a suitable post filtering method for both accurate diagnosis and response assessment. Furthermore, its capability to provide accurate quantitative measurements irrespective of the SNR, it can also be effective in reduction of radioactivity dose.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-30T13:57:02Z</published>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Mahbubunnabi Tamal</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.00283v1</id>\n",
      "    <title>Manipulating the Distributions of Experience used for Self-Play Learning in Expert Iteration</title>\n",
      "    <updated>2020-05-30T14:32:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.00283v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.00283v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Expert Iteration (ExIt) is an effective framework for learning game-playing policies from self-play. ExIt involves training a policy to mimic the search behaviour of a tree search algorithm - such as Monte-Carlo tree search - and using the trained policy to guide it. The policy and the tree search can then iteratively improve each other, through experience gathered in self-play between instances of the guided tree search algorithm. This paper outlines three different approaches for manipulating the distribution of data collected from self-play, and the procedure that samples batches for learning updates from the collected data. Firstly, samples in batches are weighted based on the durations of the episodes in which they were originally experienced. Secondly, Prioritized Experience Replay is applied within the ExIt framework, to prioritise sampling experience from which we expect to obtain valuable training signals. Thirdly, a trained exploratory policy is used to diversify the trajectories experienced in self-play. This paper summarises the effects of these manipulations on training performance evaluated in fourteen different board games. We find major improvements in early training performance in some games, and minor improvements averaged over fourteen games.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-30T14:32:46Z</published>\n",
      "    <arxiv:comment>Accepted at the IEEE Conference on Games (CoG) 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Dennis J. N. J. Soemers</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Éric Piette</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matthew Stephenson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cameron Browne</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.00757v1</id>\n",
      "    <title>Residual Squeeze-and-Excitation Network for Fast Image Deraining</title>\n",
      "    <updated>2020-06-01T07:17:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.00757v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.00757v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Image deraining is an important image processing task as rain streaks not only severely degrade the visual quality of images but also significantly affect the performance of high-level vision tasks. Traditional methods progressively remove rain streaks via different recurrent neural networks. However, these methods fail to yield plausible rain-free images in an efficient manner. In this paper, we propose a residual squeeze-and-excitation network called RSEN for fast image deraining as well as superior deraining performance compared with state-of-the-art approaches. Specifically, RSEN adopts a lightweight encoder-decoder architecture to conduct rain removal in one stage. Besides, both encoder and decoder adopt a novel residual squeeze-and-excitation block as the core of feature extraction, which contains a residual block for producing hierarchical features, followed by a squeeze-and-excitation block for channel-wisely enhancing the resulted hierarchical features. Experimental results demonstrate that our method can not only considerably reduce the computational complexity but also significantly improve the deraining performance compared with state-of-the-art methods.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-01T07:17:01Z</published>\n",
      "    <arxiv:comment>7 pages, 5 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Jun Fu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jianfeng Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kazuyuki Tasaka</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhibo Chen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05477v1</id>\n",
      "    <title>Unsupervised Paraphrase Generation using Pre-trained Language Models</title>\n",
      "    <updated>2020-06-09T19:40:19Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05477v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05477v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Large scale Pre-trained Language Models have proven to be very powerful approach in various Natural language tasks. OpenAI's GPT-2 \\cite{radford2019language} is notable for its capability to generate fluent, well formulated, grammatically consistent text and for phrase completions. In this paper we leverage this generation capability of GPT-2 to generate paraphrases without any supervision from labelled data. We examine how the results compare with other supervised and unsupervised approaches and the effect of using paraphrases for data augmentation on downstream tasks such as classification. Our experiments show that paraphrases generated with our model are of good quality, are diverse and improves the downstream task performance when used for data augmentation.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-09T19:40:19Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Chaitra Hegde</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shrikumar Patil</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05538v1</id>\n",
      "    <title>Dual-stream Maximum Self-attention Multi-instance Learning</title>\n",
      "    <updated>2020-06-09T22:44:58Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05538v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05538v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Multi-instance learning (MIL) is a form of weakly supervised learning where a single class label is assigned to a bag of instances while the instance-level labels are not available. Training classifiers to accurately determine the bag label and instance labels is a challenging but critical task in many practical scenarios, such as computational histopathology. Recently, MIL models fully parameterized by neural networks have become popular due to the high flexibility and superior performance. Most of these models rely on attention mechanisms that assign attention scores across the instance embeddings in a bag and produce the bag embedding using an aggregation operator. In this paper, we proposed a dual-stream maximum self-attention MIL model (DSMIL) parameterized by neural networks. The first stream deploys a simple MIL max-pooling while the top-activated instance embedding is determined and used to obtain self-attention scores across instance embeddings in the second stream. Different from most of the previous methods, the proposed model jointly learns an instance classifier and a bag classifier based on the same instance embeddings. The experiments results show that our method achieves superior performance compared to the best MIL methods and demonstrates state-of-the-art performance on benchmark MIL datasets.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-09T22:44:58Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Bin Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kevin W. Eliceiri</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05554v1</id>\n",
      "    <title>Causal Discovery from Incomplete Data using An Encoder and Reinforcement Learning</title>\n",
      "    <updated>2020-06-09T23:33:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05554v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05554v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Discovering causal structure among a set of variables is a fundamental problem in many domains. However, state-of-the-art methods seldom consider the possibility that the observational data has missing values (incomplete data), which is ubiquitous in many real-world situations. The missing value will significantly impair the performance and even make the causal discovery algorithms fail. In this paper, we propose an approach to discover causal structures from incomplete data by using a novel encoder and reinforcement learning (RL). The encoder is designed for missing data imputation as well as feature extraction. In particular, it learns to encode the currently available information (with missing values) into a robust feature representation which is then used to determine where to search the best graph. The encoder is integrated into a RL framework that can be optimized using the actor-critic algorithm. Our method takes the incomplete observational data as input and generates a causal structure graph. Experimental results on synthetic and real data demonstrate that our method can robustly generate causal structures from incomplete data. Compared with the direct combination of data imputation and causal discovery methods, our method performs generally better and can even obtain a performance gain as much as 43.2%.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-09T23:33:47Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xiaoshui Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fujin Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lois Holloway</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ali Haidar</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05584v1</id>\n",
      "    <title>Exploring Quality and Generalizability in Parameterized Neural Audio Effects</title>\n",
      "    <updated>2020-06-10T00:52:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05584v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05584v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep neural networks have shown promise for music audio signal processing applications, often surpassing prior approaches, particularly as end-to-end models in the waveform domain. Yet results to date have tended to be constrained by low sample rates, noise, narrow domains of signal types, and/or lack of parameterized controls (i.e. \"knobs\"), making their suitability for professional audio engineering workflows still lacking. This work expands on prior research published on modeling nonlinear time-dependent signal processing effects associated with music production by means of a deep neural network, one which includes the ability to emulate the parameterized settings you would see on an analog piece of equipment, with the goal of eventually producing commercially viable, high quality audio, i.e. 44.1 kHz sampling rate at 16-bit resolution. The results in this paper highlight progress in modeling these effects through architecture and optimization changes, towards increasing computational efficiency, lowering signal-to-noise ratio, and extending to a larger variety of nonlinear audio effects. Toward these ends, the strategies employed involved a three-pronged approach: model speed, model accuracy, and model generalizability. Most of the presented methods provide marginal or no increase in output accuracy over the original model, with the exception of dataset manipulation. We found that limiting the audio content of the dataset, for example using datasets of just a single instrument, provided a significant improvement in model accuracy over models trained on more general datasets.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-10T00:52:08Z</published>\n",
      "    <arxiv:comment>7 pages, 5 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>William Mitchell</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Scott H. Hawley</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04984v1</id>\n",
      "    <title>Making Convolutions Resilient via Algorithm-Based Error Detection Techniques</title>\n",
      "    <updated>2020-06-08T23:17:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04984v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04984v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The ability of Convolutional Neural Networks (CNNs) to accurately process real-time telemetry has boosted their use in safety-critical and high-performance computing systems. As such systems require high levels of resilience to errors, CNNs must execute correctly in the presence of hardware faults. Full duplication provides the needed assurance but incurs a prohibitive 100% overhead. Algorithmic techniques are known to offer low-cost solutions, but the practical feasibility and performance of such techniques have never been studied for CNN deployment platforms (e.g., TensorFlow or TensorRT on GPUs). In this paper, we focus on algorithmically verifying Convolutions, which are the most resource-demanding operations in CNNs. We use checksums to verify convolutions, adding a small amount of redundancy, far less than full-duplication. We first identify the challenges that arise in employing Algorithm-Based Error Detection (ABED) for Convolutions in optimized inference platforms that fuse multiple network layers and use reduced-precision operations, and demonstrate how to overcome them. We propose and evaluate variations of ABED techniques that offer implementation complexity, runtime overhead, and coverage trade-offs. Results show that ABED can detect all transient hardware errors that might otherwise corrupt output and does so while incurring low runtime overheads (6-23%), offering at least 1.6X throughput to workloads compared to full duplication.</summary>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T23:17:57Z</published>\n",
      "    <arxiv:primary_category term=\"cs.DC\"/>\n",
      "    <author>\n",
      "      <name>Siva Kumar Sastry Hari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michael B. Sullivan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Timothy Tsai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stephen W. Keckler</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04216v1</id>\n",
      "    <title>Efficient AutoML Pipeline Search with Matrix and Tensor Factorization</title>\n",
      "    <updated>2020-06-07T18:08:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04216v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04216v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Data scientists seeking a good supervised learning model on a new dataset have many choices to make: they must preprocess the data, select features, possibly reduce the dimension, select an estimation algorithm, and choose hyperparameters for each of these pipeline components. With new pipeline components comes a combinatorial explosion in the number of choices! In this work, we design a new AutoML system to address this challenge: an automated system to design a supervised learning pipeline. Our system uses matrix and tensor factorization as surrogate models to model the combinatorial pipeline search space. Under these models, we develop greedy experiment design protocols to efficiently gather information about a new dataset. Experiments on large corpora of real-world classification problems demonstrate the effectiveness of our approach.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-07T18:08:48Z</published>\n",
      "    <arxiv:comment>This is an extended version of AutoML Pipeline Selection: Efficiently Navigating the Combinatorial Space (DOI: 10.1145/3394486.3403197) at KDD 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Chengrun Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jicong Fan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ziyang Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Madeleine Udell</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04228v1</id>\n",
      "    <title>Bayesian Hidden Physics Models: Uncertainty Quantification for Discovery of Nonlinear Partial Differential Operators from Data</title>\n",
      "    <updated>2020-06-07T18:48:43Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04228v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04228v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>What do data tell us about physics-and what don't they tell us? There has been a surge of interest in using machine learning models to discover governing physical laws such as differential equations from data, but current methods lack uncertainty quantification to communicate their credibility. This work addresses this shortcoming from a Bayesian perspective. We introduce a novel model comprising \"leaf\" modules that learn to represent distinct experiments' spatiotemporal functional data as neural networks and a single \"root\" module that expresses a nonparametric distribution over their governing nonlinear differential operator as a Gaussian process. Automatic differentiation is used to compute the required partial derivatives from the leaf functions as inputs to the root. Our approach quantifies the reliability of the learned physics in terms of a posterior distribution over operators and propagates this uncertainty to solutions of novel initial-boundary value problem instances. Numerical experiments demonstrate the method on several nonlinear PDEs.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-07T18:48:43Z</published>\n",
      "    <arxiv:comment>16 pages, 8 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Steven Atkinson</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04239v3</id>\n",
      "    <title>Unsupervised Differentiable Multi-aspect Network Embedding</title>\n",
      "    <updated>2020-07-07T16:47:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04239v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04239v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Network embedding is an influential graph mining technique for representing nodes in a graph as distributed vectors. However, the majority of network embedding methods focus on learning a single vector representation for each node, which has been recently criticized for not being capable of modeling multiple aspects of a node. To capture the multiple aspects of each node, existing studies mainly rely on offline graph clustering performed prior to the actual embedding, which results in the cluster membership of each node (i.e., node aspect distribution) fixed throughout training of the embedding model. We argue that this not only makes each node always have the same aspect distribution regardless of its dynamic context, but also hinders the end-to-end training of the model that eventually leads to the final embedding quality largely dependent on the clustering. In this paper, we propose a novel end-to-end framework for multi-aspect network embedding, called asp2vec, in which the aspects of each node are dynamically assigned based on its local context. More precisely, among multiple aspects, we dynamically assign a single aspect to each node based on its current context, and our aspect selection module is end-to-end differentiable via the Gumbel-Softmax trick. We also introduce the aspect regularization framework to capture the interactions among the multiple aspects in terms of relatedness and diversity. We further demonstrate that our proposed framework can be readily extended to heterogeneous networks. Extensive experiments towards various downstream tasks on various types of homogeneous networks and a heterogeneous network demonstrate the superiority of asp2vec.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-07T19:26:20Z</published>\n",
      "    <arxiv:comment>KDD 2020 (Research Track). 9 Pages + Appendix (2 Pages). Source code can be found https://github.com/pcy1302/asp2vec. Typo fixed in Fig.2</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Chanyoung Park</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Carl Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qi Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Donghyun Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hwanjo Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiawei Han</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3394486.3403196</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3394486.3403196\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04266v2</id>\n",
      "    <title>Sparse learning with CART</title>\n",
      "    <updated>2020-11-18T21:42:04Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04266v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04266v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Decision trees with binary splits are popularly constructed using Classification and Regression Trees (CART) methodology. For regression models, this approach recursively divides the data into two near-homogenous daughter nodes according to a split point that maximizes the reduction in sum of squares error (the impurity) along a particular variable. This paper aims to study the statistical properties of regression trees constructed with CART methodology. In doing so, we find that the training error is governed by the Pearson correlation between the optimal decision stump and response data in each node, which we bound by constructing a prior distribution on the split points and solving a nonlinear optimization problem. We leverage this connection between the training error and Pearson correlation to show that CART with cost-complexity pruning achieves an optimal complexity/goodness-of-fit tradeoff when the depth scales with the logarithm of the sample size. Data dependent quantities, which adapt to the dimensionality and latent structure of the regression model, are seen to govern the rates of convergence of the prediction error.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-07T20:55:52Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Jason M. Klusowski</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04292v1</id>\n",
      "    <title>Achieving Equalized Odds by Resampling Sensitive Attributes</title>\n",
      "    <updated>2020-06-08T00:18:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04292v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04292v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present a flexible framework for learning predictive models that approximately satisfy the equalized odds notion of fairness. This is achieved by introducing a general discrepancy functional that rigorously quantifies violations of this criterion. This differentiable functional is used as a penalty driving the model parameters towards equalized odds. To rigorously evaluate fitted models, we develop a formal hypothesis test to detect whether a prediction rule violates this property, the first such test in the literature. Both the model fitting and hypothesis testing leverage a resampled version of the sensitive attribute obeying equalized odds, by construction. We demonstrate the applicability and validity of the proposed framework both in regression and multi-class classification problems, reporting improved performance over state-of-the-art methods. Lastly, we show how to incorporate techniques for equitable uncertainty quantification---unbiased for each group under study---to communicate the results of the data analysis in exact terms.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T00:18:34Z</published>\n",
      "    <arxiv:comment>14 pages, 4 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Yaniv Romano</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stephen Bates</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Emmanuel J. Candès</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04296v1</id>\n",
      "    <title>Randomised Gaussian Process Upper Confidence Bound for Bayesian Optimisation</title>\n",
      "    <updated>2020-06-08T00:28:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04296v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04296v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In order to improve the performance of Bayesian optimisation, we develop a modified Gaussian process upper confidence bound (GP-UCB) acquisition function. This is done by sampling the exploration-exploitation trade-off parameter from a distribution. We prove that this allows the expected trade-off parameter to be altered to better suit the problem without compromising a bound on the function's Bayesian regret. We also provide results showing that our method achieves better performance than GP-UCB in a range of real-world and synthetic problems.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T00:28:41Z</published>\n",
      "    <arxiv:comment>Preprint version. The final version has been accepted to appear at IJCAI20</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Julian Berk</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sunil Gupta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Santu Rana</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Svetha Venkatesh</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04331v2</id>\n",
      "    <title>Randomized Policy Learning for Continuous State and Action MDPs</title>\n",
      "    <updated>2020-11-16T01:55:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04331v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04331v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep reinforcement learning methods have achieved state-of-the-art results in a variety of challenging, high-dimensional domains ranging from video games to locomotion. The key to success has been the use of deep neural networks used to approximate the policy and value function. Yet, substantial tuning of weights is required for good results. We instead use randomized function approximation. Such networks are not only cheaper than training fully connected networks but also improve the numerical performance. We present \\texttt{RANDPOL}, a generalized policy iteration algorithm for MDPs with continuous state and action spaces. Both the policy and value functions are represented with randomized networks. We also give finite time guarantees on the performance of the algorithm. Then we show the numerical performance on challenging environments and compare them with deep neural network based algorithms.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T02:49:47Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Hiteshi Sharma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rahul Jain</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04353v1</id>\n",
      "    <title>Stable Reinforcement Learning with Unbounded State Space</title>\n",
      "    <updated>2020-06-08T05:00:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04353v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04353v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We consider the problem of reinforcement learning (RL) with unbounded state space motivated by the classical problem of scheduling in a queueing network. Traditional policies as well as error metric that are designed for finite, bounded or compact state space, require infinite samples for providing any meaningful performance guarantee (e.g. $\\ell_\\infty$ error) for unbounded state space. That is, we need a new notion of performance metric. As the main contribution of this work, inspired by the literature in queuing systems and control theory, we propose stability as the notion of \"goodness\": the state dynamics under the policy should remain in a bounded region with high probability. As a proof of concept, we propose an RL policy using Sparse-Sampling-based Monte Carlo Oracle and argue that it satisfies the stability property as long as the system dynamics under the optimal policy respects a Lyapunov function. The assumption of existence of a Lyapunov function is not restrictive as it is equivalent to the positive recurrence or stability property of any Markov chain, i.e., if there is any policy that can stabilize the system then it must possess a Lyapunov function. And, our policy does not utilize the knowledge of the specific Lyapunov function. To make our method sample efficient, we provide an improved, sample efficient Sparse-Sampling-based Monte Carlo Oracle with Lipschitz value function that may be of interest in its own right. Furthermore, we design an adaptive version of the algorithm, based on carefully constructed statistical tests, which finds the correct tuning parameter automatically.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T05:00:25Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Devavrat Shah</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qiaomin Xie</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhi Xu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04380v1</id>\n",
      "    <title>Learning the Compositional Visual Coherence for Complementary Recommendations</title>\n",
      "    <updated>2020-06-08T06:57:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04380v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04380v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Complementary recommendations, which aim at providing users product suggestions that are supplementary and compatible with their obtained items, have become a hot topic in both academia and industry in recent years. %However, it is challenging due to its complexity and subjectivity. Existing work mainly focused on modeling the co-purchased relations between two items, but the compositional associations of item collections are largely unexplored. Actually, when a user chooses the complementary items for the purchased products, it is intuitive that she will consider the visual semantic coherence (such as color collocations, texture compatibilities) in addition to global impressions. Towards this end, in this paper, we propose a novel Content Attentive Neural Network (CANN) to model the comprehensive compositional coherence on both global contents and semantic contents. Specifically, we first propose a \\textit{Global Coherence Learning} (GCL) module based on multi-heads attention to model the global compositional coherence. Then, we generate the semantic-focal representations from different semantic regions and design a \\textit{Focal Coherence Learning} (FCL) module to learn the focal compositional coherence from different semantic-focal representations. Finally, we optimize the CANN in a novel compositional optimization strategy. Extensive experiments on the large-scale real-world data clearly demonstrate the effectiveness of CANN compared with several state-of-the-art methods.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T06:57:18Z</published>\n",
      "    <arxiv:comment>Early version accepted by IJCAI2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Zhi Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bo Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qi Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Likang Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hongke Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tao Mei</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04386v1</id>\n",
      "    <title>Understanding Graph Neural Networks from Graph Signal Denoising Perspectives</title>\n",
      "    <updated>2020-06-08T07:10:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04386v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04386v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Graph neural networks (GNNs) have attracted much attention because of their excellent performance on tasks such as node classification. However, there is inadequate understanding on how and why GNNs work, especially for node representation learning. This paper aims to provide a theoretical framework to understand GNNs, specifically, spectral graph convolutional networks and graph attention networks, from graph signal denoising perspectives. Our framework shows that GNNs are implicitly solving graph signal denoising problems: spectral graph convolutions work as denoising node features, while graph attentions work as denoising edge weights. We also show that a linear self-attention mechanism is able to compete with the state-of-the-art graph attention methods. Our theoretical results further lead to two new models, GSDN-F and GSDN-EF, which work effectively for graphs with noisy node features and/or noisy edges. We validate our theoretical findings and also the effectiveness of our new models by experiments on benchmark datasets. The source code is available at \\url{https://github.com/fuguoji/GSDN}.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T07:10:39Z</published>\n",
      "    <arxiv:comment>19 pages, 8 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Guoji Fu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yifan Hou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jian Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kaili Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Barakeel Fanseu Kamhoua</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>James Cheng</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04435v1</id>\n",
      "    <title>CAST: A Correlation-based Adaptive Spectral Clustering Algorithm on Multi-scale Data</title>\n",
      "    <updated>2020-06-08T09:46:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04435v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04435v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study the problem of applying spectral clustering to cluster multi-scale data, which is data whose clusters are of various sizes and densities. Traditional spectral clustering techniques discover clusters by processing a similarity matrix that reflects the proximity of objects. For multi-scale data, distance-based similarity is not effective because objects of a sparse cluster could be far apart while those of a dense cluster have to be sufficiently close. Following [16], we solve the problem of spectral clustering on multi-scale data by integrating the concept of objects' \"reachability similarity\" with a given distance-based similarity to derive an objects' coefficient matrix. We propose the algorithm CAST that applies trace Lasso to regularize the coefficient matrix. We prove that the resulting coefficient matrix has the \"grouping effect\" and that it exhibits \"sparsity\". We show that these two characteristics imply very effective spectral clustering. We evaluate CAST and 10 other clustering methods on a wide range of datasets w.r.t. various measures. Experimental results show that CAST provides excellent performance and is highly robust across test cases of multi-scale data.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T09:46:35Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xiang Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ben Kao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Caihua Shan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dawei Yin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Martin Ester</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04437v2</id>\n",
      "    <title>The Power Spherical distribution</title>\n",
      "    <updated>2020-06-15T10:42:27Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04437v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04437v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>There is a growing interest in probabilistic models defined in hyper-spherical spaces, be it to accommodate observed data or latent structure. The von Mises-Fisher (vMF) distribution, often regarded as the Normal distribution on the hyper-sphere, is a standard modeling choice: it is an exponential family and thus enjoys important statistical results, for example, known Kullback-Leibler (KL) divergence from other vMF distributions. Sampling from a vMF distribution, however, requires a rejection sampling procedure which besides being slow poses difficulties in the context of stochastic backpropagation via the reparameterization trick. Moreover, this procedure is numerically unstable for certain vMFs, e.g., those with high concentration and/or in high dimensions. We propose a novel distribution, the Power Spherical distribution, which retains some of the important aspects of the vMF (e.g., support on the hyper-sphere, symmetry about its mean direction parameter, known KL from other vMF distributions) while addressing its main drawbacks (i.e., scalability and numerical stability). We demonstrate the stability of Power Spherical distributions with a numerical experiment and further apply it to a variational auto-encoder trained on MNIST. Code at: https://github.com/nicola-decao/power_spherical</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T09:51:43Z</published>\n",
      "    <arxiv:comment>8 pages, 2 figures, 1 table. Code at: https://github.com/nicola-decao/power_spherical</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Nicola De Cao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wilker Aziz</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04497v1</id>\n",
      "    <title>Learning under Invariable Bayesian Safety</title>\n",
      "    <updated>2020-06-08T12:07:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04497v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04497v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A recent body of work addresses safety constraints in explore-and-exploit systems. Such constraints arise where, for example, exploration is carried out by individuals whose welfare should be balanced with overall welfare. In this paper, we adopt a model inspired by recent work on a bandit-like setting for recommendations. We contribute to this line of literature by introducing a safety constraint that should be respected in every round and determines that the expected value in each round is above a given threshold. Due to our modeling, the safe explore-and-exploit policy deserves careful planning, or otherwise, it will lead to sub-optimal welfare. We devise an asymptotically optimal algorithm for the setting and analyze its instance-dependent convergence rate.</summary>\n",
      "    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T12:07:59Z</published>\n",
      "    <arxiv:primary_category term=\"cs.GT\"/>\n",
      "    <author>\n",
      "      <name>Gal Bahar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Omer Ben-Porat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kevin Leyton-Brown</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Moshe Tennenholtz</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04533v1</id>\n",
      "    <title>Investigation Into the Viability of Neural Networks as a Means for Anomaly Detection in Experiments Like Atlas at the LHC</title>\n",
      "    <updated>2020-05-29T02:45:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04533v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04533v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Petabytes of data are generated at the Atlas experiment at the Large Hadron Collider however not all of it is necessarily interesting, so what do we do with all of this data and how do we find these interesting needles in an uninteresting haystack. This problem can possibly be solved through the process of anomaly detection. In this document, Investigation Into the Viability of Neural Networks as a Means for Anomaly Detection in Experiments Like Atlas at the LHC the effectiveness of different types of neural network architectures as anomaly detectors are researched using Monte Carlo simulated data generated by the DarkMachines project. This data is meant to replicate Standard Model and Beyond Standard Model events. By finding an effective model, the Atlas experiment can become more effective and fewer interesting events will be lost.</summary>\n",
      "    <category term=\"hep-ex\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-29T02:45:37Z</published>\n",
      "    <arxiv:primary_category term=\"hep-ex\"/>\n",
      "    <author>\n",
      "      <name>Sully Billingsley</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04535v1</id>\n",
      "    <title>Improving k-Means Clustering Performance with Disentangled Internal Representations</title>\n",
      "    <updated>2020-06-05T11:32:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04535v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04535v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep clustering algorithms combine representation learning and clustering by jointly optimizing a clustering loss and a non-clustering loss. In such methods, a deep neural network is used for representation learning together with a clustering network. Instead of following this framework to improve clustering performance, we propose a simpler approach of optimizing the entanglement of the learned latent code representation of an autoencoder. We define entanglement as how close pairs of points from the same class or structure are, relative to pairs of points from different classes or structures. To measure the entanglement of data points, we use the soft nearest neighbor loss, and expand it by introducing an annealing temperature factor. Using our proposed approach, the test clustering accuracy was 96.2% on the MNIST dataset, 85.6% on the Fashion-MNIST dataset, and 79.2% on the EMNIST Balanced dataset, outperforming our baseline models.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-05T11:32:34Z</published>\n",
      "    <arxiv:comment>To be presented at IJCNN 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Abien Fred Agarap</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arnulfo P. Azcarraga</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04570v1</id>\n",
      "    <title>Incorporating Image Gradients as Secondary Input Associated with Input Image to Improve the Performance of the CNN Model</title>\n",
      "    <updated>2020-06-05T14:01:52Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04570v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04570v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>CNN is very popular neural network architecture in modern days. It is primarily most used tool for vision related task to extract the important features from the given image. Moreover, CNN works as a filter to extract the important features using convolutional operation in distinct layers. In existing CNN architectures, to train the network on given input, only single form of given input is fed to the network. In this paper, new architecture has been proposed where given input is passed in more than one form to the network simultaneously by sharing the layers with both forms of input. We incorporate image gradient as second form of the input associated with the original input image and allowing both inputs to flow in the network using same number of parameters to improve the performance of the model for better generalization. The results of the proposed CNN architecture, applying on diverse set of datasets such as MNIST, CIFAR10 and CIFAR100 show superior result compared to the benchmark CNN architecture considering inputs in single form.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-05T14:01:52Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Vijay Pandey</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shashi Bhushan Jha</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04588v2</id>\n",
      "    <title>EDCompress: Energy-Aware Model Compression for Dataflows</title>\n",
      "    <updated>2020-07-11T06:24:29Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04588v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04588v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Edge devices demand low energy consumption, cost and small form factor. To efficiently deploy convolutional neural network (CNN) models on edge device, energy-aware model compression becomes extremely important. However, existing work did not study this problem well because the lack of considering the diversity of dataflow types in hardware architectures. In this paper, we propose EDCompress, an Energy-aware model compression method for various Dataflows. It can effectively reduce the energy consumption of various edge devices, with different dataflow types. Considering the very nature of model compression procedures, we recast the optimization process to a multi-step problem, and solve it by reinforcement learning algorithms. Experiments show that EDCompress could improve 20X, 17X, 37X energy efficiency in VGG-16, MobileNet, LeNet-5 networks, respectively, with negligible loss of accuracy. EDCompress could also find the optimal dataflow type for specific neural networks in terms of energy consumption, which can guide the deployment of CNN models on hardware systems.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T13:33:40Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Zhehui Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tao Luo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joey Tianyi Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rick Siow Mong Goh</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04595v1</id>\n",
      "    <title>Softwarization, Virtualization, &amp; Machine Learning For Intelligent &amp; Effective V2X Communications</title>\n",
      "    <updated>2020-06-08T13:43:43Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04595v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04595v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The concept of the fifth generation (5G) mobile network system has emerged in recent years as telecommunication operators and service providers look to upgrade their infrastructure and delivery modes to meet the growing demand. Concepts such as softwarization, virtualization, and machine learning will be key components as innovative and flexible enablers of such networks. In particular, paradigms such as software-defined networks, software-defined perimeter, cloud &amp; edge computing, and network function virtualization will play a major role in addressing several 5G networks' challenges, especially in terms of flexibility, programmability, scalability, and security. In this work, the role and potential of these paradigms in the context of V2X communication is discussed. To do so, the paper starts off by providing an overview and background of V2X communications. Then, the paper discusses in more details the various challenges facing V2X communications and some of the previous literature work done to tackle them. Furthermore, the paper describes how softwarization, virtualization, and machine learning can be adapted to tackle the challenges of such networks.</summary>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T13:43:43Z</published>\n",
      "    <arxiv:comment>14 pages, 6 figures, 3 tables, Accepted in IEEE Intelligent Transportation Systems Magazine</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.NI\"/>\n",
      "    <author>\n",
      "      <name>Abdallah Moubayed</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abdallah Shami</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/MITS.2020.3014124</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/MITS.2020.3014124\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05038v1</id>\n",
      "    <title>Coverage probability in wireless networks with determinantal scheduling</title>\n",
      "    <updated>2020-06-09T04:05:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05038v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05038v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose a new class of algorithms for randomly scheduling network transmissions. The idea is to use (discrete) determinantal point processes (subsets) to randomly assign medium access to various {\\em repulsive} subsets of potential transmitters. This approach can be seen as a natural extension of (spatial) Aloha, which schedules transmissions independently. Under a general path loss model and Rayleigh fading, we show that, similarly to Aloha, they are also subject to elegant analysis of the coverage probabilities and transmission attempts (also known as local delay). This is mainly due to the explicit, determinantal form of the conditional (Palm) distribution and closed-form expressions for the Laplace functional of determinantal processes. Interestingly, the derived performance characteristics of the network are amenable to various optimizations of the scheduling parameters, which are determinantal kernels, allowing the use of techniques developed for statistical learning with determinantal processes. Well-established sampling algorithms for determinantal processes can be used to cope with implementation issues, which is is beyond the scope of this paper, but it creates paths for further research.</summary>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.PR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-09T04:05:50Z</published>\n",
      "    <arxiv:comment>8 pages. 2 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.NI\"/>\n",
      "    <author>\n",
      "      <name>Bartek Błaszczyszyn</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Antoine Brochard</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>H. Paul Keeler</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05048v3</id>\n",
      "    <title>Policy-focused Agent-based Modeling using RL Behavioral Models</title>\n",
      "    <updated>2020-11-05T20:41:17Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05048v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05048v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Agent-based Models (ABMs) are valuable tools for policy analysis. ABMs help analysts explore the emergent consequences of policy interventions in multi-agent decision-making settings. But the validity of inferences drawn from ABM explorations depends on the quality of the ABM agents' behavioral models. Standard specifications of agent behavioral models rely either on heuristic decision-making rules or on regressions trained on past data. Both prior specification modes have limitations. This paper examines the value of reinforcement learning (RL) models as adaptive, high-performing, and behaviorally-valid models of agent decision-making in ABMs. We test the hypothesis that RL agents are effective as utility-maximizing agents in policy ABMs. We also address the problem of adapting RL algorithms to handle multi-agency in games by adapting and extending methods from recent literature. We evaluate the performance of such RL-based ABM agents via experiments on two policy-relevant ABMs: a minority game ABM, and an ABM of Influenza Transmission. We run some analytic experiments on our AI-equipped ABMs e.g. explorations of the effects of behavioral heterogeneity in a population and the emergence of synchronization in a population. The experiments show that RL behavioral models are effective at producing reward-seeking or reward-maximizing behaviors in ABM agents. Furthermore, RL behavioral models can learn to outperform the default adaptive behavioral models in the two ABMs examined.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-09T04:55:07Z</published>\n",
      "    <arxiv:comment>This is a more detailed version of a paper (\"Modeling Agent Behaviors for Policy Analysis via Reinforcement Learning\") accepted to appear in IEEE ICMLA 2020. This also corrects an error in Fig. 7 of the original arXiv submission. Fig. 7 now specifies the right ABM architecture (\"flu\" instead of \"tax\")</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Osonde A. Osoba</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Raffaele Vardavas</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Justin Grana</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rushil Zutshi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amber Jaycocks</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05065v2</id>\n",
      "    <title>Self-Distillation as Instance-Specific Label Smoothing</title>\n",
      "    <updated>2020-10-22T03:23:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05065v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05065v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>It has been recently demonstrated that multi-generational self-distillation can improve generalization. Despite this intriguing observation, reasons for the enhancement remain poorly understood. In this paper, we first demonstrate experimentally that the improved performance of multi-generational self-distillation is in part associated with the increasing diversity in teacher predictions. With this in mind, we offer a new interpretation for teacher-student training as amortized MAP estimation, such that teacher predictions enable instance-specific regularization. Our framework allows us to theoretically relate self-distillation to label smoothing, a commonly used technique that regularizes predictive uncertainty, and suggests the importance of predictive diversity in addition to predictive uncertainty. We present experimental results using multiple datasets and neural network architectures that, overall, demonstrate the utility of predictive diversity. Finally, we propose a novel instance-specific label smoothing technique that promotes predictive diversity without the need for a separately trained teacher model. We provide an empirical evaluation of the proposed method, which, we find, often outperforms classical label smoothing.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-09T06:06:30Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>34th Conference on Neural Information Processing Systems (NeurIPS 2020)</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Zhilu Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mert R. Sabuncu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05113v2</id>\n",
      "    <title>Human brain activity for machine attention</title>\n",
      "    <updated>2020-10-02T22:06:31Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05113v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05113v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Cognitively inspired NLP leverages human-derived data to teach machines about language processing mechanisms. Recently, neural networks have been augmented with behavioral data to solve a range of NLP tasks spanning syntax and semantics. We are the first to exploit neuroscientific data, namely electroencephalography (EEG), to inform a neural attention model about language processing of the human brain. The challenge in working with EEG data is that features are exceptionally rich and need extensive pre-processing to isolate signals specific to text processing. We devise a method for finding such EEG features to supervise machine attention through combining theoretically motivated cropping with random forest tree splits. After this dimensionality reduction, the pre-processed EEG features are capable of distinguishing two reading tasks retrieved from a publicly available EEG corpus. We apply these features to regularise attention on relation classification and show that EEG is more informative than strong baselines. This improvement depends on both the cognitive load of the task and the EEG frequency domain. Hence, informing neural attention models with EEG signals is beneficial but requires further investigation to understand which dimensions are the most useful across NLP tasks.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-09T08:39:07Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Lukas Muttenthaler</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nora Hollenstein</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maria Barrett</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05169v1</id>\n",
      "    <title>DyHGCN: A Dynamic Heterogeneous Graph Convolutional Network to Learn Users' Dynamic Preferences for Information Diffusion Prediction</title>\n",
      "    <updated>2020-06-09T10:34:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05169v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05169v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Information diffusion prediction is a fundamental task for understanding the information propagation process. It has wide applications in such as misinformation spreading prediction and malicious account detection. Previous works either concentrate on utilizing the context of a single diffusion sequence or using the social network among users for information diffusion prediction. However, the diffusion paths of different messages naturally constitute a dynamic diffusion graph. For one thing, previous works cannot jointly utilize both the social network and diffusion graph for prediction, which is insufficient to model the complexity of the diffusion process and results in unsatisfactory prediction performance. For another, they cannot learn users' dynamic preferences. Intuitively, users' preferences are changing as time goes on and users' personal preference determines whether the user will repost the information. Thus, it is beneficial to consider users' dynamic preferences in information diffusion prediction.\n",
      "  In this paper, we propose a novel dynamic heterogeneous graph convolutional network (DyHGCN) to jointly learn the structural characteristics of the social graph and dynamic diffusion graph. Then, we encode the temporal information into the heterogeneous graph to learn the users' dynamic preferences. Finally, we apply multi-head attention to capture the context-dependency of the current diffusion path to facilitate the information diffusion prediction task. Experimental results show that DyHGCN significantly outperforms the state-of-the-art models on three public datasets, which shows the effectiveness of the proposed model.</summary>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-09T10:34:41Z</published>\n",
      "    <arxiv:comment>Accepted to the ECML-PKDD 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.SI\"/>\n",
      "    <author>\n",
      "      <name>Chunyuan Yuan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiacheng Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wei Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yijun Lu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaodan Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Songlin Hu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05183v1</id>\n",
      "    <title>A Note on Deepfake Detection with Low-Resources</title>\n",
      "    <updated>2020-06-09T11:07:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05183v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05183v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deepfakes are videos that include changes, quite often substituting face of a portrayed individual with a different face using neural networks. Even though the technology gained its popularity as a carrier of jokes and parodies it raises a serious threat to ones security - via biometric impersonation or besmearing. In this paper we present two methods that allow detecting Deepfakes for a user without significant computational power. In particular, we enhance MesoNet by replacing the original activation functions allowing a nearly 1% improvement as well as increasing the consistency of the results. Moreover, we introduced and verified a new activation function - Pish that at the cost of slight time overhead allows even higher consistency.\n",
      "  Additionally, we present a preliminary results of Deepfake detection method based on Local Feature Descriptors (LFD), that allows setting up the system even faster and without resorting to GPU computation. Our method achieved Equal Error Rate of 0.28, with both accuracy and recall exceeding 0.7.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-09T11:07:08Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Piotr Kawa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Piotr Syga</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04633v2</id>\n",
      "    <title>All your loss are belong to Bayes</title>\n",
      "    <updated>2020-11-05T07:05:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04633v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04633v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Loss functions are a cornerstone of machine learning and the starting point of most algorithms. Statistics and Bayesian decision theory have contributed, via properness, to elicit over the past decades a wide set of admissible losses in supervised learning, to which most popular choices belong (logistic, square, Matsushita, etc.). Rather than making a potentially biased ad hoc choice of the loss, there has recently been a boost in efforts to fit the loss to the domain at hand while training the model itself. The key approaches fit a canonical link, a function which monotonically relates the closed unit interval to R and can provide a proper loss via integration. In this paper, we rely on a broader view of proper composite losses and a recent construct from information geometry, source functions, whose fitting alleviates constraints faced by canonical links. We introduce a trick on squared Gaussian Processes to obtain a random process whose paths are compliant source functions with many desirable properties in the context of link estimation. Experimental results demonstrate substantial improvements over the state of the art.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T14:31:21Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Christian Walder</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Richard Nock</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04682v1</id>\n",
      "    <title>An Ensemble Approach for Compressive Sensing with Quantum</title>\n",
      "    <updated>2020-06-08T15:32:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04682v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04682v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We leverage the idea of a statistical ensemble to improve the quality of quantum annealing based binary compressive sensing. Since executing quantum machine instructions on a quantum annealer can result in an excited state, rather than the ground state of the given Hamiltonian, we use different penalty parameters to generate multiple distinct quadratic unconstrained binary optimization (QUBO) functions whose ground state(s) represent a potential solution of the original problem. We then employ the attained samples from minimizing all corresponding (different) QUBOs to estimate the solution of the problem of binary compressive sensing. Our experiments, on a D-Wave 2000Q quantum processor, demonstrated that the proposed ensemble scheme is notably less sensitive to the calibration of the penalty parameter that controls the trade-off between the feasibility and sparsity of recoveries.</summary>\n",
      "    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T15:32:22Z</published>\n",
      "    <arxiv:primary_category term=\"quant-ph\"/>\n",
      "    <author>\n",
      "      <name>Ramin Ayanzadeh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Milton Halem</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tim Finin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04696v1</id>\n",
      "    <title>Unsupervised Graph Representation by Periphery and Hierarchical Information Maximization</title>\n",
      "    <updated>2020-06-08T15:50:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04696v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04696v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep representation learning on non-Euclidean data types, such as graphs, has gained significant attention in recent years. Invent of graph neural networks has improved the state-of-the-art for both node and the entire graph representation in a vector space. However, for the entire graph representation, most of the existing graph neural networks are trained on a graph classification loss in a supervised way. But obtaining labels of a large number of graphs is expensive for real world applications. Thus, we aim to propose an unsupervised graph neural network to generate a vector representation of an entire graph in this paper. For this purpose, we combine the idea of hierarchical graph neural networks and mutual information maximization into a single framework. We also propose and use the concept of periphery representation of a graph and show its usefulness in the proposed algorithm which is referred as GraPHmax. We conduct thorough experiments on several real-world graph datasets and compare the performance of GraPHmax with a diverse set of both supervised and unsupervised baseline algorithms. Experimental results show that we are able to improve the state-of-the-art for multiple graph level tasks on several real-world datasets, while remain competitive on the others.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T15:50:40Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Sambaran Bandyopadhyay</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Manasvi Aggarwal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>M. Narasimha Murty</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04697v1</id>\n",
      "    <title>Supervised Whole DAG Causal Discovery</title>\n",
      "    <updated>2020-06-08T15:53:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04697v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04697v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose to address the task of causal structure learning from data in a supervised manner. Existing work on learning causal directions by supervised learning is restricted to learning pairwise relation, and not well suited for whole DAG discovery. We propose a novel approach of modeling the whole DAG structure discovery as a supervised learning. To fit the problem in hand, we propose to use permutation equivariant models that align well with the problem domain. We evaluate the proposed approach extensively on synthetic graphs of size 10,20,50,100 and real data, and show promising results compared with a variety of previous approaches.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T15:53:20Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Hebi Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qi Xiao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jin Tian</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04702v3</id>\n",
      "    <title>CycleGT: Unsupervised Graph-to-Text and Text-to-Graph Generation via Cycle Training</title>\n",
      "    <updated>2020-12-09T19:29:27Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04702v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04702v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Two important tasks at the intersection of knowledge graphs and natural language processing are graph-to-text (G2T) and text-to-graph (T2G) conversion. Due to the difficulty and high cost of data collection, the supervised data available in the two fields are usually on the magnitude of tens of thousands, for example, 18K in the WebNLG~2017 dataset after preprocessing, which is far fewer than the millions of data for other tasks such as machine translation. Consequently, deep learning models for G2T and T2G suffer largely from scarce training data. We present CycleGT, an unsupervised training method that can bootstrap from fully non-parallel graph and text data, and iteratively back translate between the two forms. Experiments on WebNLG datasets show that our unsupervised model trained on the same number of data achieves performance on par with several fully supervised models. Further experiments on the non-parallel GenWiki dataset verify that our method performs the best among unsupervised baselines. This validates our framework as an effective approach to overcome the data scarcity problem in the fields of G2T and T2G. Our code is available at https://github.com/QipengGuo/CycleGT.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T15:59:00Z</published>\n",
      "    <arxiv:comment>INLG 2020 Workshop</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Qipeng Guo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhijing Jin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xipeng Qiu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Weinan Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Wipf</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zheng Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04751v1</id>\n",
      "    <title>The Golden Ratio of Learning and Momentum</title>\n",
      "    <updated>2020-06-08T17:08:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04751v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04751v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Gradient descent has been a central training principle for artificial neural networks from the early beginnings to today's deep learning networks. The most common implementation is the backpropagation algorithm for training feed-forward neural networks in a supervised fashion. Backpropagation involves computing the gradient of a loss function, with respect to the weights of the network, to update the weights and thus minimize loss. Although the mean square error is often used as a loss function, the general stochastic gradient descent principle does not immediately connect with a specific loss function. Another drawback of backpropagation has been the search for optimal values of two important training parameters, learning rate and momentum weight, which are determined empirically in most systems. The learning rate specifies the step size towards a minimum of the loss function when following the gradient, while the momentum weight considers previous weight changes when updating current weights. Using both parameters in conjunction with each other is generally accepted as a means to improving training, although their specific values do not follow immediately from standard backpropagation theory. This paper proposes a new information-theoretical loss function motivated by neural signal processing in a synapse. The new loss function implies a specific learning rate and momentum weight, leading to empirical parameters often used in practice. The proposed framework also provides a more formal explanation of the momentum term and its smoothing effect on the training process. All results taken together show that loss, learning rate, and momentum are closely connected. To support these theoretical findings, experiments for handwritten digit recognition show the practical usefulness of the proposed loss function and training parameters.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T17:08:13Z</published>\n",
      "    <arxiv:comment>10 pages, 3 figures, under review</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Stefan Jaeger</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04757v3</id>\n",
      "    <title>Mathematical Reasoning via Self-supervised Skip-tree Training</title>\n",
      "    <updated>2020-08-12T07:48:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04757v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04757v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We examine whether self-supervised language modeling applied to mathematical formulas enables logical reasoning. We suggest several logical reasoning tasks that can be used to evaluate language models trained on formal mathematical statements, such as type inference, suggesting missing assumptions and completing equalities. To train language models for formal mathematics, we propose a novel skip-tree task. We find that models trained on the skip-tree task show surprisingly strong mathematical reasoning abilities, and outperform models trained on standard skip-sequence tasks. We also analyze the models' ability to formulate new conjectures by measuring how often the predictions are provable and useful in other proofs.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.PL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-08T17:12:08Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Markus N. Rabe</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dennis Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kshitij Bansal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christian Szegedy</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04021v1</id>\n",
      "    <title>Skill Discovery of Coordination in Multi-agent Reinforcement Learning</title>\n",
      "    <updated>2020-06-07T02:04:15Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04021v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04021v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Unsupervised skill discovery drives intelligent agents to explore the unknown environment without task-specific reward signal, and the agents acquire various skills which may be useful when the agents adapt to new tasks. In this paper, we propose \"Multi-agent Skill Discovery\"(MASD), a method for discovering skills for coordination patterns of multiple agents. The proposed method aims to maximize the mutual information between a latent code Z representing skills and the combination of the states of all agents. Meanwhile it suppresses the empowerment of Z on the state of any single agent by adversarial training. In another word, it sets an information bottleneck to avoid empowerment degeneracy. First we show the emergence of various skills on the level of coordination in a general particle multi-agent environment. Second, we reveal that the \"bottleneck\" prevents skills from collapsing to a single agent and enhances the diversity of learned skills. Finally, we show the pretrained policies have better performance on supervised RL tasks.</summary>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-07T02:04:15Z</published>\n",
      "    <arxiv:primary_category term=\"cs.MA\"/>\n",
      "    <author>\n",
      "      <name>Shuncheng He</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jianzhun Shao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiangyang Ji</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04033v1</id>\n",
      "    <title>A Comparative Analysis of E-Scooter and E-Bike Usage Patterns: Findings from the City of Austin, TX</title>\n",
      "    <updated>2020-06-07T03:27:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04033v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04033v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>E-scooter-sharing and e-bike-sharing systems are accommodating and easing the increased traffic in dense cities and are expanding considerably. However, these new micro-mobility transportation modes raise numerous operational and safety concerns. This study analyzes e-scooter and dockless e-bike sharing system user behavior. We investigate how average trip speed change depending on the day of the week and the time of the day. We used a dataset from the city of Austin, TX from December 2018 to May 2019. Our results generally show that the trip average speed for e-bikes ranges between 3.01 and 3.44 m/s, which is higher than that for e-scooters (2.19 to 2.78 m/s). Results also show a similar usage pattern for the average speed of e-bikes and e-scooters throughout the days of the week and a different usage pattern for the average speed of e-bikes and e-scooters over the hours of the day. We found that users tend to ride e-bikes and e-scooters with a slower average speed for recreational purposes compared to when they are ridden for commuting purposes. This study is a building block in this field, which serves as a first of its kind, and sheds the light of significant new understanding of this emerging class of shared-road users.</summary>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-07T03:27:44Z</published>\n",
      "    <arxiv:comment>Submitted to the International Journal of Sustainable Transportation</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CY\"/>\n",
      "    <author>\n",
      "      <name>Mohammed Hamad Almannaa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Huthaifa I. Ashqar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mohammed Elhenawy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mahmoud Masoud</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andry Rakotonirainy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hesham Rakha</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04037v1</id>\n",
      "    <title>Reinforcement Learning for Multi-Product Multi-Node Inventory Management in Supply Chains</title>\n",
      "    <updated>2020-06-07T04:02:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04037v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04037v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper describes the application of reinforcement learning (RL) to multi-product inventory management in supply chains. The problem description and solution are both adapted from a real-world business solution. The novelty of this problem with respect to supply chain literature is (i) we consider concurrent inventory management of a large number (50 to 1000) of products with shared capacity, (ii) we consider a multi-node supply chain consisting of a warehouse which supplies three stores, (iii) the warehouse, stores, and transportation from warehouse to stores have finite capacities, (iv) warehouse and store replenishment happen at different time scales and with realistic time lags, and (v) demand for products at the stores is stochastic. We describe a novel formulation in a multi-agent (hierarchical) reinforcement learning framework that can be used for parallelised decision-making, and use the advantage actor critic (A2C) algorithm with quantised action spaces to solve the problem. Experiments show that the proposed approach is able to handle a multi-objective reward comprised of maximising product sales and minimising wastage of perishable products.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-07T04:02:59Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Nazneen N Sultana</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hardik Meisheri</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vinita Baniwal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Somjit Nath</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Balaraman Ravindran</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Harshad Khadilkar</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04082v2</id>\n",
      "    <title>End-to-end Learning for Inter-Vehicle Distance and Relative Velocity Estimation in ADAS with a Monocular Camera</title>\n",
      "    <updated>2020-06-09T07:40:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04082v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04082v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Inter-vehicle distance and relative velocity estimations are two basic functions for any ADAS (Advanced driver-assistance systems). In this paper, we propose a monocular camera-based inter-vehicle distance and relative velocity estimation method based on end-to-end training of a deep neural network. The key novelty of our method is the integration of multiple visual clues provided by any two time-consecutive monocular frames, which include deep feature clue, scene geometry clue, as well as temporal optical flow clue. We also propose a vehicle-centric sampling mechanism to alleviate the effect of perspective distortion in the motion field (i.e. optical flow). We implement the method by a light-weight deep neural network. Extensive experiments are conducted which confirm the superior performance of our method over other state-of-the-art methods, in terms of estimation accuracy, computational speed, and memory footprint.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-07T08:18:31Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Zhenbo Song</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jianfeng Lu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tong Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hongdong Li</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04097v1</id>\n",
      "    <title>Optimally Combining Classifiers for Semi-Supervised Learning</title>\n",
      "    <updated>2020-06-07T09:28:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04097v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04097v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper considers semi-supervised learning for tabular data. It is widely known that Xgboost based on tree model works well on the heterogeneous features while transductive support vector machine can exploit the low density separation assumption. However, little work has been done to combine them together for the end-to-end semi-supervised learning. In this paper, we find these two methods have complementary properties and larger diversity, which motivates us to propose a new semi-supervised learning method that is able to adaptively combine the strengths of Xgboost and transductive support vector machine. Instead of the majority vote rule, an optimization problem in terms of ensemble weight is established, which helps to obtain more accurate pseudo labels for unlabeled data. The experimental results on the UCI data sets and real commercial data set demonstrate the superior classification performance of our method over the five state-of-the-art algorithms improving test accuracy by about $3\\%-4\\%$. The partial code can be found at https://github.com/hav-cam-mit/CTO.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-07T09:28:34Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Zhiguo Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liusha Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Feng Yin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ke Lin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qingjiang Shi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhi-Quan Luo</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.04198v1</id>\n",
      "    <title>EnK: Encoding time-information in convolution</title>\n",
      "    <updated>2020-06-07T16:43:07Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.04198v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.04198v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent development in deep learning techniques has attracted attention in decoding and classification in EEG signals. Despite several efforts utilizing different features of EEG signals, a significant research challenge is to use time-dependent features in combination with local and global features. There have been several efforts to remodel the deep learning convolution neural networks (CNNs) to capture time-dependency information by incorporating hand-crafted features, slicing the input data in a smaller time-windows, and recurrent convolution. However, these approaches partially solve the problem, but simultaneously hinder the CNN's capability to learn from unknown information that might be present in the data. To solve this, we have proposed a novel time encoding kernel (EnK) approach, which introduces the increasing time information during convolution operation in CNN. The encoded information by EnK lets CNN learn time-dependent features in-addition to local and global features. We performed extensive experiments on several EEG datasets: cognitive conflict (CC), physical-human robot collaboration (pHRC), P300 visual-evoked potentials, movement-related cortical potentials (MRCP). EnK outperforms the state-of-art by 12\\% (F1 score). Moreover, the EnK approach required only one additional parameter to learn and can be applied to a virtually any CNN architectures with minimal efforts. These results support our methodology and show high potential to improve CNN performance in the context of time-series data in general.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-07T16:43:07Z</published>\n",
      "    <arxiv:comment>Under review</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Avinash Kumar Singh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chin-Teng Lin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05255v1</id>\n",
      "    <title>DeepFair: Deep Learning for Improving Fairness in Recommender Systems</title>\n",
      "    <updated>2020-06-09T13:39:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05255v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05255v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The lack of bias management in Recommender Systems leads to minority groups receiving unfair recommendations. Moreover, the trade-off between equity and precision makes it difficult to obtain recommendations that meet both criteria. Here we propose a Deep Learning based Collaborative Filtering algorithm that provides recommendations with an optimum balance between fairness and accuracy without knowing demographic information about the users. Experimental results show that it is possible to make fair recommendations without losing a significant proportion of accuracy.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-09T13:39:38Z</published>\n",
      "    <arxiv:comment>18 pages, 9 figures, 4 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>International Journal of Interactive Multimedia and Artificial Intelligence, 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Jesús Bobadilla</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Raúl Lara-Cabrera</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ángel González-Prieto</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fernando Ortega</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.9781/ijimai.2020.11.001</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.9781/ijimai.2020.11.001\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05324v2</id>\n",
      "    <title>ORCAS: 18 Million Clicked Query-Document Pairs for Analyzing Search</title>\n",
      "    <updated>2020-08-18T14:45:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05324v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05324v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Users of Web search engines reveal their information needs through queries and clicks, making click logs a useful asset for information retrieval. However, click logs have not been publicly released for academic use, because they can be too revealing of personally or commercially sensitive information. This paper describes a click data release related to the TREC Deep Learning Track document corpus. After aggregation and filtering, including a k-anonymity requirement, we find 1.4 million of the TREC DL URLs have 18 million connections to 10 million distinct queries. Our dataset of these queries and connections to TREC documents is of similar size to proprietary datasets used in previous papers on query mining and ranking. We perform some preliminary experiments using the click data to augment the TREC DL training data, offering by comparison: 28x more queries, with 49x more connections to 4.4x more URLs in the corpus. We present a description of the dataset's generation process, characteristics, use in ranking and suggest other potential uses.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-09T14:58:21Z</published>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Nick Craswell</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniel Campos</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bhaskar Mitra</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Emine Yilmaz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bodo Billerbeck</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05331v2</id>\n",
      "    <title>Data Augmentation for Enhancing EEG-based Emotion Recognition with Deep Generative Models</title>\n",
      "    <updated>2020-06-17T08:09:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05331v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05331v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The data scarcity problem in emotion recognition from electroencephalography (EEG) leads to difficulty in building an affective model with high accuracy using machine learning algorithms or deep neural networks. Inspired by emerging deep generative models, we propose three methods for augmenting EEG training data to enhance the performance of emotion recognition models. Our proposed methods are based on two deep generative models, variational autoencoder (VAE) and generative adversarial network (GAN), and two data augmentation strategies. For the full usage strategy, all of the generated data are augmented to the training dataset without judging the quality of the generated data, while for partial usage, only high-quality data are selected and appended to the training dataset. These three methods are called conditional Wasserstein GAN (cWGAN), selective VAE (sVAE), and selective WGAN (sWGAN). To evaluate the effectiveness of these methods, we perform a systematic experimental study on two public EEG datasets for emotion recognition, namely, SEED and DEAP. We first generate realistic-like EEG training data in two forms: power spectral density and differential entropy. Then, we augment the original training datasets with a different number of generated realistic-like EEG data. Finally, we train support vector machines and deep neural networks with shortcut layers to build affective models using the original and augmented training datasets. The experimental results demonstrate that the augmented training datasets produced by our methods enhance the performance of EEG-based emotion recognition models and outperform the existing data augmentation methods such as conditional VAE, Gaussian noise, and rotational data augmentation.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-04T21:23:09Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Yun Luo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Li-Zhen Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zi-Yu Wan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bao-Liang Lu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05379v3</id>\n",
      "    <title>Adversarial Infidelity Learning for Model Interpretation</title>\n",
      "    <updated>2020-08-03T02:41:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05379v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05379v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Model interpretation is essential in data mining and knowledge discovery. It can help understand the intrinsic model working mechanism and check if the model has undesired characteristics. A popular way of performing model interpretation is Instance-wise Feature Selection (IFS), which provides an importance score of each feature representing the data samples to explain how the model generates the specific output. In this paper, we propose a Model-agnostic Effective Efficient Direct (MEED) IFS framework for model interpretation, mitigating concerns about sanity, combinatorial shortcuts, model identifiability, and information transmission. Also, we focus on the following setting: using selected features to directly predict the output of the given model, which serves as a primary evaluation metric for model-interpretation methods. Apart from the features, we involve the output of the given model as an additional input to learn an explainer based on more accurate information. To learn the explainer, besides fidelity, we propose an Adversarial Infidelity Learning (AIL) mechanism to boost the explanation learning by screening relatively unimportant features. Through theoretical and experimental analysis, we show that our AIL mechanism can help learn the desired conditional distribution between selected features and targets. Moreover, we extend our framework by integrating efficient interpretation methods as proper priors to provide a warm start. Comprehensive empirical evaluation results are provided by quantitative metrics and human evaluation to demonstrate the effectiveness and superiority of our proposed method. Our code is publicly available online at https://github.com/langlrsw/MEED.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-09T16:27:17Z</published>\n",
      "    <arxiv:comment>26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '20), August 23--27, 2020, Virtual Event, USA</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Jian Liang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bing Bai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuren Cao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kun Bai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fei Wang</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3394486.3403071</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3394486.3403071\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05385v1</id>\n",
      "    <title>Interpretable Deep Graph Generation with Node-Edge Co-Disentanglement</title>\n",
      "    <updated>2020-06-09T16:33:49Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05385v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05385v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Disentangled representation learning has recently attracted a significant amount of attention, particularly in the field of image representation learning. However, learning the disentangled representations behind a graph remains largely unexplored, especially for the attributed graph with both node and edge features. Disentanglement learning for graph generation has substantial new challenges including 1) the lack of graph deconvolution operations to jointly decode node and edge attributes; and 2) the difficulty in enforcing the disentanglement among latent factors that respectively influence: i) only nodes, ii) only edges, and iii) joint patterns between them. To address these challenges, we propose a new disentanglement enhancement framework for deep generative models for attributed graphs. In particular, a novel variational objective is proposed to disentangle the above three types of latent factors, with novel architecture for node and edge deconvolutions. Moreover, within each type, individual-factor-wise disentanglement is further enhanced, which is shown to be a generalization of the existing framework for images. Qualitative and quantitative experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed model and its extensions.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-09T16:33:49Z</published>\n",
      "    <arxiv:comment>This paper has been accepted by KDD 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xiaojie Guo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liang Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhao Qin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lingfei Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amarda Shehu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yanfang Ye</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3394486.3403221</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3394486.3403221\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11353v1</id>\n",
      "    <title>A Tree Architecture of LSTM Networks for Sequential Regression with Missing Data</title>\n",
      "    <updated>2020-05-22T18:57:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11353v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11353v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We investigate regression for variable length sequential data containing missing samples and introduce a novel tree architecture based on the Long Short-Term Memory (LSTM) networks. In our architecture, we employ a variable number of LSTM networks, which use only the existing inputs in the sequence, in a tree-like architecture without any statistical assumptions or imputations on the missing data, unlike all the previous approaches. In particular, we incorporate the missingness information by selecting a subset of these LSTM networks based on \"presence-pattern\" of a certain number of previous inputs. From the mixture of experts perspective, we train different LSTM networks as our experts for various missingness patterns and then combine their outputs to generate the final prediction. We also provide the computational complexity analysis of the proposed architecture, which is in the same order of the complexity of the conventional LSTM architectures for the sequence length. Our method can be readily extended to similar structures such as GRUs, RNNs as remarked in the paper. In the experiments, we achieve significant performance improvements with respect to the state-of-the-art methods for the well-known financial and real life datasets.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-22T18:57:47Z</published>\n",
      "    <arxiv:comment>10 pages, 8 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>S. Onur Sahin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Suleyman S. Kozat</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.12781v1</id>\n",
      "    <title>How to Grow a (Product) Tree: Personalized Category Suggestions for eCommerce Type-Ahead</title>\n",
      "    <updated>2020-05-26T15:03:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.12781v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.12781v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In an attempt to balance precision and recall in the search page, leading digital shops have been effectively nudging users into select category facets as early as in the type-ahead suggestions. In this work, we present SessionPath, a novel neural network model that improves facet suggestions on two counts: first, the model is able to leverage session embeddings to provide scalable personalization; second, SessionPath predicts facets by explicitly producing a probability distribution at each node in the taxonomy path. We benchmark SessionPath on two partnering shops against count-based and neural models, and show how business requirements and model behavior can be combined in a principled way.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-26T15:03:16Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jacopo Tagliabue</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bingqing Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marie Beaulieu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.12838v1</id>\n",
      "    <title>Neuro4Neuro: A neural network approach for neural tract segmentation using large-scale population-based diffusion imaging</title>\n",
      "    <updated>2020-05-26T16:14:31Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.12838v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.12838v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Subtle changes in white matter (WM) microstructure have been associated with normal aging and neurodegeneration. To study these associations in more detail, it is highly important that the WM tracts can be accurately and reproducibly characterized from brain diffusion MRI. In addition, to enable analysis of WM tracts in large datasets and in clinical practice it is essential to have methodology that is fast and easy to apply. This work therefore presents a new approach for WM tract segmentation: Neuro4Neuro, that is capable of direct extraction of WM tracts from diffusion tensor images using convolutional neural network (CNN). This 3D end-to-end method is trained to segment 25 WM tracts in aging individuals from a large population-based study (N=9752, 1.5T MRI). The proposed method showed good segmentation performance and high reproducibility, i.e., a high spatial agreement (Cohen's kappa, k = 0.72 ~ 0.83) and a low scan-rescan error in tract-specific diffusion measures (e.g., fractional anisotropy: error = 1% ~ 5%). The reproducibility of the proposed method was higher than that of a tractography-based segmentation algorithm, while being orders of magnitude faster (0.5s to segment one tract). In addition, we showed that the method successfully generalizes to diffusion scans from an external dementia dataset (N=58, 3T MRI). In two proof-of-principle experiments, we associated WM microstructure obtained using the proposed method with age in a normal elderly population, and with disease subtypes in a dementia cohort. In concordance with the literature, results showed a widespread reduction of microstructural organization with aging and substantial group-wise microstructure differences between dementia subtypes. In conclusion, we presented a highly reproducible and fast method for WM tract segmentation that has the potential of being used in large-scale studies and clinical practice.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"physics.med-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-26T16:14:31Z</published>\n",
      "    <arxiv:comment>Preprint to be published in NeuroImage</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Bo Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marius de Groot</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rebecca M. E. Steketee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rozanna Meijboom</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marion Smits</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Meike W. Vernooij</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>M. Arfan Ikram</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiren Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wiro J. Niessen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Esther E. Bron</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.12901v2</id>\n",
      "    <title>A Framework for Behavioral Biometric Authentication using Deep Metric Learning on Mobile Devices</title>\n",
      "    <updated>2020-08-17T16:39:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.12901v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.12901v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Mobile authentication using behavioral biometrics has been an active area of research. Existing research relies on building machine learning classifiers to recognize an individual's unique patterns. However, these classifiers are not powerful enough to learn the discriminative features. When implemented on the mobile devices, they face new challenges from the behavioral dynamics, data privacy and side-channel leaks. To address these challenges, we present a new framework to incorporate training on battery-powered mobile devices, so private data never leaves the device and training can be flexibly scheduled to adapt the behavioral patterns at runtime. We re-formulate the classification problem into deep metric learning to improve the discriminative power and design an effective countermeasure to thwart side-channel leaks by embedding a noise signature in the sensing signals without sacrificing too much usability. The experiments demonstrate authentication accuracy over 95% on three public datasets, a sheer 15% gain from multi-class classification with less data and robustness against brute-force and side-channel attacks with 99% and 90% success, respectively. We show the feasibility of training with mobile CPUs, where training 100 epochs takes less than 10 mins and can be boosted 3-5 times with feature transfer. Finally, we profile memory, energy and computational overhead. Our results indicate that training consumes lower energy than watching videos and slightly higher energy than playing games.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-26T17:56:20Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Cong Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yanru Xiao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xing Gao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Li Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jun Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11760v2</id>\n",
      "    <title>SERIL: Noise Adaptive Speech Enhancement using Regularization-based Incremental Learning</title>\n",
      "    <updated>2020-09-17T20:08:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11760v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11760v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Numerous noise adaptation techniques have been proposed to fine-tune deep-learning models in speech enhancement (SE) for mismatched noise environments. Nevertheless, adaptation to a new environment may lead to catastrophic forgetting of the previously learned environments. The catastrophic forgetting issue degrades the performance of SE in real-world embedded devices, which often revisit previous noise environments. The nature of embedded devices does not allow solving the issue with additional storage of all pre-trained models or earlier training data. In this paper, we propose a regularization-based incremental learning SE (SERIL) strategy, complementing existing noise adaptation strategies without using additional storage. With a regularization constraint, the parameters are updated to the new noise environment while retaining the knowledge of the previous noise environments. The experimental results show that, when faced with a new noise domain, the SERIL model outperforms the unadapted SE model. Meanwhile, compared with the current adaptive technique based on fine-tuning, the SERIL model can reduce the forgetting of previous noise environments by 52%. The results verify that the SERIL model can effectively adjust itself to new noise environments while overcoming the catastrophic forgetting issue. The results make SERIL a favorable choice for real-world SE applications, where the noise environment changes frequently.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-24T14:49:10Z</published>\n",
      "    <arxiv:comment>Accepted to Interspeech 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Chi-Chang Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu-Chen Lin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hsuan-Tien Lin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hsin-Min Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu Tsao</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11777v1</id>\n",
      "    <title>Acoustic Word Embedding System for Code-Switching Query-by-example Spoken Term Detection</title>\n",
      "    <updated>2020-05-24T15:27:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11777v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11777v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we propose a deep convolutional neural network-based acoustic word embedding system on code-switching query by example spoken term detection. Different from previous configurations, we combine audio data in two languages for training instead of only using one single language. We transform the acoustic features of keyword templates and searching content to fixed-dimensional vectors and calculate the distances between keyword segments and searching content segments obtained in a sliding manner. An auxiliary variability-invariant loss is also applied to training data within the same word but different speakers. This strategy is used to prevent the extractor from encoding undesired speaker- or accent-related information into the acoustic word embeddings. Experimental results show that our proposed system produces promising searching results in the code-switching test scenario. With the increased number of templates and the employment of variability-invariant loss, the searching performance is further enhanced.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-24T15:27:56Z</published>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Murong Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Haiwei Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xuyang Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lin Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Junjie Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ming Li</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11856v3</id>\n",
      "    <title>Predicting COVID-19 Pneumonia Severity on Chest X-ray with Deep Learning</title>\n",
      "    <updated>2020-06-30T17:09:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11856v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11856v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Purpose: The need to streamline patient management for COVID-19 has become more pressing than ever. Chest X-rays provide a non-invasive (potentially bedside) tool to monitor the progression of the disease. In this study, we present a severity score prediction model for COVID-19 pneumonia for frontal chest X-ray images. Such a tool can gauge severity of COVID-19 lung infections (and pneumonia in general) that can be used for escalation or de-escalation of care as well as monitoring treatment efficacy, especially in the ICU.\n",
      "  Methods: Images from a public COVID-19 database were scored retrospectively by three blinded experts in terms of the extent of lung involvement as well as the degree of opacity. A neural network model that was pre-trained on large (non-COVID-19) chest X-ray datasets is used to construct features for COVID-19 images which are predictive for our task.\n",
      "  Results: This study finds that training a regression model on a subset of the outputs from an this pre-trained chest X-ray model predicts our geographic extent score (range 0-8) with 1.14 mean absolute error (MAE) and our lung opacity score (range 0-6) with 0.78 MAE.\n",
      "  Conclusions: These results indicate that our model's ability to gauge severity of COVID-19 lung infections could be used for escalation or de-escalation of care as well as monitoring treatment efficacy, especially in the intensive care unit (ICU). A proper clinical trial is needed to evaluate efficacy. To enable this we make our code, labels, and data available online at https://github.com/mlmed/torchxrayvision/tree/master/scripts/covid-severity and https://github.com/ieee8023/covid-chestxray-dataset</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-24T23:13:16Z</published>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Joseph Paul Cohen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lan Dao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paul Morrison</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Karsten Roth</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yoshua Bengio</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Beiyi Shen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Almas Abbasi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mahsa Hoshmand-Kochi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marzyeh Ghassemi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Haifang Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tim Q Duong</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.13596v1</id>\n",
      "    <title>Breiman's \"Two Cultures\" Revisited and Reconciled</title>\n",
      "    <updated>2020-05-27T19:02:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.13596v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.13596v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In a landmark paper published in 2001, Leo Breiman described the tense standoff between two cultures of data modeling: parametric statistical and algorithmic machine learning. The cultural division between these two statistical learning frameworks has been growing at a steady pace in recent years. What is the way forward? It has become blatantly obvious that this widening gap between \"the two cultures\" cannot be averted unless we find a way to blend them into a coherent whole. This article presents a solution by establishing a link between the two cultures. Through examples, we describe the challenges and potential gains of this new integrated statistical thinking.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"econ.EM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-27T19:02:56Z</published>\n",
      "    <arxiv:comment>This paper celebrates the 70th anniversary of Statistical Machine Learning--- how far we've come, and how far we have to go. Keywords: Integrated statistical learning theory, Exploratory machine learning, Uncertainty prediction machine, ML-powered modern applied statistics, Information theory</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name> Subhadeep</name>\n",
      "      <arxiv:affiliation>DEEP</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name> Mukhopadhyay</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kaijun Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.12974v1</id>\n",
      "    <title>Opportunistic Multi-aspect Fairness through Personalized Re-ranking</title>\n",
      "    <updated>2020-05-21T04:25:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.12974v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.12974v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>As recommender systems have become more widespread and moved into areas with greater social impact, such as employment and housing, researchers have begun to seek ways to ensure fairness in the results that such systems produce. This work has primarily focused on developing recommendation approaches in which fairness metrics are jointly optimized along with recommendation accuracy. However, the previous work had largely ignored how individual preferences may limit the ability of an algorithm to produce fair recommendations. Furthermore, with few exceptions, researchers have only considered scenarios in which fairness is measured relative to a single sensitive feature or attribute (such as race or gender). In this paper, we present a re-ranking approach to fairness-aware recommendation that learns individual preferences across multiple fairness dimensions and uses them to enhance provider fairness in recommendation results. Specifically, we show that our opportunistic and metric-agnostic approach achieves a better trade-off between accuracy and fairness than prior re-ranking approaches and does so across multiple fairness dimensions.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-21T04:25:20Z</published>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Nasim Sonboli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Farzad Eskandanian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Robin Burke</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Weiwen Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bamshad Mobasher</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.13078v1</id>\n",
      "    <title>To update or not to update? Delayed Nonparametric Bandits with Randomized Allocation</title>\n",
      "    <updated>2020-05-26T23:06:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.13078v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.13078v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Delayed rewards problem in contextual bandits has been of interest in various practical settings. We study randomized allocation strategies and provide an understanding on how the exploration-exploitation tradeoff is affected by delays in observing the rewards. In randomized strategies, the extent of exploration-exploitation is controlled by a user-determined exploration probability sequence. In the presence of delayed rewards, one may choose between using the original exploration sequence that updates at every time point or update the sequence only when a new reward is observed, leading to two competing strategies. In this work, we show that while both strategies may lead to strong consistency in allocation, the property holds for a wider scope of situations for the latter. However, for finite sample performance, we illustrate that both strategies have their own advantages and disadvantages, depending on the severity of the delay and underlying reward generating mechanisms.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-26T23:06:20Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Sakshi Arya</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuhong Yang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.13085v1</id>\n",
      "    <title>Arm order recognition in multi-armed bandit problem with laser chaos time series</title>\n",
      "    <updated>2020-05-26T23:43:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.13085v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.13085v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>By exploiting ultrafast and irregular time series generated by lasers with delayed feedback, we have previously demonstrated a scalable algorithm to solve multi-armed bandit (MAB) problems utilizing the time-division multiplexing of laser chaos time series. Although the algorithm detects the arm with the highest reward expectation, the correct recognition of the order of arms in terms of reward expectations is not achievable. Here, we present an algorithm where the degree of exploration is adaptively controlled based on confidence intervals that represent the estimation accuracy of reward expectations. We have demonstrated numerically that our approach did improve arm order recognition accuracy significantly, along with reduced dependence on reward environments, and the total reward is almost maintained compared with conventional MAB methods. This study applies to sectors where the order information is critical, such as efficient allocation of resources in information and communications technology.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.ET\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-26T23:43:54Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Naoki Narisawa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nicolas Chauvet</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mikio Hasegawa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Makoto Naruse</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.13097v1</id>\n",
      "    <title>On the Convergence of Langevin Monte Carlo: The Interplay between Tail Growth and Smoothness</title>\n",
      "    <updated>2020-05-27T00:26:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.13097v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.13097v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study sampling from a target distribution ${ν_* = e^{-f}}$ using the unadjusted Langevin Monte Carlo (LMC) algorithm. For any potential function $f$ whose tails behave like ${\\|x\\|^α}$ for ${α\\in [1,2]}$, and has $β$-Hölder continuous gradient, we prove that ${\\widetilde{\\mathcal{O}} \\Big(d^{\\frac{1}β+\\frac{1+β}β(\\frac{2}α - \\boldsymbol{1}_{\\{α\\neq 1\\}})} ε^{-\\frac{1}β}\\Big)}$ steps are sufficient to reach the $ε$-neighborhood of a $d$-dimensional target distribution $ν_*$ in KL-divergence. This convergence rate, in terms of $ε$ dependency, is not directly influenced by the tail growth rate $α$ of the potential function as long as its growth is at least linear, and it only relies on the order of smoothness $β$. One notable consequence of this result is that for potentials with Lipschitz gradient, i.e. $β=1$, our rate recovers the best known rate ${\\widetilde{\\mathcal{O}}(dε^{-1})}$ which was established for strongly convex potentials in terms of $ε$ dependency, but we show that the same rate is achievable for a wider class of potentials that are degenerately convex at infinity. The growth rate $α$ starts to have an effect on the established rate in high dimensions where $d$ is large; furthermore, it recovers the best-known dimension dependency when the tail growth of the potential is quadratic, i.e. ${α= 2}$, in the current setup. Our framework allows for finite perturbations, and any order of smoothness ${β\\in(0,1]}$; consequently, our results are applicable to a wide class of non-convex potentials that are weakly smooth and exhibit at least linear tail growth.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.PR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-27T00:26:20Z</published>\n",
      "    <arxiv:comment>51 pages, 2 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Murat A. Erdogdu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rasa Hosseinzadeh</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.13992v1</id>\n",
      "    <title>A Novel Ramp Metering Approach Based on Machine Learning and Historical Data</title>\n",
      "    <updated>2020-05-26T21:05:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.13992v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.13992v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The random nature of traffic conditions on freeways can cause excessive congestions and irregularities in the traffic flow. Ramp metering is a proven effective method to maintain freeway efficiency under various traffic conditions. Creating a reliable and practical ramp metering algorithm that considers both critical traffic measures and historical data is still a challenging problem. In this study we use machine learning approaches to develop a novel real-time prediction model for ramp metering. We evaluate the potentials of our approach in providing promising results by comparing it with a baseline traffic-responsive ramp metering algorithm.</summary>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-26T21:05:01Z</published>\n",
      "    <arxiv:comment>5 pages, 11 figures, 2 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.SY\"/>\n",
      "    <author>\n",
      "      <name>Anahita Sanandaji</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Saeed Ghanbartehrani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zahra Mokhtari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kimia Tajik</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.3390/make2040021</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.3390/make2040021\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.14090v2</id>\n",
      "    <title>ODEN: A Framework to Solve Ordinary Differential Equations using Artificial Neural Networks</title>\n",
      "    <updated>2020-06-01T10:06:43Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.14090v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.14090v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We explore in detail a method to solve ordinary differential equations using feedforward neural networks. We prove a specific loss function, which does not require knowledge of the exact solution, to be a suitable standard metric to evaluate neural networks' performance. Neural networks are shown to be proficient at approximating continuous solutions within their training domains. We illustrate neural networks' ability to outperform traditional standard numerical techniques. Training is thoroughly examined and three universal phases are found: (i) a prior tangent adjustment, (ii) a curvature fitting, and (iii) a fine-tuning stage. The main limitation of the method is the nontrivial task of finding the appropriate neural network architecture and the choice of neural network hyperparameters for efficient optimization. However, we observe an optimal architecture that matches the complexity of the differential equation. A user-friendly and adaptable open-source code (ODE$\\mathcal{N}$) is provided on GitHub.</summary>\n",
      "    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-28T15:34:10Z</published>\n",
      "    <arxiv:comment>10 pages, 7 figures. Prepared for submission to NeurIPS</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"physics.comp-ph\"/>\n",
      "    <author>\n",
      "      <name>Liam L. H. Lau</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Denis Werth</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.13171v1</id>\n",
      "    <title>Precisely Predicting Acute Kidney Injury with Convolutional Neural Network Based on Electronic Health Record Data</title>\n",
      "    <updated>2020-05-27T05:39:42Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.13171v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.13171v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The incidence of Acute Kidney Injury (AKI) commonly happens in the Intensive Care Unit (ICU) patients, especially in the adults, which is an independent risk factor affecting short-term and long-term mortality. Though researchers in recent years highlight the early prediction of AKI, the performance of existing models are not precise enough. The objective of this research is to precisely predict AKI by means of Convolutional Neural Network on Electronic Health Record (EHR) data. The data sets used in this research are two public Electronic Health Record (EHR) databases: MIMIC-III and eICU database. In this study, we take several Convolutional Neural Network models to train and test our AKI predictor, which can precisely predict whether a certain patient will suffer from AKI after admission in ICU according to the last measurements of the 16 blood gas and demographic features. The research is based on Kidney Disease Improving Global Outcomes (KDIGO) criteria for AKI definition. Our work greatly improves the AKI prediction precision, and the best AUROC is up to 0.988 on MIMIC-III data set and 0.936 on eICU data set, both of which outperform the state-of-art predictors. And the dimension of the input vector used in this predictor is much fewer than that used in other existing researches. Compared with the existing AKI predictors, the predictor in this work greatly improves the precision of early prediction of AKI by using the Convolutional Neural Network architecture and a more concise input vector. Early and precise prediction of AKI will bring much benefit to the decision of treatment, so it is believed that our work is a very helpful clinical application.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-27T05:39:42Z</published>\n",
      "    <arxiv:comment>14 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yu Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>JunPeng Bao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>JianQiang Du</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>YongFeng Li</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.13209v2</id>\n",
      "    <title>A Structural Model for Contextual Code Changes</title>\n",
      "    <updated>2020-10-12T17:52:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.13209v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.13209v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We address the problem of predicting edit completions based on a learned model that was trained on past edits. Given a code snippet that is partially edited, our goal is to predict a completion of the edit for the rest of the snippet. We refer to this task as the EditCompletion task and present a novel approach for tackling it. The main idea is to directly represent structural edits. This allows us to model the likelihood of the edit itself, rather than learning the likelihood of the edited code. We represent an edit operation as a path in the program's Abstract Syntax Tree (AST), originating from the source of the edit to the target of the edit. Using this representation, we present a powerful and lightweight neural model for the EditCompletion task.\n",
      "  We conduct a thorough evaluation, comparing our approach to a variety of representation and modeling approaches that are driven by multiple strong models such as LSTMs, Transformers, and neural CRFs. Our experiments show that our model achieves a 28% relative gain over state-of-the-art sequential models and 2x higher accuracy than syntactic models that learn to generate the edited code, as opposed to modeling the edits directly.\n",
      "  Our code, dataset, and trained models are publicly available at https://github.com/tech-srl/c3po/ .</summary>\n",
      "    <category term=\"cs.PL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-27T07:16:19Z</published>\n",
      "    <arxiv:comment>Accepted to OOPSLA 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.PL\"/>\n",
      "    <author>\n",
      "      <name>Shaked Brody</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Uri Alon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eran Yahav</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.13796v1</id>\n",
      "    <title>A Feature-map Discriminant Perspective for Pruning Deep Neural Networks</title>\n",
      "    <updated>2020-05-28T06:25:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.13796v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.13796v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Network pruning has become the de facto tool to accelerate deep neural networks for mobile and edge applications. Recently, feature-map discriminant based channel pruning has shown promising results, as it aligns well with the CNN objective of differentiating multiple classes and offers better interpretability of the pruning decision. However, existing discriminant-based methods are challenged by computation inefficiency, as there is a lack of theoretical guidance on quantifying the feature-map discriminant power. In this paper, we present a new mathematical formulation to accurately and efficiently quantify the feature-map discriminativeness, which gives rise to a novel criterion,Discriminant Information(DI). We analyze the theoretical property of DI, specifically the non-decreasing property, that makes DI a valid selection criterion. DI-based pruning removes channels with minimum influence to DI value, as they contain little information regarding to the discriminant power. The versatility of DI criterion also enables an intra-layer mixed precision quantization to further compress the network. Moreover, we propose a DI-based greedy pruning algorithm and structure distillation technique to automatically decide the pruned structure that satisfies certain resource budget, which is a common requirement in reality. Extensive experiments demonstratethe effectiveness of our method: our pruned ResNet50 on ImageNet achieves 44% FLOPs reduction without any Top-1 accuracy loss compared to unpruned model</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-28T06:25:22Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Zejiang Hou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sun-Yuan Kung</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.13835v3</id>\n",
      "    <title>Speech-to-Singing Conversion based on Boundary Equilibrium GAN</title>\n",
      "    <updated>2020-08-05T13:21:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.13835v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.13835v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper investigates the use of generative adversarial network (GAN)-based models for converting the spectrogram of a speech signal into that of a singing one, without reference to the phoneme sequence underlying the speech. This is achieved by viewing speech-to-singing conversion as a style transfer problem. Specifically, given a speech input, and optionally the F0 contour of the target singing, the proposed model generates as the output a singing signal with a progressive-growing encoder/decoder architecture and boundary equilibrium GAN loss functions. Our quantitative and qualitative analysis show that the proposed model generates singing voices with much higher naturalness than an existing non adversarially-trained baseline. For reproducibility, the code will be publicly available at a GitHub repository upon paper publication.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-28T08:18:02Z</published>\n",
      "    <arxiv:comment>Accepted for publication at INTERSPEECH 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Da-Yi Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yi-Hsuan Yang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.13485v3</id>\n",
      "    <title>Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing</title>\n",
      "    <updated>2020-12-22T11:45:03Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.13485v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.13485v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>One daunting problem for semantic parsing is the scarcity of annotation. Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance. The downstream naive semantic parser accepts the intermediate output and returns the target logical form. Furthermore, the entire training process is split into two phases: pre-training and cycle learning. Three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model. Experimental results on benchmarks Overnight and GeoGranno demonstrate that our framework is effective and compatible with supervised training.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-27T16:47:44Z</published>\n",
      "    <arxiv:comment>accepted by ACL 2020 Long, 12 pages, 5 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Ruisheng Cao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Su Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chenyu Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chen Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rao Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yanbin Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lu Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kai Yu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.02227v1</id>\n",
      "    <title>Variational Mutual Information Maximization Framework for VAE Latent Codes with Continuous and Discrete Priors</title>\n",
      "    <updated>2020-06-02T09:05:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.02227v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.02227v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Learning interpretable and disentangled representations of data is a key topic in machine learning research. Variational Autoencoder (VAE) is a scalable method for learning directed latent variable models of complex data. It employs a clear and interpretable objective that can be easily optimized. However, this objective does not provide an explicit measure for the quality of latent variable representations which may result in their poor quality. We propose Variational Mutual Information Maximization Framework for VAE to address this issue. In comparison to other methods, it provides an explicit objective that maximizes lower bound on mutual information between latent codes and observations. The objective acts as a regularizer that forces VAE to not ignore the latent variable and allows one to select particular components of it to be most informative with respect to the observations. On top of that, the proposed framework provides a way to evaluate mutual information between latent codes and observations for a fixed VAE model. We have conducted our experiments on VAE models with Gaussian and joint Gaussian and discrete latent variables. Our results illustrate that the proposed approach strengthens relationships between latent codes and observations and improves learned representations.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-02T09:05:51Z</published>\n",
      "    <arxiv:comment>arXiv admin note: text overlap with arXiv:2005.13953</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Andriy Serdega</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dae-Shik Kim</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.13140/RG.2.2.30511.15523</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.13140/RG.2.2.30511.15523\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.02267v1</id>\n",
      "    <title>FastONN -- Python based open-source GPU implementation for Operational Neural Networks</title>\n",
      "    <updated>2020-06-03T13:33:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.02267v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.02267v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Operational Neural Networks (ONNs) have recently been proposed as a special class of artificial neural networks for grid structured data. They enable heterogenous non-linear operations to generalize the widely adopted convolution-based neuron model. This work introduces a fast GPU-enabled library for training operational neural networks, FastONN, which is based on a novel vectorized formulation of the operational neurons. Leveraging on automatic reverse-mode differentiation for backpropagation, FastONN enables increased flexibility with the incorporation of new operator sets and customized gradient flows. Additionally, bundled auxiliary modules offer interfaces for performance tracking and checkpointing across different data partitions and customized metrics.</summary>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-03T13:33:35Z</published>\n",
      "    <arxiv:primary_category term=\"cs.NE\"/>\n",
      "    <author>\n",
      "      <name>Junaid Malik</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Serkan Kiranyaz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Moncef Gabbouj</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.02286v3</id>\n",
      "    <title>Learning Kernel Tests Without Data Splitting</title>\n",
      "    <updated>2020-10-19T08:00:26Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.02286v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.02286v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Modern large-scale kernel-based tests such as maximum mean discrepancy (MMD) and kernelized Stein discrepancy (KSD) optimize kernel hyperparameters on a held-out sample via data splitting to obtain the most powerful test statistics. While data splitting results in a tractable null distribution, it suffers from a reduction in test power due to smaller test sample size. Inspired by the selective inference framework, we propose an approach that enables learning the hyperparameters and testing on the full sample without data splitting. Our approach can correctly calibrate the test in the presence of such dependency, and yield a test threshold in closed form. At the same significance level, our approach's test power is empirically larger than that of the data-splitting approach, regardless of its split proportion.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-03T14:07:39Z</published>\n",
      "    <arxiv:comment>24 (11+13) pages, 10 figures. Camera-Ready version. Accepted to the Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jonas M. Kübler</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wittawat Jitkrittum</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bernhard Schölkopf</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Krikamol Muandet</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01816v1</id>\n",
      "    <title>Age-Based Coded Computation for Bias Reduction in Distributed Learning</title>\n",
      "    <updated>2020-06-02T17:51:11Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01816v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01816v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Coded computation can be used to speed up distributed learning in the presence of straggling workers. Partial recovery of the gradient vector can further reduce the computation time at each iteration; however, this can result in biased estimators, which may slow down convergence, or even cause divergence. Estimator bias will be particularly prevalent when the straggling behavior is correlated over time, which results in the gradient estimators being dominated by a few fast servers. To mitigate biased estimators, we design a $timely$ dynamic encoding framework for partial recovery that includes an ordering operator that changes the codewords and computation orders at workers over time. To regulate the recovery frequencies, we adopt an $age$ metric in the design of the dynamic encoding scheme. We show through numerical results that the proposed dynamic encoding strategy increases the timeliness of the recovered computations, which as a result, reduces the bias in model updates, and accelerates the convergence compared to the conventional static partial recovery schemes.</summary>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-02T17:51:11Z</published>\n",
      "    <arxiv:primary_category term=\"cs.IT\"/>\n",
      "    <author>\n",
      "      <name>Emre Ozfatura</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Baturalp Buyukates</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Deniz Gunduz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sennur Ulukus</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01854v1</id>\n",
      "    <title>Event Arguments Extraction via Dilate Gated Convolutional Neural Network with Enhanced Local Features</title>\n",
      "    <updated>2020-06-02T18:05:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01854v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01854v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Event Extraction plays an important role in information-extraction to understand the world. Event extraction could be split into two subtasks: one is event trigger extraction, the other is event arguments extraction. However, the F-Score of event arguments extraction is much lower than that of event trigger extraction, i.e. in the most recent work, event trigger extraction achieves 80.7%, while event arguments extraction achieves only 58%. In pipelined structures, the difficulty of event arguments extraction lies in its lack of classification feature, and the much higher computation consumption. In this work, we proposed a novel Event Extraction approach based on multi-layer Dilate Gated Convolutional Neural Network (EE-DGCNN) which has fewer parameters. In addition, enhanced local information is incorporated into word features, to assign event arguments roles for triggers predicted by the first subtask. The numerical experiments demonstrated significant performance improvement beyond state-of-art event extraction approaches on real-world datasets. Further analysis of extraction procedure is presented, as well as experiments are conducted to analyze impact factors related to the performance improvement.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-02T18:05:34Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Zhigang Kan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Linbo Qiao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sen Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Feng Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Feng Huang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01997v1</id>\n",
      "    <title>Automatic Text Summarization of COVID-19 Medical Research Articles using BERT and GPT-2</title>\n",
      "    <updated>2020-06-03T00:54:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01997v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01997v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>With the COVID-19 pandemic, there is a growing urgency for medical community to keep up with the accelerating growth in the new coronavirus-related literature. As a result, the COVID-19 Open Research Dataset Challenge has released a corpus of scholarly articles and is calling for machine learning approaches to help bridging the gap between the researchers and the rapidly growing publications. Here, we take advantage of the recent advances in pre-trained NLP models, BERT and OpenAI GPT-2, to solve this challenge by performing text summarization on this dataset. We evaluate the results using ROUGE scores and visual inspection. Our model provides abstractive and comprehensive information based on keywords extracted from the original articles. Our work can help the the medical community, by providing succinct summaries of articles for which the abstract are not already available.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-03T00:54:44Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Virapat Kieuvongngam</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bowen Tan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yiming Niu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01022v2</id>\n",
      "    <title>A novel approach for multi-agent cooperative pursuit to capture grouped evaders</title>\n",
      "    <updated>2020-06-27T10:25:27Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01022v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01022v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>An approach of mobile multi-agent pursuit based on application of self-organizing feature map (SOFM) and along with that reinforcement learning based on agent group role membership function (AGRMF) model is proposed. This method promotes dynamic organization of the pursuers' groups and also makes pursuers' group evader according to their desire based on SOFM and AGRMF techniques. This helps to overcome the shortcomings of the pursuers that they cannot fully reorganize when the goal is too independent in process of AGRMF models operation. Besides, we also discuss a new reward function. After the formation of the group, reinforcement learning is applied to get the optimal solution for each agent. The results of each step in capturing process will finally affect the AGR membership function to speed up the convergence of the competitive neural network. The experiments result shows that this approach is more effective for the mobile agents to capture evaders.</summary>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-01T15:39:58Z</published>\n",
      "    <arxiv:comment>published paper's draft version</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.AI\"/>\n",
      "    <arxiv:journal_ref>Journal of Supercomputing, J Supercomput 76 (2020)</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Muhammad Zuhair Qadir</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Songhao Piao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Haiyang Jiang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mohammed El Habib Souidi</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1007/s11227-018-2591-3</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1007/s11227-018-2591-3\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01035v2</id>\n",
      "    <title>Data-Driven Prediction of Embryo Implantation Probability Using IVF Time-lapse Imaging</title>\n",
      "    <updated>2020-06-02T14:02:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01035v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01035v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The process of fertilizing a human egg outside the body in order to help those suffering from infertility to conceive is known as in vitro fertilization (IVF). Despite being the most effective method of assisted reproductive technology (ART), the average success rate of IVF is a mere 20-40%. One step that is critical to the success of the procedure is selecting which embryo to transfer to the patient, a process typically conducted manually and without any universally accepted and standardized criteria. In this paper we describe a novel data-driven system trained to directly predict embryo implantation probability from embryogenesis time-lapse imaging videos. Using retrospectively collected videos from 272 embryos, we demonstrate that, when compared to an external panel of embryologists, our algorithm results in a 12% increase of positive predictive value and a 29% increase of negative predictive value.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-01T16:04:08Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>David H. Silver</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Martin Feder</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yael Gold-Zamir</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Avital L. Polsky</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shahar Rosentraub</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Efrat Shachor</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Adi Weinberger</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pavlo Mazur</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Valery D. Zukin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alex M. Bronstein</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01044v1</id>\n",
      "    <title>Anomaly Detection Under Controlled Sensing Using Actor-Critic Reinforcement Learning</title>\n",
      "    <updated>2020-05-26T22:53:17Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01044v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01044v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We consider the problem of detecting anomalies among a given set of processes using their noisy binary sensor measurements. The noiseless sensor measurement corresponding to a normal process is 0, and the measurement is 1 if the process is anomalous. The decision-making algorithm is assumed to have no knowledge of the number of anomalous processes. The algorithm is allowed to choose a subset of the sensors at each time instant until the confidence level on the decision exceeds the desired value. Our objective is to design a sequential sensor selection policy that dynamically determines which processes to observe at each time and when to terminate the detection algorithm. The selection policy is designed such that the anomalous processes are detected with the desired confidence level while incurring minimum cost which comprises the delay in detection and the cost of sensing. We cast this problem as a sequential hypothesis testing problem within the framework of Markov decision processes, and solve it using the actor-critic deep reinforcement learning algorithm. This deep neural network-based algorithm offers a low complexity solution with good detection accuracy. We also study the effect of statistical dependence between the processes on the algorithm performance. Through numerical experiments, we show that our algorithm is able to adapt to any unknown statistical dependence pattern of the processes.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-26T22:53:17Z</published>\n",
      "    <arxiv:comment>5 pages, 2 figures, accepted at 2020 IEEE 21st International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Geethu Joseph</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>M. Cenk Gursoy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pramod K. Varshney</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01073v2</id>\n",
      "    <title>Analysis of Regularized Least Squares in Reproducing Kernel Krein Spaces</title>\n",
      "    <updated>2020-11-24T19:18:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01073v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01073v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we study the asymptotic properties of regularized least squares with indefinite kernels in reproducing kernel Krein spaces (RKKS). By introducing a bounded hyper-sphere constraint to such non-convex regularized risk minimization problem, we theoretically demonstrate that this problem has a globally optimal solution with a closed form on the sphere, which makes approximation analysis feasible in RKKS. Regarding to the original regularizer induced by the indefinite inner product, we modify traditional error decomposition techniques, prove convergence results for the introduced hypothesis error based on matrix perturbation theory, and derive learning rates of such regularized regression problem in RKKS. Under some conditions, the derived learning rates in RKKS are the same as that in reproducing kernel Hilbert spaces (RKHS), which is actually the first work on approximation analysis of regularized learning algorithms in RKKS.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-01T16:55:35Z</published>\n",
      "    <arxiv:comment>24 pages, 2 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Fanghui Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lei Shi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaolin Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jie Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Johan A. K. Suykens</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01096v3</id>\n",
      "    <title>Invariant Policy Optimization: Towards Stronger Generalization in Reinforcement Learning</title>\n",
      "    <updated>2020-11-09T09:54:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01096v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01096v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A fundamental challenge in reinforcement learning is to learn policies that generalize beyond the operating domains experienced during training. In this paper, we approach this challenge through the following invariance principle: an agent must find a representation such that there exists an action-predictor built on top of this representation that is simultaneously optimal across all training domains. Intuitively, the resulting invariant policy enhances generalization by finding causes of successful actions. We propose a novel learning algorithm, Invariant Policy Optimization (IPO), that implements this principle and learns an invariant policy during training. We compare our approach with standard policy gradient methods and demonstrate significant improvements in generalization performance on unseen domains for linear quadratic regulator and grid-world problems, and an example where a robot must learn to open doors with varying physical properties.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-01T17:28:19Z</published>\n",
      "    <arxiv:comment>16 pages, 4 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Anoopkumar Sonar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vincent Pacelli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anirudha Majumdar</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01112v2</id>\n",
      "    <title>Cascaded Text Generation with Markov Transformers</title>\n",
      "    <updated>2020-12-05T05:26:19Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01112v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01112v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The two dominant approaches to neural text generation are fully autoregressive models, using serial beam search decoding, and non-autoregressive models, using parallel decoding with no output dependencies. This work proposes an autoregressive model with sub-linear parallel time generation. Noting that conditional random fields with bounded context can be decoded in parallel, we propose an efficient cascaded decoding approach for generating high-quality output. To parameterize this cascade, we introduce a Markov transformer, a variant of the popular fully autoregressive model that allows us to simultaneously decode with specific autoregressive context cutoffs. This approach requires only a small modification from standard autoregressive training, while showing competitive accuracy/speed tradeoff compared to existing methods on five machine translation datasets.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-01T17:52:15Z</published>\n",
      "    <arxiv:comment>NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Yuntian Deng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexander M. Rush</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01125v2</id>\n",
      "    <title>Neural Network-Aided BCJR Algorithm for Joint Symbol Detection and Channel Decoding</title>\n",
      "    <updated>2020-07-21T15:56:27Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01125v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01125v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recently, deep learning-assisted communication systems have achieved many eye-catching results and attracted more and more researchers in this emerging field. Instead of completely replacing the functional blocks of communication systems with neural networks, a hybrid manner of BCJRNet symbol detection is proposed to combine the advantages of the BCJR algorithm and neural networks. However, its separate block design not only degrades the system performance but also results in additional hardware complexity. In this work, we propose a BCJR receiver for joint symbol detection and channel decoding. It can simultaneously utilize the trellis diagram and channel state information for a more accurate calculation of branch probability and thus achieve global optimum with 2.3 dB gain over separate block design. Furthermore, a dedicated neural network model is proposed to replace the channel-model-based computation of the BCJR receiver, which can avoid the requirements of perfect CSI and is more robust under CSI uncertainty with 1.0 dB gain.</summary>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-30T10:25:58Z</published>\n",
      "    <arxiv:comment>6 pages, six figures, accepted by 2020 IEEE International Workshop on Signal Processing Systems (SiPS)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IT\"/>\n",
      "    <author>\n",
      "      <name>Wen-Chiao Tsai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chieh-Fang Teng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Han-Mo Ou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>An-Yeu Wu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01130v3</id>\n",
      "    <title>Theory and Algorithms for Shapelet-based Multiple-Instance Learning</title>\n",
      "    <updated>2020-10-13T06:57:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01130v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01130v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose a new formulation of Multiple-Instance Learning (MIL), in which a unit of data consists of a set of instances called a bag. The goal is to find a good classifier of bags based on the similarity with a \"shapelet\" (or pattern), where the similarity of a bag with a shapelet is the maximum similarity of instances in the bag. In previous work, some of the training instances are chosen as shapelets with no theoretical justification. In our formulation, we use all possible, and thus infinitely many shapelets, resulting in a richer class of classifiers. We show that the formulation is tractable, that is, it can be reduced through Linear Programming Boosting (LPBoost) to Difference of Convex (DC) programs of finite (actually polynomial) size. Our theoretical result also gives justification to the heuristics of some of the previous work. The time complexity of the proposed algorithm highly depends on the size of the set of all instances in the training sample. To apply to the data containing a large number of instances, we also propose a heuristic option of the algorithm without the loss of the theoretical guarantee. Our empirical study demonstrates that our algorithm uniformly works for Shapelet Learning tasks on time-series classification and various MIL tasks with comparable accuracy to the existing methods. Moreover, we show that the proposed heuristics allow us to achieve the result with reasonable computational time.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-31T17:10:59Z</published>\n",
      "    <arxiv:comment>The full version of this paper is published in Neural Computation. arXiv admin note: substantial text overlap with arXiv:1811.08084</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Daiki Suehiro</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kohei Hatano</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eiji Takimoto</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shuji Yamamoto</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kenichi Bannai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Akiko Takeda</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1162/neco_a_01297</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1162/neco_a_01297\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01203v1</id>\n",
      "    <title>Hybrid Improved Document-level Embedding (HIDE)</title>\n",
      "    <updated>2020-06-01T19:09:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01203v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01203v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In recent times, word embeddings are taking a significant role in sentiment analysis. As the generation of word embeddings needs huge corpora, many applications use pretrained embeddings. In spite of the success, word embeddings suffers from certain drawbacks such as it does not capture sentiment information of a word, contextual information in terms of parts of speech tags and domain-specific information. In this work we propose HIDE a Hybrid Improved Document level Embedding which incorporates domain information, parts of speech information and sentiment information into existing word embeddings such as GloVe and Word2Vec. It combine improved word embeddings into document level embeddings. Further, Latent Semantic Analysis (LSA) has been used to represent documents as a vectors. HIDE is generated, combining LSA and document level embeddings, which is computed from improved word embeddings. We test HIDE with six different datasets and shown considerable improvement over the accuracy of existing pretrained word vectors such as GloVe and Word2Vec. We further compare our work with two existing document level sentiment analysis approaches. HIDE performs better than existing systems.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-01T19:09:13Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Satanik Mitra</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mamata Jenamani</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01671v2</id>\n",
      "    <title>A generalized linear joint trained framework for semi-supervised learning of sparse features</title>\n",
      "    <updated>2020-10-02T12:24:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01671v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01671v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The elastic-net is among the most widely used types of regularization algorithms, commonly associated with the problem of supervised generalized linear model estimation via penalized maximum likelihood. Its nice properties originate from a combination of $\\ell_1$ and $\\ell_2$ norms, which endow this method with the ability to select variables taking into account the correlations between them. In the last few years, semi-supervised approaches, that use both labeled and unlabeled data, have become an important component in the statistical research. Despite this interest, however, few researches have investigated semi-supervised elastic-net extensions. This paper introduces a novel solution for semi-supervised learning of sparse features in the context of generalized linear model estimation: the generalized semi-supervised elastic-net (s2net), which extends the supervised elastic-net method, with a general mathematical formulation that covers, but is not limited to, both regression and classification problems. We develop a flexible and fast implementation for s2net in R, and its advantages are illustrated using both real and synthetic data sets.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-02T14:44:48Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Juan C. Laria</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Line H. Clemmensen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bjarne K. Ersbøll</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01673v1</id>\n",
      "    <title>Learning Opinion Dynamics From Social Traces</title>\n",
      "    <updated>2020-06-02T14:48:17Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01673v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01673v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Opinion dynamics - the research field dealing with how people's opinions form and evolve in a social context - traditionally uses agent-based models to validate the implications of sociological theories. These models encode the causal mechanism that drives the opinion formation process, and have the advantage of being easy to interpret. However, as they do not exploit the availability of data, their predictive power is limited. Moreover, parameter calibration and model selection are manual and difficult tasks.\n",
      "  In this work we propose an inference mechanism for fitting a generative, agent-like model of opinion dynamics to real-world social traces. Given a set of observables (e.g., actions and interactions between agents), our model can recover the most-likely latent opinion trajectories that are compatible with the assumptions about the process dynamics. This type of model retains the benefits of agent-based ones (i.e., causal interpretation), while adding the ability to perform model selection and hypothesis testing on real data.\n",
      "  We showcase our proposal by translating a classical agent-based model of opinion dynamics into its generative counterpart. We then design an inference algorithm based on online expectation maximization to learn the latent parameters of the model. Such algorithm can recover the latent opinion trajectories from traces generated by the classical agent-based model. In addition, it can identify the most likely set of macro parameters used to generate a data trace, thus allowing testing of sociological hypotheses. Finally, we apply our model to real-world data from Reddit to explore the long-standing question about the impact of backfire effect. Our results suggest a low prominence of the effect in Reddit's political conversation.</summary>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-02T14:48:17Z</published>\n",
      "    <arxiv:comment>Published at KDD2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.SI\"/>\n",
      "    <arxiv:journal_ref>Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD2020)</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Corrado Monti</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gianmarco De Francisci Morales</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Francesco Bonchi</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3394486.3403119</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3394486.3403119\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01681v4</id>\n",
      "    <title>Neural Power Units</title>\n",
      "    <updated>2020-12-17T14:40:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01681v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01681v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Conventional Neural Networks can approximate simple arithmetic operations, but fail to generalize beyond the range of numbers that were seen during training. Neural Arithmetic Units aim to overcome this difficulty, but current arithmetic units are either limited to operate on positive numbers or can only represent a subset of arithmetic operations. We introduce the Neural Power Unit (NPU) that operates on the full domain of real numbers and is capable of learning arbitrary power functions in a single layer. The NPU thus fixes the shortcomings of existing arithmetic units and extends their expressivity. We achieve this by using complex arithmetic without requiring a conversion of the network to complex numbers. A simplification of the unit to the RealNPU yields a highly transparent model. We show that the NPUs outperform their competitors in terms of accuracy and sparsity on artificial arithmetic datasets, and that the RealNPU can discover the governing equations of a dynamical system only from data.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-02T14:58:07Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Niklas Heim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tomáš Pevný</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Václav Šmídl</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.00882v1</id>\n",
      "    <title>Should artificial agents ask for help in human-robot collaborative problem-solving?</title>\n",
      "    <updated>2020-05-25T09:15:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.00882v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.00882v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Transferring as fast as possible the functioning of our brain to artificial intelligence is an ambitious goal that would help advance the state of the art in AI and robotics. It is in this perspective that we propose to start from hypotheses derived from an empirical study in a human-robot interaction and to verify if they are validated in the same way for children as for a basic reinforcement learning algorithm. Thus, we check whether receiving help from an expert when solving a simple close-ended task (the Towers of Hanoï) allows to accelerate or not the learning of this task, depending on whether the intervention is canonical or requested by the player. Our experiences have allowed us to conclude that, whether requested or not, a Q-learning algorithm benefits in the same way from expert help as children do.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-25T09:15:30Z</published>\n",
      "    <arxiv:comment>Accepted at Brain-PIL Workshop - ICRA2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Adrien Bennetot</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vicky Charisi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Natalia Díaz-Rodríguez</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01293v1</id>\n",
      "    <title>From Sets to Multisets: Provable Variational Inference for Probabilistic Integer Submodular Models</title>\n",
      "    <updated>2020-06-01T22:20:45Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01293v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01293v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Submodular functions have been studied extensively in machine learning and data mining. In particular, the optimization of submodular functions over the integer lattice (integer submodular functions) has recently attracted much interest, because this domain relates naturally to many practical problem settings, such as multilabel graph cut, budget allocation and revenue maximization with discrete assignments. In contrast, the use of these functions for probabilistic modeling has received surprisingly little attention so far. In this work, we firstly propose the Generalized Multilinear Extension, a continuous DR-submodular extension for integer submodular functions. We study central properties of this extension and formulate a new probabilistic model which is defined through integer submodular functions. Then, we introduce a block-coordinate ascent algorithm to perform approximate inference for those class of models. Finally, we demonstrate its effectiveness and viability on several real-world social connection graph datasets with integer submodular objectives.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-01T22:20:45Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Aytunc Sahin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yatao Bian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joachim M. Buhmann</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andreas Krause</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01310v1</id>\n",
      "    <title>Eye Movements Biometrics: A Bibliometric Analysis from 2004 to 2019</title>\n",
      "    <updated>2020-06-01T23:14:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01310v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01310v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Person identification based on eye movements is getting more and more attention, as it is anti-spoofing resistant and can be useful for continuous authentication. Therefore, it is noteworthy for researchers to know who and what is relevant in the field, including authors, journals, conferences, and institutions. This paper presents a comprehensive quantitative overview of the field of eye movement biometrics using a bibliometric approach. All data and analyses are based on documents written in English published between 2004 and 2019. Scopus was used to perform information retrieval. This research focused on temporal evolution, leading authors, most cited papers, leading journals, competitions and collaboration networks.</summary>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-01T23:14:10Z</published>\n",
      "    <arxiv:comment>9 pages, 2 figures, journal</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.HC\"/>\n",
      "    <arxiv:journal_ref>International Journal of Computer Applications 176(24):1-9, May 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Antonio Ricardo Alexandre Brasil</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jefferson Oliveira Andrade</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Karin Satie Komati</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.5120/ijca2020920243</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.5120/ijca2020920243\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01446v1</id>\n",
      "    <title>Identification of hydrodynamic instability by convolutional neural networks</title>\n",
      "    <updated>2020-06-02T08:32:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01446v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01446v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The onset of hydrodynamic instabilities is of great importance in both industry and daily life, due to the dramatic mechanical and thermodynamic changes for different types of flow motions. In this paper, modern machine learning techniques, especially the convolutional neural networks (CNN), are applied to identify the transition between different flow motions raised by hydrodynamic instability, as well as critical non-dimensionalized parameters for characterizing this transit. CNN not only correctly predicts the critical transition values for both Taylor-Couette (TC) flow and Rayleigh- Bénard (RB) convection under various setups and conditions, but also shows an outstanding performance on robustness and noise-tolerance. In addition, key spatial features used for classifying different flow patterns are revealed by the principal component analysis.</summary>\n",
      "    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-02T08:32:08Z</published>\n",
      "    <arxiv:primary_category term=\"physics.comp-ph\"/>\n",
      "    <author>\n",
      "      <name>Wuyue Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liangrong Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yi Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liu Hong</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01451v1</id>\n",
      "    <title>Careful analysis of XRD patterns with Attention</title>\n",
      "    <updated>2020-06-02T08:44:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01451v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01451v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The important peaks related to the physical properties of a lithium ion rechargeable battery were extracted from the measured X ray diffraction spectrum by a convolutional neural network based on the Attention mechanism. Among the deep features, the lattice constant of the cathodic active material was selected as a cell voltage predictor, and the crystallographic behavior of the active anodic and cathodic materials revealed the rate property during the charge discharge states. The machine learning automatically selected the significant peaks from the experimental spectrum. Applying the Attention mechanism with appropriate objective variables in multi task trained models, one can selectively visualize the correlations between interesting physical properties. As the deep features are automatically defined, this approach can adapt to the conditions of various physical experiments.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-02T08:44:05Z</published>\n",
      "    <arxiv:comment>12 pages, 5 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Koichi Kano</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Takashi Segi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hiroshi Ozono</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01561v1</id>\n",
      "    <title>Studying The Effect of MIL Pooling Filters on MIL Tasks</title>\n",
      "    <updated>2020-06-02T12:33:03Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01561v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01561v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>There are different multiple instance learning (MIL) pooling filters used in MIL models. In this paper, we study the effect of different MIL pooling filters on the performance of MIL models in real world MIL tasks. We designed a neural network based MIL framework with 5 different MIL pooling filters: `max', `mean', `attention', `distribution' and `distribution with attention'. We also formulated 5 different MIL tasks on a real world lymph node metastases dataset. We found that the performance of our framework in a task is different for different filters. We also observed that the performances of the five pooling filters are also different from task to task. Hence, the selection of a correct MIL pooling filter for each MIL task is crucial for better performance. Furthermore, we noticed that models with `distribution' and `distribution with attention' pooling filters consistently perform well in almost all of the tasks. We attribute this phenomena to the amount of information captured by `distribution' based pooling filters. While point estimate based pooling filters, like `max' and `mean', produce point estimates of distributions, `distribution' based pooling filters capture the full information in distributions. Lastly, we compared the performance of our neural network model with `distribution' pooling filter with the performance of the best MIL methods in the literature on classical MIL datasets and our model outperformed the others.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-02T12:33:03Z</published>\n",
      "    <arxiv:comment>16 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Mustafa Umit Oner</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jared Marc Song Kye-Jet</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hwee Kuan Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wing-Kin Sung</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.01209v1</id>\n",
      "    <title>Learning Constraints for Structured Prediction Using Rectifier Networks</title>\n",
      "    <updated>2020-05-23T18:31:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.01209v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.01209v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy. However, designing good constraints often relies on domain expertise. In this paper, we study the problem of learning such constraints. We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables. Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-23T18:31:30Z</published>\n",
      "    <arxiv:comment>to be published in ACL 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Xingyuan Pan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maitrey Mehta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vivek Srikumar</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.02676v1</id>\n",
      "    <title>Exchangeable Neural ODE for Set Modeling</title>\n",
      "    <updated>2020-08-06T14:11:36Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.02676v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.02676v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Reasoning over an instance composed of a set of vectors, like a point cloud, requires that one accounts for intra-set dependent features among elements. However, since such instances are unordered, the elements' features should remain unchanged when the input's order is permuted. This property, permutation equivariance, is a challenging constraint for most neural architectures. While recent work has proposed global pooling and attention-based solutions, these may be limited in the way that intradependencies are captured in practice. In this work we propose a more general formulation to achieve permutation equivariance through ordinary differential equations (ODE). Our proposed module, Exchangeable Neural ODE (ExNODE), can be seamlessly applied for both discriminative and generative tasks. We also extend set modeling in the temporal dimension and propose a VAE based model for temporal set modeling. Extensive experiments demonstrate the efficacy of our method over strong baselines.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-06T14:11:36Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yang Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Haidong Yi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christopher M. Bender</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Siyuan Shan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Junier B. Oliva</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.02830v1</id>\n",
      "    <title>Unsupervised Cross-Domain Singing Voice Conversion</title>\n",
      "    <updated>2020-08-06T18:29:11Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.02830v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.02830v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present a wav-to-wav generative model for the task of singing voice conversion from any identity. Our method utilizes both an acoustic model, trained for the task of automatic speech recognition, together with melody extracted features to drive a waveform-based generator. The proposed generative architecture is invariant to the speaker's identity and can be trained to generate target singers from unlabeled training data, using either speech or singing sources. The model is optimized in an end-to-end fashion without any manual supervision, such as lyrics, musical notes or parallel samples. The proposed approach is fully-convolutional and can generate audio in real-time. Experiments show that our method significantly outperforms the baseline methods while generating convincingly better audio samples than alternative attempts.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-06T18:29:11Z</published>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Adam Polyak</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lior Wolf</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yossi Adi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yaniv Taigman</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.01670v1</id>\n",
      "    <title>Multi-stream RNN for Merchant Transaction Prediction</title>\n",
      "    <updated>2020-07-25T01:20:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.01670v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.01670v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recently, digital payment systems have significantly changed people's lifestyles. New challenges have surfaced in monitoring and guaranteeing the integrity of payment processing systems. One important task is to predict the future transaction statistics of each merchant. These predictions can thus be used to steer other tasks, ranging from fraud detection to recommendation. This problem is challenging as we need to predict not only multivariate time series but also multi-steps into the future. In this work, we propose a multi-stream RNN model for multi-step merchant transaction predictions tailored to these requirements. The proposed multi-stream RNN summarizes transaction data in different granularity and makes predictions for multiple steps in the future. Our extensive experimental results have demonstrated that the proposed model is capable of outperforming existing state-of-the-art methods.</summary>\n",
      "    <category term=\"q-fin.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-25T01:20:48Z</published>\n",
      "    <arxiv:comment>Accepted by KDD 2020 Workshop on Machine Learning in Finance</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"q-fin.ST\"/>\n",
      "    <author>\n",
      "      <name>Zhongfang Zhuang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chin-Chia Michael Yeh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liang Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wei Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Junpeng Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.01825v2</id>\n",
      "    <title>Robust Reinforcement Learning using Adversarial Populations</title>\n",
      "    <updated>2020-09-22T22:41:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.01825v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.01825v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across robotics benchmarks that the use of an adversarial population results in a more robust policy that also improves out-of-distribution generalization. Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-04T20:57:32Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Eugene Vinitsky</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuqing Du</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kanaad Parvate</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kathy Jang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pieter Abbeel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexandre Bayen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03127v1</id>\n",
      "    <title>A Machine of Few Words -- Interactive Speaker Recognition with Reinforcement Learning</title>\n",
      "    <updated>2020-08-07T12:44:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03127v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03127v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Speaker recognition is a well known and studied task in the speech processing domain. It has many applications, either for security or speaker adaptation of personal devices. In this paper, we present a new paradigm for automatic speaker recognition that we call Interactive Speaker Recognition (ISR). In this paradigm, the recognition system aims to incrementally build a representation of the speakers by requesting personalized utterances to be spoken in contrast to the standard text-dependent or text-independent schemes. To do so, we cast the speaker recognition task into a sequential decision-making problem that we solve with Reinforcement Learning. Using a standard dataset, we show that our method achieves excellent performance while using little speech signal amounts. This method could also be applied as an utterance selection mechanism for building speech synthesis systems.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-07T12:44:08Z</published>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Mathieu Seurin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Florian Strub</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Philippe Preux</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Olivier Pietquin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.02450v1</id>\n",
      "    <title>Training DNN Model with Secret Key for Model Protection</title>\n",
      "    <updated>2020-08-06T04:25:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.02450v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.02450v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we propose a model protection method by using block-wise pixel shuffling with a secret key as a preprocessing technique to input images for the first time. The protected model is built by training with such preprocessed images. Experiment results show that the performance of the protected model is close to that of non-protected models when the key is correct, while the accuracy is severely dropped when an incorrect key is given, and the proposed model protection is robust against not only brute-force attacks but also fine-tuning attacks, while maintaining almost the same performance accuracy as that of using a non-protected model.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-06T04:25:59Z</published>\n",
      "    <arxiv:comment>to appear in 2020 IEEE 9th Global Conference on Consumer Electronics (GCCE 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>MaungMaung AprilPyone</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hitoshi Kiya</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.02480v1</id>\n",
      "    <title>Mixing-Specific Data Augmentation Techniques for Improved Blind Violin/Piano Source Separation</title>\n",
      "    <updated>2020-08-06T07:02:24Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.02480v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.02480v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Blind music source separation has been a popular and active subject of research in both the music information retrieval and signal processing communities. To counter the lack of available multi-track data for supervised model training, a data augmentation method that creates artificial mixtures by combining tracks from different songs has been shown useful in recent works. Following this light, we examine further in this paper extended data augmentation methods that consider more sophisticated mixing settings employed in the modern music production routine, the relationship between the tracks to be combined, and factors of silence. As a case study, we consider the separation of violin and piano tracks in a violin piano ensemble, evaluating the performance in terms of common metrics, namely SDR, SIR, and SAR. In addition to examining the effectiveness of these new data augmentation methods, we also study the influence of the amount of training data. Our evaluation shows that the proposed mixing-specific data augmentation methods can help improve the performance of a deep learning-based model for source separation, especially in the case of small training data.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-06T07:02:24Z</published>\n",
      "    <arxiv:comment>Accepted to IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Ching-Yu Chiu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wen-Yi Hsiao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yin-Cheng Yeh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yi-Hsuan Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alvin Wen-Yu Su</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.02616v2</id>\n",
      "    <title>The Emergence of Adversarial Communication in Multi-Agent Reinforcement Learning</title>\n",
      "    <updated>2020-11-04T18:01:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.02616v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.02616v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Many real-world problems require the coordination of multiple autonomous agents. Recent work has shown the promise of Graph Neural Networks (GNNs) to learn explicit communication strategies that enable complex multi-agent coordination. These works use models of cooperative multi-agent systems whereby agents strive to achieve a shared global goal. When considering agents with self-interested local objectives, the standard design choice is to model these as separate learning systems (albeit sharing the same environment). Such a design choice, however, precludes the existence of a single, differentiable communication channel, and consequently prohibits the learning of inter-agent communication strategies. In this work, we address this gap by presenting a learning model that accommodates individual non-shared rewards and a differentiable communication channel that is common among all agents. We focus on the case where agents have self-interested objectives, and develop a learning algorithm that elicits the emergence of adversarial communications. We perform experiments on multi-agent coverage and path planning problems, and employ a post-hoc interpretability technique to visualize the messages that agents communicate to each other. We show how a single self-interested agent is capable of learning highly manipulative communication strategies that allows it to significantly outperform a cooperative team of agents.</summary>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-06T12:48:08Z</published>\n",
      "    <arxiv:comment>Accepted to Conference on Robot Learning (CoRL) 2020. Camera-ready version incorporating rebuttal</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.RO\"/>\n",
      "    <author>\n",
      "      <name>Jan Blumenkamp</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amanda Prorok</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.01951v1</id>\n",
      "    <title>MusPy: A Toolkit for Symbolic Music Generation</title>\n",
      "    <updated>2020-08-05T06:16:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.01951v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.01951v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we present MusPy, an open source Python library for symbolic music generation. MusPy provides easy-to-use tools for essential components in a music generation system, including dataset management, data I/O, data preprocessing and model evaluation. In order to showcase its potential, we present statistical analysis of the eleven datasets currently supported by MusPy. Moreover, we conduct a cross-dataset generalizability experiment by training an autoregressive model on each dataset and measuring held-out likelihood on the others---a process which is made easier by MusPy's dataset management system. The results provide a map of domain overlap between various commonly used datasets and show that some datasets contain more representative cross-genre samples than others. Along with the dataset analysis, these results might serve as a guide for choosing datasets in future research. Source code and documentation are available at https://github.com/salu133445/muspy .</summary>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-05T06:16:13Z</published>\n",
      "    <arxiv:comment>Accepted by International Society for Music Information Retrieval Conference (ISMIR), 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.SD\"/>\n",
      "    <author>\n",
      "      <name>Hao-Wen Dong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ke Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Julian McAuley</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Taylor Berg-Kirkpatrick</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.01973v1</id>\n",
      "    <title>MultiCheXNet: A Multi-Task Learning Deep Network For Pneumonia-like Diseases Diagnosis From X-ray Scans</title>\n",
      "    <updated>2020-08-05T07:45:24Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.01973v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.01973v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present MultiCheXNet, an end-to-end Multi-task learning model, that is able to take advantage of different X-rays data sets of Pneumonia-like diseases in one neural architecture, performing three tasks at the same time; diagnosis, segmentation and localization. The common encoder in our architecture can capture useful common features present in the different tasks. The common encoder has another advantage of efficient computations, which speeds up the inference time compared to separate models. The specialized decoders heads can then capture the task-specific features. We employ teacher forcing to address the issue of negative samples that hurt the segmentation and localization performance. Finally,we employ transfer learning to fine tune the classifier on unseen pneumonia-like diseases. The MTL architecture can be trained on joint or dis-joint labeled data sets. The training of the architecture follows a carefully designed protocol, that pre trains different sub-models on specialized datasets, before being integrated in the joint MTL model. Our experimental setup involves variety of data sets, where the baseline performance of the 3 tasks is compared to the MTL architecture performance. Moreover, we evaluate the transfer learning mode to COVID-19 data set,both from individual classifier model, and from MTL architecture classification head.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-05T07:45:24Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Abdullah Tarek Farag</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ahmed Raafat Abd El-Wahab</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mahmoud Nada</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mohamed Yasser Abd El-Hakeem</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Omar Sayed Mahmoud</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Reem Khaled Rashwan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ahmad El Sallab</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.01994v1</id>\n",
      "    <title>Improving End-to-End Speech-to-Intent Classification with Reptile</title>\n",
      "    <updated>2020-08-05T08:32:15Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.01994v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.01994v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>End-to-end spoken language understanding (SLU) systems have many advantages over conventional pipeline systems, but collecting in-domain speech data to train an end-to-end system is costly and time consuming. One question arises from this: how to train an end-to-end SLU with limited amounts of data? Many researchers have explored approaches that make use of other related data resources, typically by pre-training parts of the model on high-resource speech recognition. In this paper, we suggest improving the generalization performance of SLU models with a non-standard learning algorithm, Reptile. Though Reptile was originally proposed for model-agnostic meta learning, we argue that it can also be used to directly learn a target task and result in better generalization than conventional gradient descent. In this work, we employ Reptile to the task of end-to-end spoken intent classification. Experiments on four datasets of different languages and domains show improvement of intent prediction accuracy, both when Reptile is used alone and used in addition to pre-training.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-05T08:32:15Z</published>\n",
      "    <arxiv:comment>4 pages + 1 page references, 4 figures, 3 tables, 1 algorithm, accepted for presentation at InterSpeech2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Yusheng Tian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Philip John Gorinski</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.02033v5</id>\n",
      "    <title>Fast Adaptive Task Offloading in Edge Computing based on Meta Reinforcement Learning</title>\n",
      "    <updated>2020-10-24T10:04:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.02033v5\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.02033v5\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Multi-access edge computing (MEC) aims to extend cloud service to the network edge to reduce network traffic and service latency. A fundamental problem in MEC is how to efficiently offload heterogeneous tasks of mobile applications from user equipment (UE) to MEC hosts. Recently, many deep reinforcement learning (DRL) based methods have been proposed to learn offloading policies through interacting with the MEC environment that consists of UE, wireless channels, and MEC hosts. However, these methods have weak adaptability to new environments because they have low sample efficiency and need full retraining to learn updated policies for new environments. To overcome this weakness, we propose a task offloading method based on meta reinforcement learning, which can adapt fast to new environments with a small number of gradient updates and samples. We model mobile applications as Directed Acyclic Graphs (DAGs) and the offloading policy by a custom sequence-to-sequence (seq2seq) neural network. To efficiently train the seq2seq network, we propose a method that synergizes the first order approximation and clipped surrogate objective. The experimental results demonstrate that this new offloading method can reduce the latency by up to 25% compared to three baselines while being able to adapt fast to new environments.</summary>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-05T10:16:25Z</published>\n",
      "    <arxiv:comment>Accepted by IEEE Transaction on Parallel and Distributed Systems</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.DC\"/>\n",
      "    <author>\n",
      "      <name>Jin Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jia Hu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Geyong Min</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Albert Y. Zomaya</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nektarios Georgalas</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/TPDS.2020.3014896</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/TPDS.2020.3014896\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.02043v1</id>\n",
      "    <title>Learning Boost by Exploiting the Auxiliary Task in Multi-task Domain</title>\n",
      "    <updated>2020-08-05T10:56:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.02043v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.02043v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Learning two tasks in a single shared function has some benefits. Firstly by acquiring information from the second task, the shared function leverages useful information that could have been neglected or underestimated in the first task. Secondly, it helps to generalize the function that can be learned using generally applicable information for both tasks. To fully enjoy these benefits, Multi-task Learning (MTL) has long been researched in various domains such as computer vision, language understanding, and speech synthesis. While MTL benefits from the positive transfer of information from multiple tasks, in a real environment, tasks inevitably have a conflict between them during the learning phase, called negative transfer. The negative transfer hampers function from achieving the optimality and degrades the performance. To solve the problem of the task conflict, previous works only suggested partial solutions that are not fundamental, but ad-hoc. A common approach is using a weighted sum of losses. The weights are adjusted to induce positive transfer. Paradoxically, this kind of solution acknowledges the problem of negative transfer and cannot remove it unless the weight of the task is set to zero. Therefore, these previous methods had limited success. In this paper, we introduce a novel approach that can drive positive transfer and suppress negative transfer by leveraging class-wise weights in the learning process. The weights act as an arbitrator of the fundamental unit of information to determine its positive or negative status to the main task.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-05T10:56:56Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jonghwa Yim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sang Hwan Kim</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.02312v4</id>\n",
      "    <title>Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs</title>\n",
      "    <updated>2020-08-19T06:04:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.02312v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.02312v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>To have a better understanding and usage of Convolution Neural Networks (CNNs), the visualization and interpretation of CNNs has attracted increasing attention in recent years. In particular, several Class Activation Mapping (CAM) methods have been proposed to discover the connection between CNN's decision and image regions. In spite of the reasonable visualization, lack of clear and sufficient theoretical support is the main limitation of these methods. In this paper, we introduce two axioms -- Conservation and Sensitivity -- to the visualization paradigm of the CAM methods. Meanwhile, a dedicated Axiom-based Grad-CAM (XGrad-CAM) is proposed to satisfy these axioms as much as possible. Experiments demonstrate that XGrad-CAM is an enhanced version of Grad-CAM in terms of conservation and sensitivity. It is able to achieve better visualization performance than Grad-CAM, while also be class-discriminative and easy-to-implement compared with Grad-CAM++ and Ablation-CAM. The code is available at https://github.com/Fu0511/XGrad-CAM.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-05T18:42:33Z</published>\n",
      "    <arxiv:comment>BMVC 2020 (Oral presentation). Code is avaliable at: https://github.com/Fu0511/XGrad-CAM</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Ruigang Fu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qingyong Hu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaohu Dong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yulan Guo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yinghui Gao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Biao Li</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.02340v2</id>\n",
      "    <title>Global Voxel Transformer Networks for Augmented Microscopy</title>\n",
      "    <updated>2020-11-23T16:45:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.02340v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.02340v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Advances in deep learning have led to remarkable success in augmented microscopy, enabling us to obtain high-quality microscope images without using expensive microscopy hardware and sample preparation techniques. However, current deep learning models for augmented microscopy are mostly U-Net based neural networks, thus sharing certain drawbacks that limit the performance. In this work, we introduce global voxel transformer networks (GVTNets), an advanced deep learning tool for augmented microscopy that overcomes intrinsic limitations of the current U-Net based models and achieves improved performance. GVTNets are built on global voxel transformer operators (GVTOs), which are able to aggregate global information, as opposed to local operators like convolutions. We apply the proposed methods on existing datasets for three different augmented microscopy tasks under various settings. The performance is significantly and consistently better than previous U-Net based approaches.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-05T20:11:15Z</published>\n",
      "    <arxiv:comment>Supplementary Material: https://documentcloud.adobe.com/link/track?uri=urn:aaid:scds:US:9fcf9e0d-6ea2-470b-8a89-ed09ac634ef8</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Zhengyang Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yaochen Xie</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shuiwang Ji</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.02447v3</id>\n",
      "    <title>Functional Regularization for Representation Learning: A Unified Theoretical Perspective</title>\n",
      "    <updated>2020-10-22T00:54:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.02447v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.02447v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Unsupervised and self-supervised learning approaches have become a crucial tool to learn representations for downstream prediction tasks. While these approaches are widely used in practice and achieve impressive empirical gains, their theoretical understanding largely lags behind. Towards bridging this gap, we present a unifying perspective where several such approaches can be viewed as imposing a regularization on the representation via a learnable function using unlabeled data. We propose a discriminative theoretical framework for analyzing the sample complexity of these approaches, which generalizes the framework of (Balcan and Blum, 2010) to allow learnable regularization functions. Our sample complexity bounds show that, with carefully chosen hypothesis classes to exploit the structure in the data, these learnable regularization functions can prune the hypothesis space, and help reduce the amount of labeled data needed. We then provide two concrete examples of functional regularization, one using auto-encoders and the other using masked self-supervision, and apply our framework to quantify the reduction in the sample complexity bound of labeled data. We also provide complementary empirical results to support our analysis.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-06T04:06:04Z</published>\n",
      "    <arxiv:comment>Accepted at NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Siddhant Garg</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yingyu Liang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03297v1</id>\n",
      "    <title>Multi-Stage Optimized Machine Learning Framework for Network Intrusion Detection</title>\n",
      "    <updated>2020-08-09T03:18:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03297v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03297v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Cyber-security garnered significant attention due to the increased dependency of individuals and organizations on the Internet and their concern about the security and privacy of their online activities. Several previous machine learning (ML)-based network intrusion detection systems (NIDSs) have been developed to protect against malicious online behavior. This paper proposes a novel multi-stage optimized ML-based NIDS framework that reduces computational complexity while maintaining its detection performance. This work studies the impact of oversampling techniques on the models' training sample size and determines the minimal suitable training sample size. Furthermore, it compares between two feature selection techniques, information gain and correlation-based, and explores their effect on detection performance and time complexity. Moreover, different ML hyper-parameter optimization techniques are investigated to enhance the NIDS's performance. The performance of the proposed framework is evaluated using two recent intrusion detection datasets, the CICIDS 2017 and the UNSW-NB 2015 datasets. Experimental results show that the proposed model significantly reduces the required training sample size (up to 74%) and feature set size (up to 50%). Moreover, the model performance is enhanced with hyper-parameter optimization with detection accuracies over 99% for both datasets, outperforming recent literature works by 1-2% higher accuracy and 1-2% lower false alarm rate.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-09T03:18:00Z</published>\n",
      "    <arxiv:comment>14 Pages, 13 Figures, 4 tables, Published IEEE Transactions on Network and Service Management ( Early Access )</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <arxiv:journal_ref>Electronic ISSN: 1932-4537</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>MohammadNoor Injadat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abdallah Moubayed</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ali Bou Nassif</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abdallah Shami</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/TNSM.2020.3014929</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/TNSM.2020.3014929\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03323v3</id>\n",
      "    <title>COVID-19 in differential diagnosis of online symptom assessments</title>\n",
      "    <updated>2020-11-30T22:13:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03323v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03323v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The COVID-19 pandemic has magnified an already existing trend of people looking for healthcare solutions online. One class of solutions are symptom checkers, which have become very popular in the context of COVID-19. Traditional symptom checkers, however, are based on manually curated expert systems that are inflexible and hard to modify, especially in a quickly changing situation like the one we are facing today. That is why all COVID-19 existing solutions are manual symptom checkers that can only estimate the probability of this disease and cannot contemplate alternative hypothesis or come up with a differential diagnosis. While machine learning offers an alternative, the lack of reliable data does not make it easy to apply to COVID-19 either. In this paper we present an approach that combines the strengths of traditional AI expert systems and novel deep learning models. In doing so we can leverage prior knowledge as well as any amount of existing data to quickly derive models that best adapt to the current state of the world and latest scientific knowledge. We use the approach to train a COVID-19 aware differential diagnosis model that can be used for medical decision support both for doctors or patients. We show that our approach is able to accurately model new incoming data about COVID-19 while still preserving accuracy on conditions that had been modeled in the past. While our approach shows evident and clear advantages for an extreme situation like the one we are currently facing, we also show that its flexibility generalizes beyond this concrete, but very important, example.</summary>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-07T18:10:42Z</published>\n",
      "    <arxiv:comment>Accepted at the Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended Abstract</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.AI\"/>\n",
      "    <author>\n",
      "      <name>Anitha Kannan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Richard Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vignesh Venkataraman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Geoffrey J. Tso</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xavier Amatriain</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03356v1</id>\n",
      "    <title>X-Ray bone abnormalities detection using MURA dataset</title>\n",
      "    <updated>2020-08-07T19:58:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03356v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03356v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We introduce the deep network trained on the MURA dataset from the Stanford University released in 2017. Our system is able to detect bone abnormalities on the radiographs and visualise such zones. We found that our solution has the accuracy comparable to the best results that have been achieved by other development teams that used MURA dataset, in particular the overall Kappa score that was achieved by our team is about 0.942 on the wrist, 0.862 on the hand and o.735 on the shoulder (compared to the best available results to this moment on the official web-site 0.931, 0.851 and 0.729 accordingly). However, despite the good results there are a lot of directions for the future enhancement of the proposed technology. We see a big potential in the further development computer aided systems (CAD) for the radiographs as the one that will help practical specialists diagnose bone fractures as well as bone oncology cases faster and with the higher accuracy.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-07T19:58:56Z</published>\n",
      "    <arxiv:comment>12 pages, 3 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>A. Solovyova</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>I. Solovyov</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03397v2</id>\n",
      "    <title>Navigating the landscape of COVID-19 research through literature analysis: A bird's eye view</title>\n",
      "    <updated>2020-09-11T21:01:27Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03397v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03397v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Timely access to accurate scientific literature in the battle with the ongoing COVID-19 pandemic is critical. This unprecedented public health risk has motivated research towards understanding the disease in general, identifying drugs to treat the disease, developing potential vaccines, etc. This has given rise to a rapidly growing body of literature that doubles in number of publications every 20 days as of May 2020. Providing medical professionals with means to quickly analyze the literature and discover growing areas of knowledge is necessary for addressing their question and information needs.\n",
      "  In this study we analyze the LitCovid collection, 13,369 COVID-19 related articles found in PubMed as of May 15th, 2020 with the purpose of examining the landscape of literature and presenting it in a format that facilitates information navigation and understanding. We do that by applying state-of-the-art named entity recognition, classification, clustering and other NLP techniques. By applying NER tools, we capture relevant bioentities (such as diseases, internal body organs, etc.) and assess the strength of their relationship with COVID-19 by the extent they are discussed in the corpus. We also collect a variety of symptoms and co-morbidities discussed in reference to COVID-19. Our clustering algorithm identifies topics represented by groups of related terms, and computes clusters corresponding to documents associated with the topic terms. Among the topics we observe several that persist through the duration of multiple weeks and have numerous associated documents, as well several that appear as emerging topics with fewer documents. All the tools and data are publicly available, and this framework can be applied to any literature collection. Taken together, these analyses produce a comprehensive, synthesized view of COVID-19 research to facilitate knowledge discovery from literature.</summary>\n",
      "    <category term=\"cs.DL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-07T23:39:29Z</published>\n",
      "    <arxiv:comment>10 pages, 8 Figures, Submitted to KDD 2020 Health Day</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.DL\"/>\n",
      "    <arxiv:journal_ref>KDD 2020 Health Day: AI for COVID, August 23-27, 2020, Virtual Conference, CA, US</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Lana Yeganova</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rezarta Islamaj</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qingyu Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Robert Leaman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexis Allot</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chin-Hsuan Wei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Donald C. Comeau</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Won Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yifan Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>W. John Wilbur</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhiyong Lu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03399v1</id>\n",
      "    <title>Nyström Approximation with Nonnegative Matrix Factorization</title>\n",
      "    <updated>2020-08-07T23:52:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03399v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03399v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Motivated by the needs of estimating the proximity clustering with partial distance measurements from vantage points or landmarks for remote networked systems, we show that the proximity clustering problem can be effectively formulated as the Nyström approximation problem, which solves the kernel K-means clustering problem in the complex space. We implement the Nyström approximation based on a landmark based Nonnegative Matrix Factorization (NMF) process. Evaluation results show that the proposed method finds nearly optimal clustering quality on both synthetic and real-world data sets as we vary the range of parameter choices and network conditions.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-07T23:52:59Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yongquan Fu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.02956v2</id>\n",
      "    <title>Bootstrapping Neural Processes</title>\n",
      "    <updated>2020-10-27T04:06:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.02956v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.02956v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Unlike in the traditional statistical modeling for which a user typically hand-specify a prior, Neural Processes (NPs) implicitly define a broad class of stochastic processes with neural networks. Given a data stream, NP learns a stochastic process that best describes the data. While this \"data-driven\" way of learning stochastic processes has proven to handle various types of data, NPs still rely on an assumption that uncertainty in stochastic processes is modeled by a single latent variable, which potentially limits the flexibility. To this end, we propose the Boostrapping Neural Process (BNP), a novel extension of the NP family using the bootstrap. The bootstrap is a classical data-driven technique for estimating uncertainty, which allows BNP to learn the stochasticity in NPs without assuming a particular form. We demonstrate the efficacy of BNP on various types of data and its robustness in the presence of model-data mismatch.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-07T02:23:34Z</published>\n",
      "    <arxiv:comment>Published in Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS 2020) Code is available at https://github.com/juho-lee/bnp</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Juho Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yoonho Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jungtaek Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eunho Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sung Ju Hwang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yee Whye Teh</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.02999v2</id>\n",
      "    <title>Single-stage intake gesture detection using CTC loss and extended prefix beam search</title>\n",
      "    <updated>2020-11-21T01:05:45Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.02999v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.02999v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Accurate detection of individual intake gestures is a key step towards automatic dietary monitoring. Both inertial sensor data of wrist movements and video data depicting the upper body have been used for this purpose. The most advanced approaches to date use a two-stage approach, in which (i) frame-level intake probabilities are learned from the sensor data using a deep neural network, and then (ii) sparse intake events are detected by finding the maxima of the frame-level probabilities. In this study, we propose a single-stage approach which directly decodes the probabilities learned from sensor data into sparse intake detections. This is achieved by weakly supervised training using Connectionist Temporal Classification (CTC) loss, and decoding using a novel extended prefix beam search decoding algorithm. Benefits of this approach include (i) end-to-end training for detections, (ii) simplified timing requirements for intake gesture labels, and (iii) improved detection performance compared to existing approaches. Across two separate datasets, we achieve relative $F_1$ score improvements between 1.9% and 6.2% over the two-stage approach for intake detection and eating/drinking detection tasks, for both video and inertial sensors.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-07T06:04:25Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Philipp V. Rouast</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marc T. P. Adam</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.02883v1</id>\n",
      "    <title>Stronger and Faster Wasserstein Adversarial Attacks</title>\n",
      "    <updated>2020-08-06T21:36:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.02883v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.02883v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep models, while being extremely flexible and accurate, are surprisingly vulnerable to \"small, imperceptible\" perturbations known as adversarial attacks. While the majority of existing attacks focus on measuring perturbations under the $\\ell_p$ metric, Wasserstein distance, which takes geometry in pixel space into account, has long been known to be a suitable metric for measuring image quality and has recently risen as a compelling alternative to the $\\ell_p$ metric in adversarial attacks. However, constructing an effective attack under the Wasserstein metric is computationally much more challenging and calls for better optimization algorithms. We address this gap in two ways: (a) we develop an exact yet efficient projection operator to enable a stronger projected gradient attack; (b) we show that the Frank-Wolfe method equipped with a suitable linear minimization oracle works extremely fast under Wasserstein constraints. Our algorithms not only converge faster but also generate much stronger attacks. For instance, we decrease the accuracy of a residual network on CIFAR-10 to $3.4\\%$ within a Wasserstein perturbation ball of radius $0.005$, in contrast to $65.6\\%$ using the previous Wasserstein attack based on an \\emph{approximate} projection operator. Furthermore, employing our stronger attacks in adversarial training significantly improves the robustness of adversarially trained models.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-06T21:36:12Z</published>\n",
      "    <arxiv:comment>30 pages, accepted to ICML 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Kaiwen Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Allen Houze Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yaoliang Yu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.02891v1</id>\n",
      "    <title>Mesh sampling and weighting for the hyperreduction of nonlinear Petrov-Galerkin reduced-order models with local reduced-order bases</title>\n",
      "    <updated>2020-08-06T22:20:29Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.02891v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.02891v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The energy-conserving sampling and weighting (ECSW) method is a hyperreduction method originally developed for accelerating the performance of Galerkin projection-based reduced-order models (PROMs) associated with large-scale finite element models, when the underlying projected operators need to be frequently recomputed as in parametric and/or nonlinear problems. In this paper, this hyperreduction method is extended to Petrov-Galerkin PROMs where the underlying high-dimensional models can be associated with arbitrary finite element, finite volume, and finite difference semi-discretization methods. Its scope is also extended to cover local PROMs based on piecewise-affine approximation subspaces, such as those designed for mitigating the Kolmogorov $n$-width barrier issue associated with convection-dominated flow problems. The resulting ECSW method is shown in this paper to be robust and accurate. In particular, its offline phase is shown to be fast and parallelizable, and the potential of its online phase for large-scale applications of industrial relevance is demonstrated for turbulent flow problems with $O(10^7)$ and $O(10^8)$ degrees of freedom. For such problems, the online part of the ECSW method proposed in this paper for Petrov-Galerkin PROMs is shown to enable wall-clock time and CPU time speedup factors of several orders of magnitude while delivering exceptional accuracy.</summary>\n",
      "    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-06T22:20:29Z</published>\n",
      "    <arxiv:comment>29 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"math.NA\"/>\n",
      "    <author>\n",
      "      <name>Sebastian Grimberg</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Charbel Farhat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Radek Tezaur</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Charbel Bou-Mosleh</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03875v1</id>\n",
      "    <title>RocNet: Recursive Octree Network for Efficient 3D Deep Representation</title>\n",
      "    <updated>2020-08-10T03:02:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03875v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03875v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We introduce a deep recursive octree network for the compression of 3D voxel data. Our network compresses a voxel grid of any size down to a very small latent space in an autoencoder-like network. We show results for compressing 32, 64 and 128 grids down to just 80 floats in the latent space. We demonstrate the effectiveness and efficiency of our proposed method on several publicly available datasets with three experiments: 3D shape classification, 3D shape reconstruction, and shape generation. Experimental results show that our algorithm maintains accuracy while consuming less memory with shorter training times compared to existing methods, especially in 3D reconstruction tasks.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-10T03:02:10Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Juncheng Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Steven Mills</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Brendan McCane</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03964v1</id>\n",
      "    <title>DQI: A Guide to Benchmark Evaluation</title>\n",
      "    <updated>2020-08-10T08:38:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03964v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03964v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A `state of the art' model A surpasses humans in a benchmark B, but fails on similar benchmarks C, D, and E. What does B have that the other benchmarks do not? Recent research provides the answer: spurious bias. However, developing A to solve benchmarks B through E does not guarantee that it will solve future benchmarks. To progress towards a model that `truly learns' an underlying task, we need to quantify the differences between successive benchmarks, as opposed to existing binary and black-box approaches. We propose a novel approach to solve this underexplored task of quantifying benchmark quality by debuting a data quality metric: DQI.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-10T08:38:55Z</published>\n",
      "    <arxiv:comment>ICML UDL 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Swaroop Mishra</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anjana Arunkumar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bhavdeep Sachdeva</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chris Bryan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chitta Baral</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03989v1</id>\n",
      "    <title>Optimal Bayesian experimental design for subsurface flow problems</title>\n",
      "    <updated>2020-08-10T09:42:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03989v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03989v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Optimal Bayesian design techniques provide an estimate for the best parameters of an experiment in order to maximize the value of measurements prior to the actual collection of data. In other words, these techniques explore the space of possible observations and determine an experimental setup that produces maximum information about the system parameters on average. Generally, optimal Bayesian design formulations result in multiple high-dimensional integrals that are difficult to evaluate without incurring significant computational costs as each integration point corresponds to solving a coupled system of partial differential equations. In the present work, we propose a novel approach for development of polynomial chaos expansion (PCE) surrogate model for the design utility function. In particular, we demonstrate how the orthogonality of PCE basis polynomials can be utilized in order to replace the expensive integration over the space of possible observations by direct construction of PCE approximation for the expected information gain. This novel technique enables the derivation of a reasonable quality response surface for the targeted objective function with a computational budget comparable to several single-point evaluations. Therefore, the proposed technique reduces dramatically the overall cost of optimal Bayesian experimental design. We evaluate this alternative formulation utilizing PCE on few numerical test cases with various levels of complexity to illustrate the computational advantages of the proposed approach.</summary>\n",
      "    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-10T09:42:59Z</published>\n",
      "    <arxiv:comment>30 pages, 9 figures. Published in Computer Methods in Applied Mechanics and Engineering</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"physics.comp-ph\"/>\n",
      "    <arxiv:journal_ref>Computer Methods in Applied Mechanics and Engineering, Volume 370, 2020, 113208, ISSN 0045-7825</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Alexander Tarakanov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ahmed H. Elsheikh</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1016/j.cma.2020.113208</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1016/j.cma.2020.113208\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.04387v1</id>\n",
      "    <title>Probability Link Models with Symmetric Information Divergence</title>\n",
      "    <updated>2020-08-10T19:49:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.04387v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.04387v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper introduces link functions for transforming one probability distribution to another such that the Kullback-Leibler and Rényi divergences between the two distributions are symmetric. Two general classes of link models are proposed. The first model links two survival functions and is applicable to models such as the proportional odds and change point, which are used in survival analysis and reliability modeling. A prototype application involving the proportional odds model demonstrates advantages of symmetric divergence measures over asymmetric measures for assessing the efficacy of features and for model averaging purposes. The advantages include providing unique ranks for models and unique information weights for model averaging with one-half as much computation requirement of asymmetric divergences. The second model links two cumulative probability distribution functions. This model produces a generalized location model which are continuous counterparts of the binary probability models such as probit and logit models. Examples include the generalized probit and logit models which have appeared in the survival analysis literature, and a generalized Laplace model and a generalized Student-$t$ model, which are survival time models corresponding to the respective binary probability models. Lastly, extensions to symmetric divergence between survival functions and conditions for copula dependence information are presented.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-10T19:49:51Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Majid Asadi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Karthik Devarajan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nader Ebrahimi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ehsan Soofi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lauren Spirko-Burns</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.04391v2</id>\n",
      "    <title>DeepDrummer : Generating Drum Loops using Deep Learning and a Human in the Loop</title>\n",
      "    <updated>2020-08-26T21:09:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.04391v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.04391v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>DeepDrummer is a drum loop generation tool that uses active learning to learn the preferences (or current artistic intentions) of a human user from a small number of interactions. The principal goal of this tool is to enable an efficient exploration of new musical ideas. We train a deep neural network classifier on audio data and show how it can be used as the core component of a system that generates drum loops based on few prior beliefs as to how these loops should be structured.\n",
      "  We aim to build a system that can converge to meaningful results even with a limited number of interactions with the user. This property enables our method to be used from a cold start situation (no pre-existing dataset), or starting from a collection of audio samples provided by the user. In a proof of concept study with 25 participants, we empirically demonstrate that DeepDrummer is able to converge towards the preference of our subjects after a small number of interactions.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-10T20:04:15Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Guillaume Alain</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maxime Chevalier-Boisvert</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Frederic Osterrath</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Remi Piche-Taillefer</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.04407v1</id>\n",
      "    <title>Fault-Tolerant Control of Degrading Systems with On-Policy Reinforcement Learning</title>\n",
      "    <updated>2020-08-10T20:42:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.04407v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.04407v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose a novel adaptive reinforcement learning control approach for fault tolerant control of degrading systems that is not preceded by a fault detection and diagnosis step. Therefore, \\textit{a priori} knowledge of faults that may occur in the system is not required. The adaptive scheme combines online and offline learning of the on-policy control method to improve exploration and sample efficiency, while guaranteeing stable learning. The offline learning phase is performed using a data-driven model of the system, which is frequently updated to track the system's operating conditions. We conduct experiments on an aircraft fuel transfer system to demonstrate the effectiveness of our approach.</summary>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-10T20:42:59Z</published>\n",
      "    <arxiv:comment>Published in IFAC World Congress 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.SY\"/>\n",
      "    <author>\n",
      "      <name>Ibrahim Ahmed</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marcos Quiñones-Grueiro</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gautam Biswas</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.04419v1</id>\n",
      "    <title>Balanced k-Means Clustering on an Adiabatic Quantum Computer</title>\n",
      "    <updated>2020-08-10T21:15:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.04419v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.04419v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Adiabatic quantum computers are a promising platform for approximately solving challenging optimization problems. We present a quantum approach to solving the balanced $k$-means clustering training problem on the D-Wave 2000Q adiabatic quantum computer. Existing classical approaches scale poorly for large datasets and only guarantee a locally optimal solution. We show that our quantum approach better targets the global solution of the training problem, while achieving better theoretic scalability on large datasets. We test our quantum approach on a number of small problems, and observe clustering performance similar to the best classical algorithms.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"physics.data-an\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-10T21:15:47Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Davis Arthur</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Prasanna Date</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.04757v4</id>\n",
      "    <title>Transfer Learning for Protein Structure Classification at Low Resolution</title>\n",
      "    <updated>2020-08-31T17:02:33Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.04757v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.04757v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Structure determination is key to understanding protein function at a molecular level. Whilst significant advances have been made in predicting structure and function from amino acid sequence, researchers must still rely on expensive, time-consuming analytical methods to visualise detailed protein conformation. In this study, we demonstrate that it is possible to make accurate ($\\geq$80%) predictions of protein class and architecture from structures determined at low ($&gt;$3A) resolution, using a deep convolutional neural network trained on high-resolution ($\\leq$3A) structures represented as 2D matrices. Thus, we provide proof of concept for high-speed, low-cost protein structure classification at low resolution, and a basis for extension to prediction of function. We investigate the impact of the input representation on classification performance, showing that side-chain information may not be necessary for fine-grained structure predictions. Finally, we confirm that high-resolution, low-resolution and NMR-determined structures inhabit a common feature space, and thus provide a theoretical foundation for boosting with single-image super-resolution.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.BM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-11T15:01:32Z</published>\n",
      "    <arxiv:comment>9 pages excluding references and appendices</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Alexander Hudson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shaogang Gong</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.04820v2</id>\n",
      "    <title>LTIatCMU at SemEval-2020 Task 11: Incorporating Multi-Level Features for Multi-Granular Propaganda Span Identification</title>\n",
      "    <updated>2020-08-20T15:14:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.04820v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.04820v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper we describe our submission for the task of Propaganda Span Identification in news articles. We introduce a BERT-BiLSTM based span-level propaganda classification model that identifies which token spans within the sentence are indicative of propaganda. The \"multi-granular\" model incorporates linguistic knowledge at various levels of text granularity, including word, sentence and document level syntactic, semantic and pragmatic affect features, which significantly improve model performance, compared to its language-agnostic variant. To facilitate better representation learning, we also collect a corpus of 10k news articles, and use it for fine-tuning the model. The final model is a majority-voting ensemble which learns different propaganda class boundaries by leveraging different subsets of incorporated knowledge and attains $4^{th}$ position on the test leaderboard. Our final model and code is released at https://github.com/sopu/PropagandaSemEval2020.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-11T16:14:47Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Sopan Khosla</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rishabh Joshi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ritam Dutt</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alan W Black</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yulia Tsvetkov</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03493v1</id>\n",
      "    <title>An AI-powered blood test to detect cancer using nanoDSF</title>\n",
      "    <updated>2020-08-08T11:20:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03493v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03493v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We describe a novel cancer diagnostic method based on plasma denaturation profiles obtained by a non-conventional use of Differential Scanning Fluorimetry. We show that 84 glioma patients and 63 healthy controls can be automatically classified using denaturation profiles with the help of machine learning algorithms with 92% accuracy. Proposed high throughput workflow can be applied to any type of cancer and could become a powerful pan-cancer diagnostic and monitoring tool from a simple blood test.</summary>\n",
      "    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.TO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-08T11:20:53Z</published>\n",
      "    <arxiv:primary_category term=\"q-bio.QM\"/>\n",
      "    <author>\n",
      "      <name>Philipp O. Tsvetkov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rémi Eyraud</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stéphane Ayache</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anton A. Bougaev</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Soazig Malesinski</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hamed Benazha</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Svetlana Gorokhova</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christophe Buffat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Caroline Dehais</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marc Sanson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Franck Bielle</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dominique Figarella-Branger</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Olivier Chinot</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Emeline Tabouret</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>François Devred</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03501v1</id>\n",
      "    <title>Why to \"grow\" and \"harvest\" deep learning models?</title>\n",
      "    <updated>2020-08-08T11:55:24Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03501v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03501v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Current expectations from training deep learning models with gradient-based methods include: 1) transparency; 2) high convergence rates; 3) high inductive biases. While the state-of-art methods with adaptive learning rate schedules are fast, they still fail to meet the other two requirements. We suggest reconsidering neural network models in terms of single-species population dynamics where adaptation comes naturally from open-ended processes of \"growth\" and \"harvesting\". We show that the stochastic gradient descent (SGD) with two balanced pre-defined values of per capita growth and harvesting rates outperform the most common adaptive gradient methods in all of the three requirements.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-08T11:55:24Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Ilona Kulikovskikh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tarzan Legović</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03519v1</id>\n",
      "    <title>Learning abstract structure for drawing by efficient motor program induction</title>\n",
      "    <updated>2020-08-08T13:31:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03519v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03519v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Humans flexibly solve new problems that differ qualitatively from those they were trained on. This ability to generalize is supported by learned concepts that capture structure common across different problems. Here we develop a naturalistic drawing task to study how humans rapidly acquire structured prior knowledge. The task requires drawing visual objects that share underlying structure, based on a set of composable geometric rules. We show that people spontaneously learn abstract drawing procedures that support generalization, and propose a model of how learners can discover these reusable drawing programs. Trained in the same setting as humans, and constrained to produce efficient motor actions, this model discovers new drawing routines that transfer to test objects and resemble learned features of human sequences. These results suggest that two principles guiding motor program induction in the model - abstraction (general programs that ignore object-specific details) and compositionality (recombining previously learned programs) - are key for explaining how humans learn structured internal representations that guide flexible reasoning and learning.</summary>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-08T13:31:14Z</published>\n",
      "    <arxiv:primary_category term=\"cs.AI\"/>\n",
      "    <author>\n",
      "      <name>Lucas Y. Tian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kevin Ellis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marta Kryven</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joshua B. Tenenbaum</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03523v2</id>\n",
      "    <title>Scission: Performance-driven and Context-aware Cloud-Edge Distribution of Deep Neural Networks</title>\n",
      "    <updated>2020-12-16T19:45:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03523v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03523v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Partitioning and distributing deep neural networks (DNNs) across end-devices, edge resources and the cloud has a potential twofold advantage: preserving privacy of the input data, and reducing the ingress bandwidth demand beyond the edge. However, for a given DNN, identifying the optimal partition configuration for distributing the DNN that maximizes performance is a significant challenge. This is because the combination of potential target hardware resources that maximizes performance and the sequence of layers of the DNN that should be distributed across the target resources needs to be determined, while accounting for user-defined objectives/constraints for partitioning. This paper presents Scission, a tool for automated benchmarking of DNNs on a given set of target device, edge and cloud resources for determining optimal partitions that maximize DNN performance. The decision-making approach is context-aware by capitalizing on hardware capabilities of the target resources, their locality, the characteristics of DNN layers, and the network condition. Experimental studies are carried out on 18 DNNs. The decisions made by Scission cannot be manually made by a human given the complexity and the number of dimensions affecting the search space. The benchmarking overheads of Scission allow for responding to operational changes periodically rather than in real-time. Scission is available for public download at https://github.com/qub-blesson/Scission.</summary>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-08T13:39:57Z</published>\n",
      "    <arxiv:comment>Accepted to IEEE/ACM UCC 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.DC\"/>\n",
      "    <author>\n",
      "      <name>Luke Lockhart</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paul Harvey</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pierre Imai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peter Willis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Blesson Varghese</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03539v1</id>\n",
      "    <title>HASeparator: Hyperplane-Assisted Softmax</title>\n",
      "    <updated>2020-08-08T15:24:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03539v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03539v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Efficient feature learning with Convolutional Neural Networks (CNNs) constitutes an increasingly imperative property since several challenging tasks of computer vision tend to require cascade schemes and modalities fusion. Feature learning aims at CNN models capable of extracting embeddings, exhibiting high discrimination among the different classes, as well as intra-class compactness. In this paper, a novel approach is introduced that has separator, which focuses on an effective hyperplane-based segregation of the classes instead of the common class centers separation scheme. Accordingly, an innovatory separator, namely the Hyperplane-Assisted Softmax separator (HASeparator), is proposed that demonstrates superior discrimination capabilities, as evaluated on popular image classification benchmarks.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-08T15:24:56Z</published>\n",
      "    <arxiv:comment>Submitted to IEEE ICMLA 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Ioannis Kansizoglou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nicholas Santavas</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Loukas Bampis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Antonios Gasteratos</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03616v1</id>\n",
      "    <title>Variable frame rate-based data augmentation to handle speaking-style variability for automatic speaker verification</title>\n",
      "    <updated>2020-08-08T22:47:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03616v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03616v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The effects of speaking-style variability on automatic speaker verification were investigated using the UCLA Speaker Variability database which comprises multiple speaking styles per speaker. An x-vector/PLDA (probabilistic linear discriminant analysis) system was trained with the SRE and Switchboard databases with standard augmentation techniques and evaluated with utterances from the UCLA database. The equal error rate (EER) was low when enrollment and test utterances were of the same style (e.g., 0.98% and 0.57% for read and conversational speech, respectively), but it increased substantially when styles were mismatched between enrollment and test utterances. For instance, when enrolled with conversation utterances, the EER increased to 3.03%, 2.96% and 22.12% when tested on read, narrative, and pet-directed speech, respectively. To reduce the effect of style mismatch, we propose an entropy-based variable frame rate technique to artificially generate style-normalized representations for PLDA adaptation. The proposed system significantly improved performance. In the aforementioned conditions, the EERs improved to 2.69% (conversation -- read), 2.27% (conversation -- narrative), and 18.75% (pet-directed -- read). Overall, the proposed technique performed comparably to multi-style PLDA adaptation without the need for training data in different speaking styles per speaker.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-08T22:47:12Z</published>\n",
      "    <arxiv:comment>Accepted to Interspeech 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Amber Afshan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jinxi Guo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Soo Jin Park</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vijay Ravi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alan McCree</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abeer Alwan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03638v1</id>\n",
      "    <title>Big Networks: A Survey</title>\n",
      "    <updated>2020-08-09T03:40:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03638v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03638v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A network is a typical expressive form of representing complex systems in terms of vertices and links, in which the pattern of interactions amongst components of the network is intricate. The network can be static that does not change over time or dynamic that evolves through time. The complication of network analysis is different under the new circumstance of network size explosive increasing. In this paper, we introduce a new network science concept called big network. Big networks are generally in large-scale with a complicated and higher-order inner structure. This paper proposes a guideline framework that gives an insight into the major topics in the area of network science from the viewpoint of a big network. We first introduce the structural characteristics of big networks from three levels, which are micro-level, meso-level, and macro-level. We then discuss some state-of-the-art advanced topics of big network analysis. Big network models and related approaches, including ranking methods, partition approaches, as well as network embedding algorithms are systematically introduced. Some typical applications in big networks are then reviewed, such as community detection, link prediction, recommendation, etc. Moreover, we also pinpoint some critical open issues that need to be investigated further.</summary>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"physics.data-an\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-09T03:40:20Z</published>\n",
      "    <arxiv:comment>69 pages, 4 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.SI\"/>\n",
      "    <arxiv:journal_ref>Computer Science Review, Volume 37, August 2020, 100247</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Hayat Dino Bedru</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shuo Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xinru Xiao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Da Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liangtian Wan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>He Guo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Feng Xia</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1016/j.cosrev.2020.100247</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1016/j.cosrev.2020.100247\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.04105v1</id>\n",
      "    <title>Distributed Deep Reinforcement Learning for Functional Split Control in Energy Harvesting Virtualized Small Cells</title>\n",
      "    <updated>2020-08-07T12:27:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.04105v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.04105v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>To meet the growing quest for enhanced network capacity, mobile network operators (MNOs) are deploying dense infrastructures of small cells. This, in turn, increases the power consumption of mobile networks, thus impacting the environment. As a result, we have seen a recent trend of powering mobile networks with harvested ambient energy to achieve both environmental and cost benefits. In this paper, we consider a network of virtualized small cells (vSCs) powered by energy harvesters and equipped with rechargeable batteries, which can opportunistically offload baseband (BB) functions to a grid-connected edge server depending on their energy availability. We formulate the corresponding grid energy and traffic drop rate minimization problem, and propose a distributed deep reinforcement learning (DDRL) solution. Coordination among vSCs is enabled via the exchange of battery state information. The evaluation of the network performance in terms of grid energy consumption and traffic drop rate confirms that enabling coordination among the vSCs via knowledge exchange achieves a performance close to the optimal. Numerical results also confirm that the proposed DDRL solution provides higher network performance, better adaptation to the changing environment, and higher cost savings with respect to a tabular multi-agent reinforcement learning (MRL) solution used as a benchmark.</summary>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-07T12:27:01Z</published>\n",
      "    <arxiv:comment>Submitted to IEEE transaction on sustainable computing. arXiv admin note: text overlap with arXiv:1906.05735</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.NI\"/>\n",
      "    <author>\n",
      "      <name>Dagnachew Azene Temesgene</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marco Miozzo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Deniz Gündüz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paolo Dini</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.04175v1</id>\n",
      "    <title>EagerPy: Writing Code That Works Natively with PyTorch, TensorFlow, JAX, and NumPy</title>\n",
      "    <updated>2020-08-10T14:57:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.04175v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.04175v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>EagerPy is a Python framework that lets you write code that automatically works natively with PyTorch, TensorFlow, JAX, and NumPy. Library developers no longer need to choose between supporting just one of these frameworks or reimplementing the library for each framework and dealing with code duplication. Users of such libraries can more easily switch frameworks without being locked in by a specific 3rd party library. Beyond multi-framework support, EagerPy also brings comprehensive type annotations and consistent support for method chaining to any framework. The latest documentation is available online at https://eagerpy.jonasrauber.de and the code can be found on GitHub at https://github.com/jonasrauber/eagerpy.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-10T14:57:41Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jonas Rauber</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matthias Bethge</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wieland Brendel</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.04203v1</id>\n",
      "    <title>FireBERT: Hardening BERT-based classifiers against adversarial attack</title>\n",
      "    <updated>2020-08-10T15:43:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.04203v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.04203v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present FireBERT, a set of three proof-of-concept NLP classifiers hardened against TextFooler-style word-perturbation by producing diverse alternatives to original samples. In one approach, we co-tune BERT against the training data and synthetic adversarial samples. In a second approach, we generate the synthetic samples at evaluation time through substitution of words and perturbation of embedding vectors. The diversified evaluation results are then combined by voting. A third approach replaces evaluation-time word substitution with perturbation of embedding vectors. We evaluate FireBERT for MNLI and IMDB Movie Review datasets, in the original and on adversarial examples generated by TextFooler. We also test whether TextFooler is less successful in creating new adversarial samples when manipulating FireBERT, compared to working on unhardened classifiers. We show that it is possible to improve the accuracy of BERT-based models in the face of adversarial attacks without significantly reducing the accuracy for regular benchmark samples. We present co-tuning with a synthetic data generator as a highly effective method to protect against 95% of pre-manufactured adversarial samples while maintaining 98% of original benchmark performance. We also demonstrate evaluation-time perturbation as a promising direction for further research, restoring accuracy up to 75% of benchmark performance for pre-made adversarials, and up to 65% (from a baseline of 75% orig. / 12% attack) under active attack by TextFooler.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-10T15:43:28Z</published>\n",
      "    <arxiv:comment>8 pages, 10 figures, code available at: https://github.com/FireBERT-author/FireBERT</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Gunnar Mein</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kevin Hartman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrew Morris</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.04210v3</id>\n",
      "    <title>From the logistic-sigmoid to nlogistic-sigmoid: modelling the COVID-19 pandemic growth</title>\n",
      "    <updated>2020-12-09T02:25:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.04210v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.04210v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Real-world growth processes, such as epidemic growth, are inherently noisy, uncertain and often involve multiple growth phases. The logistic-sigmoid function has been suggested and applied in the domain of modelling such growth processes. However, existing definitions are limiting, as they do not consider growth as restricted in two-dimension. Additionally, as the number of growth phases increase, the modelling and estimation of logistic parameters becomes more cumbersome, requiring more complex tools and analysis. To remedy this, we introduce the nlogistic-sigmoid function as a compact, unified modern definition of logistic growth for modelling such real-world growth phenomena. Also, we introduce two characteristic metrics of the logistic-sigmoid curve that can give more robust projections on the state of the growth process in each dimension. Specifically, we apply this function to modelling the daily World Health Organization published COVID-19 time-series data of infection and death cases of the world and countries of the world to date. Our results demonstrate statistically significant goodness of fit greater than or equal to 99% for affected countries of the world exhibiting patterns of either single or multiple stages of the ongoing COVID-19 outbreak, such as the USA. Consequently, this modern logistic definition and its metrics, as a machine learning tool, can help to provide clearer and more robust monitoring and quantification of the ongoing pandemic growth process.</summary>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-06T15:33:48Z</published>\n",
      "    <arxiv:comment>applied to a real-world phenomenon. 11 pages, 13 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.NE\"/>\n",
      "    <author>\n",
      "      <name>Oluwasegun A. Somefun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kayode Akingbade</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Folasade Dahunsi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03677v1</id>\n",
      "    <title>Enhancing Robustness Against Adversarial Examples in Network Intrusion Detection Systems</title>\n",
      "    <updated>2020-08-09T07:04:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03677v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03677v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The increase of cyber attacks in both the numbers and varieties in recent years demands to build a more sophisticated network intrusion detection system (NIDS). These NIDS perform better when they can monitor all the traffic traversing through the network like when being deployed on a Software-Defined Network (SDN). Because of the inability to detect zero-day attacks, signature-based NIDS which were traditionally used for detecting malicious traffic are beginning to get replaced by anomaly-based NIDS built on neural networks. However, recently it has been shown that such NIDS have their own drawback namely being vulnerable to the adversarial example attack. Moreover, they were mostly evaluated on the old datasets which don't represent the variety of attacks network systems might face these days. In this paper, we present Reconstruction from Partial Observation (RePO) as a new mechanism to build an NIDS with the help of denoising autoencoders capable of detecting different types of network attacks in a low false alert setting with an enhanced robustness against adversarial example attack. Our evaluation conducted on a dataset with a variety of network attacks shows denoising autoencoders can improve detection of malicious traffic by up to 29% in a normal setting and by up to 45% in an adversarial setting compared to other recently proposed anomaly detectors.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-09T07:04:06Z</published>\n",
      "    <arxiv:comment>Submitted to 6th IEEE Conference on Network Functions Virtualization and Software Defined Networks (IEEE NFV-SDN 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Mohammad J. Hashemi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eric Keller</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03703v1</id>\n",
      "    <title>What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation</title>\n",
      "    <updated>2020-08-09T10:12:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03703v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03703v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep learning algorithms are well-known to have a propensity for fitting the training data very well and often fit even outliers and mislabeled data points. Such fitting requires memorization of training data labels, a phenomenon that has attracted significant research interest but has not been given a compelling explanation so far. A recent work of Feldman (2019) proposes a theoretical explanation for this phenomenon based on a combination of two insights. First, natural image and data distributions are (informally) known to be long-tailed, that is have a significant fraction of rare and atypical examples. Second, in a simple theoretical model such memorization is necessary for achieving close-to-optimal generalization error when the data distribution is long-tailed. However, no direct empirical evidence for this explanation or even an approach for obtaining such evidence were given.\n",
      "  In this work we design experiments to test the key ideas in this theory. The experiments require estimation of the influence of each training example on the accuracy at each test example as well as memorization values of training examples. Estimating these quantities directly is computationally prohibitive but we show that closely-related subsampled influence and memorization values can be estimated much more efficiently. Our experiments demonstrate the significant benefits of memorization for generalization on several standard benchmarks. They also provide quantitative and visually compelling evidence for the theory put forth in (Feldman, 2019).</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-09T10:12:28Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Vitaly Feldman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chiyuan Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03707v1</id>\n",
      "    <title>Risk-Sensitive Markov Decision Processes with Combined Metrics of Mean and Variance</title>\n",
      "    <updated>2020-08-09T10:35:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03707v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03707v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper investigates the optimization problem of an infinite stage discrete time Markov decision process (MDP) with a long-run average metric considering both mean and variance of rewards together. Such performance metric is important since the mean indicates average returns and the variance indicates risk or fairness. However, the variance metric couples the rewards at all stages, the traditional dynamic programming is inapplicable as the principle of time consistency fails. We study this problem from a new perspective called the sensitivity-based optimization theory. A performance difference formula is derived and it can quantify the difference of the mean-variance combined metrics of MDPs under any two different policies. The difference formula can be utilized to generate new policies with strictly improved mean-variance performance. A necessary condition of the optimal policy and the optimality of deterministic policies are derived. We further develop an iterative algorithm with a form of policy iteration, which is proved to converge to local optima both in the mixed and randomized policy space. Specially, when the mean reward is constant in policies, the algorithm is guaranteed to converge to the global optimum. Finally, we apply our approach to study the fluctuation reduction of wind power in an energy storage system, which demonstrates the potential applicability of our optimization method.</summary>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-09T10:35:35Z</published>\n",
      "    <arxiv:comment>43 pages, 7 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"math.OC\"/>\n",
      "    <author>\n",
      "      <name>Li Xia</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03787v1</id>\n",
      "    <title>Neural Manipulation Planning on Constraint Manifolds</title>\n",
      "    <updated>2020-08-09T18:58:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03787v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03787v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The presence of task constraints imposes a significant challenge to motion planning. Despite all recent advancements, existing algorithms are still computationally expensive for most planning problems. In this paper, we present Constrained Motion Planning Networks (CoMPNet), the first neural planner for multimodal kinematic constraints. Our approach comprises the following components: i) constraint and environment perception encoders; ii) neural robot configuration generator that outputs configurations on/near the constraint manifold(s), and iii) a bidirectional planning algorithm that takes the generated configurations to create a feasible robot motion trajectory. We show that CoMPNet solves practical motion planning tasks involving both unconstrained and constrained problems. Furthermore, it generalizes to new unseen locations of the objects, i.e., not seen during training, in the given environments with high success rates. When compared to the state-of-the-art constrained motion planning algorithms, CoMPNet outperforms by order of magnitude improvement in computational speed with a significantly lower variance.</summary>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-09T18:58:10Z</published>\n",
      "    <arxiv:comment>This is the preprint version of the paper published at IEEE Robotics and Automation Letters 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.RO\"/>\n",
      "    <arxiv:journal_ref>in IEEE Robotics and Automation Letters, vol. 5, no. 4, pp. 6089-6096, Oct. 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Ahmed H. Qureshi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiangeng Dong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Austin Choe</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michael C. Yip</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/LRA.2020.3010220</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/LRA.2020.3010220\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03825v1</id>\n",
      "    <title>Comparative Analysis of the Hidden Markov Model and LSTM: A Simulative Approach</title>\n",
      "    <updated>2020-08-09T22:13:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03825v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03825v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Time series and sequential data have gained significant attention recently since many real-world processes in various domains such as finance, education, biology, and engineering can be modeled as time series. Although many algorithms and methods such as the Kalman filter, hidden Markov model, and long short term memory (LSTM) are proposed to make inferences and predictions for the data, their usage significantly depends on the application, type of the problem, available data, and sufficient accuracy or loss. In this paper, we compare the supervised and unsupervised hidden Markov model to LSTM in terms of the amount of data needed for training, complexity, and forecasting accuracy. Moreover, we propose various techniques to discretize the observations and convert the problem to a discrete hidden Markov model under stationary and non-stationary situations. Our results indicate that even an unsupervised hidden Markov model can outperform LSTM when a massive amount of labeled data is not available. Furthermore, we show that the hidden Markov model can still be an effective method to process the sequence data even when the first-order Markov assumption is not satisfied.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-09T22:13:10Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Manie Tadayon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Greg Pottie</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.03843v1</id>\n",
      "    <title>Question Identification in Arabic Language Using Emotional Based Features</title>\n",
      "    <updated>2020-08-10T00:18:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.03843v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.03843v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>With the growth of content on social media networks, enterprises and services providers have become interested in identifying the questions of their customers. Tracking these questions become very challenging with the growth of text that grows directly proportional to the increase of Arabic users thus making it very difficult to be tracked manually. By automatic identifying the questions seeking answers on the social media networks and defining their category, we can automatically answer them by finding an existing answer or even routing them to those responsible for answering those questions in the customer service. This will result in saving the time and the effort and enhancing the customer feedback and improving the business. In this paper, we have implemented a binary classifier to classify Arabic text to either question seeking answer or not. We have added emotional based features to the state of the art features. Experimental evaluation has done and showed that these emotional features have improved the accuracy of the classifier.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-10T00:18:32Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Ahmed Ramzy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ahmed Elazab</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.04527v1</id>\n",
      "    <title>Neural PLDA Modeling for End-to-End Speaker Verification</title>\n",
      "    <updated>2020-08-11T05:54:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.04527v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.04527v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>While deep learning models have made significant advances in supervised classification problems, the application of these models for out-of-set verification tasks like speaker recognition has been limited to deriving feature embeddings. The state-of-the-art x-vector PLDA based speaker verification systems use a generative model based on probabilistic linear discriminant analysis (PLDA) for computing the verification score. Recently, we had proposed a neural network approach for backend modeling in speaker verification called the neural PLDA (NPLDA) where the likelihood ratio score of the generative PLDA model is posed as a discriminative similarity function and the learnable parameters of the score function are optimized using a verification cost. In this paper, we extend this work to achieve joint optimization of the embedding neural network (x-vector network) with the NPLDA network in an end-to-end (E2E) fashion. This proposed end-to-end model is optimized directly from the acoustic features with a verification cost function and during testing, the model directly outputs the likelihood ratio score. With various experiments using the NIST speaker recognition evaluation (SRE) 2018 and 2019 datasets, we show that the proposed E2E model improves significantly over the x-vector PLDA baseline speaker verification system.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-11T05:54:54Z</published>\n",
      "    <arxiv:comment>Accepted in Interspeech 2020. GitHub Implementation Repos: https://github.com/iiscleap/E2E-NPLDA and https://github.com/iiscleap/NeuralPlda</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Shreyas Ramoji</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Prashant Krishnan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sriram Ganapathy</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.04612v1</id>\n",
      "    <title>Holdout SGD: Byzantine Tolerant Federated Learning</title>\n",
      "    <updated>2020-08-11T10:16:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.04612v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.04612v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This work presents a new distributed Byzantine tolerant federated learning algorithm, HoldOut SGD, for Stochastic Gradient Descent (SGD) optimization. HoldOut SGD uses the well known machine learning technique of holdout estimation, in a distributed fashion, in order to select parameter updates that are likely to lead to models with low loss values. This makes it more effective at discarding Byzantine workers inputs than existing methods that eliminate outliers in the parameter-space of the learned model. HoldOut SGD first randomly selects a set of workers that use their private data in order to propose gradient updates. Next, a voting committee of workers is randomly selected, and each voter uses its private data as holdout data, in order to select the best proposals via a voting scheme. We propose two possible mechanisms for the coordination of workers in the distributed computation of HoldOut SGD. The first uses a truthful central server and corresponds to the typical setting of current federated learning. The second is fully distributed and requires no central server, paving the way to fully decentralized federated learning. The fully distributed version implements HoldOut SGD via ideas from the blockchain domain, and specifically the Algorand committee selection and consensus processes. We provide formal guarantees for the HoldOut SGD process in terms of its convergence to the optimal model, and its level of resilience to the fraction of Byzantine workers. Empirical evaluation shows that HoldOut SGD is Byzantine-resilient and efficiently converges to an effectual model for deep-learning tasks, as long as the total number of participating workers is large and the fraction of Byzantine workers is less than half (&lt;1/3 for the fully distributed variant).</summary>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-11T10:16:37Z</published>\n",
      "    <arxiv:comment>12 pages, 2 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.DC\"/>\n",
      "    <author>\n",
      "      <name>Shahar Azulay</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lior Raz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amir Globerson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tomer Koren</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yehuda Afek</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.04893v5</id>\n",
      "    <title>Channel Leakage, Information-Theoretic Limitations of Obfuscation, and Optimal Privacy Mask Design for Streaming Data</title>\n",
      "    <updated>2020-09-29T21:15:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.04893v5\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.04893v5\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we first introduce the notion of channel leakage as the minimum mutual information between the channel input and channel output. As its name indicates, channel leakage quantifies the minimum information leakage to the malicious receiver. In a broad sense, it can be viewed as a dual concept of channel capacity, which characterizes the maximum information transmission to the targeted receiver. We obtain explicit formulas of channel leakage for the white Gaussian case, the colored Gaussian case, and the fading case. We then utilize this notion to investigate the fundamental limitations of obfuscation in terms of privacy-distortion tradeoffs (as well as privacy-power tradeoffs) for streaming data; particularly, we derive analytical tradeoff equations for the stationary case, the non-stationary case, and the finite-time case. Our results also indicate explicitly how to design the privacy masks in an optimal way.</summary>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-11T17:55:47Z</published>\n",
      "    <arxiv:comment>The title was changed from \"Channel Leakage and Information Theoretic Privacy-Distortion Tradeoffs for Streaming Data\" to the current one on 29th September 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IT\"/>\n",
      "    <author>\n",
      "      <name>Song Fang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Quanyan Zhu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.07981v1</id>\n",
      "    <title>Differential Replication in Machine Learning</title>\n",
      "    <updated>2020-07-15T20:26:49Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.07981v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.07981v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>When deployed in the wild, machine learning models are usually confronted with data and requirements that constantly vary, either because of changes in the generating distribution or because external constraints change the environment where the model operates. To survive in such an ecosystem, machine learning models need to adapt to new conditions by evolving over time. The idea of model adaptability has been studied from different perspectives. In this paper, we propose a solution based on reusing the knowledge acquired by the already deployed machine learning models and leveraging it to train future generations. This is the idea behind differential replication of machine learning models.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-15T20:26:49Z</published>\n",
      "    <arxiv:comment>8 pages, 1 Figure, 34 References</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Irene Unceta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jordi Nin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Oriol Pujol</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.07985v1</id>\n",
      "    <title>Faster Uncertainty Quantification for Inverse Problems with Conditional Normalizing Flows</title>\n",
      "    <updated>2020-07-15T20:36:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.07985v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.07985v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In inverse problems, we often have access to data consisting of paired samples $(x,y)\\sim p_{X,Y}(x,y)$ where $y$ are partial observations of a physical system, and $x$ represents the unknowns of the problem. Under these circumstances, we can employ supervised training to learn a solution $x$ and its uncertainty from the observations $y$. We refer to this problem as the \"supervised\" case. However, the data $y\\sim p_{Y}(y)$ collected at one point could be distributed differently than observations $y'\\sim p_{Y}'(y')$, relevant for a current set of problems. In the context of Bayesian inference, we propose a two-step scheme, which makes use of normalizing flows and joint data to train a conditional generator $q_θ(x|y)$ to approximate the target posterior density $p_{X|Y}(x|y)$. Additionally, this preliminary phase provides a density function $q_θ(x|y)$, which can be recast as a prior for the \"unsupervised\" problem, e.g.~when only the observations $y'\\sim p_{Y}'(y')$, a likelihood model $y'|x$, and a prior on $x'$ are known. We then train another invertible generator with output density $q'_φ(x|y')$ specifically for $y'$, allowing us to sample from the posterior $p_{X|Y}'(x|y')$. We present some synthetic results that demonstrate considerable training speedup when reusing the pretrained network $q_θ(x|y')$ as a warm start or preconditioning for approximating $p_{X|Y}'(x|y')$, instead of learning from scratch. This training modality can be interpreted as an instance of transfer learning. This result is particularly relevant for large-scale inverse problems that employ expensive numerical simulations.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"physics.geo-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-15T20:36:30Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Ali Siahkoohi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gabrio Rizzuti</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Philipp A. Witte</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Felix J. Herrmann</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.07986v1</id>\n",
      "    <title>Boosting Weakly Supervised Object Detection with Progressive Knowledge Transfer</title>\n",
      "    <updated>2020-07-15T20:38:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.07986v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.07986v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we propose an effective knowledge transfer framework to boost the weakly supervised object detection accuracy with the help of an external fully-annotated source dataset, whose categories may not overlap with the target domain. This setting is of great practical value due to the existence of many off-the-shelf detection datasets. To more effectively utilize the source dataset, we propose to iteratively transfer the knowledge from the source domain by a one-class universal detector and learn the target-domain detector. The box-level pseudo ground truths mined by the target-domain detector in each iteration effectively improve the one-class universal detector. Therefore, the knowledge in the source dataset is more thoroughly exploited and leveraged. Extensive experiments are conducted with Pascal VOC 2007 as the target weakly-annotated dataset and COCO/ImageNet as the source fully-annotated dataset. With the proposed solution, we achieved an mAP of $59.7\\%$ detection performance on the VOC test set and an mAP of $60.2\\%$ after retraining a fully supervised Faster RCNN with the mined pseudo ground truths. This is significantly better than any previously known results in related literature and sets a new state-of-the-art of weakly supervised object detection under the knowledge transfer setting. Code: \\url{https://github.com/mikuhatsune/wsod_transfer}.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-15T20:38:25Z</published>\n",
      "    <arxiv:comment>ECCV 2020. Code: https://github.com/mikuhatsune/wsod_transfer</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Yuanyi Zhong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jianfeng Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jian Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lei Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.08001v1</id>\n",
      "    <title>Computation Offloading in Beyond 5G Networks: A Distributed Learning Framework and Applications</title>\n",
      "    <updated>2020-07-15T21:25:15Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.08001v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.08001v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Facing the trend of merging wireless communications and multi-access edge computing (MEC), this article studies computation offloading in the beyond fifth-generation networks. To address the technical challenges originating from the uncertainties and the sharing of limited resource in an MEC system, we formulate the computation offloading problem as a multi-agent Markov decision process, for which a distributed learning framework is proposed. We present a case study on resource orchestration in computation offloading to showcase the potentials of an online distributed reinforcement learning algorithm developed under the proposed framework. Experimental results demonstrate that our learning algorithm outperforms the benchmark resource orchestration algorithms. Furthermore, we outline the research directions worth in-depth investigation to minimize the time cost, which is one of the main practical issues that prevent the implementation of the proposed distributed learning framework.</summary>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-15T21:25:15Z</published>\n",
      "    <arxiv:primary_category term=\"cs.DC\"/>\n",
      "    <author>\n",
      "      <name>Xianfu Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Celimuge Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhi Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ning Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yusheng Ji</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.10865v1</id>\n",
      "    <title>Automated and Sound Synthesis of Lyapunov Functions with SMT Solvers</title>\n",
      "    <updated>2020-07-21T14:45:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.10865v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.10865v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper we employ SMT solvers to soundly synthesise Lyapunov functions that assert the stability of a given dynamical model. The search for a Lyapunov function is framed as the satisfiability of a second-order logical formula, asking whether there exists a function satisfying a desired specification (stability) for all possible initial conditions of the model. We synthesise Lyapunov functions for linear, non-linear (polynomial), and for parametric models. For non-linear models, the algorithm also determines a region of validity for the Lyapunov function. We exploit an inductive framework to synthesise Lyapunov functions, starting from parametric templates. The inductive framework comprises two elements: a learner proposes a Lyapunov function, and a verifier checks its validity - its lack is expressed via a counterexample (a point over the state space), for further use by the learner. Whilst the verifier uses the SMT solver Z3, thus ensuring the overall soundness of the procedure, we examine two alternatives for the learner: a numerical approach based on the optimisation tool Gurobi, and a sound approach based again on Z3. The overall technique is evaluated over a broad set of benchmarks, which shows that this methodology not only scales to 10-dimensional models within reasonable computational time, but also offers a novel soundness proof for the generated Lyapunov functions and their domains of validity.</summary>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-21T14:45:23Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SY\"/>\n",
      "    <author>\n",
      "      <name>Daniele Ahmed</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrea Peruffo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alessandro Abate</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1007/978-3-030-45190-5_6</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1007/978-3-030-45190-5_6\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.10877v1</id>\n",
      "    <title>problemConquero at SemEval-2020 Task 12: Transformer and Soft label-based approaches</title>\n",
      "    <updated>2020-07-21T15:06:58Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.10877v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.10877v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we present various systems submitted by our team problemConquero for SemEval-2020 Shared Task 12 Multilingual Offensive Language Identification in Social Media. We participated in all the three sub-tasks of OffensEval-2020, and our final submissions during the evaluation phase included transformer-based approaches and a soft label-based approach. BERT based fine-tuned models were submitted for each language of sub-task A (offensive tweet identification). RoBERTa based fine-tuned model for sub-task B (automatic categorization of offense types) was submitted. We submitted two models for sub-task C (offense target identification), one using soft labels and the other using BERT based fine-tuned model. Our ranks for sub-task A were Greek-19 out of 37, Turkish-22 out of 46, Danish-26 out of 39, Arabic-39 out of 53, and English-20 out of 85. We achieved a rank of 28 out of 43 for sub-task B. Our best rank for sub-task C was 20 out of 39 using BERT based fine-tuned model.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-21T15:06:58Z</published>\n",
      "    <arxiv:comment>10 pages,2 figures,8 tables, Accepted at Proceedings of the 14th International Workshop on Semantic Evaluation (SemEval-2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Karishma Laud</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jagriti Singh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Randeep Kumar Sahu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ashutosh Modi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.10908v1</id>\n",
      "    <title>Robust Hierarchical Graph Classification with Subgraph Attention</title>\n",
      "    <updated>2020-07-19T10:03:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.10908v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.10908v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Graph neural networks get significant attention for graph representation and classification in machine learning community. Attention mechanism applied on the neighborhood of a node improves the performance of graph neural networks. Typically, it helps to identify a neighbor node which plays more important role to determine the label of the node under consideration. But in real world scenarios, a particular subset of nodes together, but not the individual pairs in the subset, may be important to determine the label of the graph. To address this problem, we introduce the concept of subgraph attention for graphs. On the other hand, hierarchical graph pooling has been shown to be promising in recent literature. But due to noisy hierarchical structure of real world graphs, not all the hierarchies of a graph play equal role for graph classification. Towards this end, we propose a graph classification algorithm called SubGattPool which jointly learns the subgraph attention and employs two different types of hierarchical attention mechanisms to find the important nodes in a hierarchy and the importance of individual hierarchies in a graph. Experimental evaluation with different types of graph classification algorithms shows that SubGattPool is able to improve the state-of-the-art or remains competitive on multiple publicly available graph classification datasets. We conduct further experiments on both synthetic and real world graph datasets to justify the usefulness of different components of SubGattPool and to show its consistent performance on other downstream tasks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-19T10:03:06Z</published>\n",
      "    <arxiv:comment>A shorter version of this work was accepted and presented at ICML 2020 (non-archival) Workshop on Graph Representation Learning and Beyond (GRL+) with the title \"Hierarchically Attentive Graph Pooling with Subgraph Attention\"</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Sambaran Bandyopadhyay</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Manasvi Aggarwal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>M. Narasimha Murty</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.08657v1</id>\n",
      "    <title>Training with reduced precision of a support vector machine model for text classification</title>\n",
      "    <updated>2020-07-17T11:59:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.08657v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.08657v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper presents the impact of using quantization on the efficiency of multi-class text classification in the training process of a support vector machine (SVM). This work is focused on comparing the efficiency of SVM model trained using reduced precision with its original form. The main advantage of using quantization is decrease in computation time and in memory footprint on the dedicated hardware platform which supports low precision computation like GPU (16-bit) or FPGA (any bit-width). The paper presents the impact of a precision reduction of the SVM training process on text classification accuracy. The implementation of the CPU was performed using the OpenMP library. Additionally, the results of the implementation of the GPU using double, single and half precision are presented.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-17T11:59:30Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Dominik Żurek</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marcin Pietroń</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.08690v1</id>\n",
      "    <title>Transfer Deep Reinforcement Learning-enabled Energy Management Strategy for Hybrid Tracked Vehicle</title>\n",
      "    <updated>2020-07-16T23:39:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.08690v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.08690v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper proposes an adaptive energy management strategy for hybrid electric vehicles by combining deep reinforcement learning (DRL) and transfer learning (TL). This work aims to address the defect of DRL in tedious training time. First, an optimization control modeling of a hybrid tracked vehicle is built, wherein the elaborate powertrain components are introduced. Then, a bi-level control framework is constructed to derive the energy management strategies (EMSs). The upper-level is applying the particular deep deterministic policy gradient (DDPG) algorithms for EMS training at different speed intervals. The lower-level is employing the TL method to transform the pre-trained neural networks for a novel driving cycle. Finally, a series of experiments are executed to prove the effectiveness of the presented control framework. The optimality and adaptability of the formulated EMS are illuminated. The founded DRL and TL-enabled control policy is capable of enhancing energy efficiency and improving system performance.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-16T23:39:34Z</published>\n",
      "    <arxiv:comment>11 pages, 11 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Xiaowei Guo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Teng Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bangbei Tang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaolin Tang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jinwei Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wenhao Tan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shufeng Jin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.08749v3</id>\n",
      "    <title>Towards an Automated SOAP Note: Classifying Utterances from Medical Conversations</title>\n",
      "    <updated>2020-07-27T15:35:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.08749v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.08749v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Summaries generated from medical conversations can improve recall and understanding of care plans for patients and reduce documentation burden for doctors. Recent advancements in automatic speech recognition (ASR) and natural language understanding (NLU) offer potential solutions to generate these summaries automatically, but rigorous quantitative baselines for benchmarking research in this domain are lacking. In this paper, we bridge this gap for two tasks: classifying utterances from medical conversations according to (i) the SOAP section and (ii) the speaker role. Both are fundamental building blocks along the path towards an end-to-end, automated SOAP note for medical conversations. We provide details on a dataset that contains human and ASR transcriptions of medical conversations and corresponding machine learning optimized SOAP notes. We then present a systematic analysis in which we adapt an existing deep learning architecture to the two aforementioned tasks. The results suggest that modelling context in a hierarchical manner, which captures both word and utterance level context, yields substantial improvements on both classification tasks. Additionally, we develop and analyze a modular method for adapting our model to ASR output.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-17T04:19:30Z</published>\n",
      "    <arxiv:comment>21 pages,1 figure</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Benjamin Schloss</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sandeep Konam</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.09989v1</id>\n",
      "    <title>Bayesian optimization for automatic design of face stimuli</title>\n",
      "    <updated>2020-07-20T10:27:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.09989v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.09989v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Investigating the cognitive and neural mechanisms involved with face processing is a fundamental task in modern neuroscience and psychology. To date, the majority of such studies have focused on the use of pre-selected stimuli. The absence of personalized stimuli presents a serious limitation as it fails to account for how each individual face processing system is tuned to cultural embeddings or how it is disrupted in disease. In this work, we propose a novel framework which combines generative adversarial networks (GANs) with Bayesian optimization to identify individual response patterns to many different faces. Formally, we employ Bayesian optimization to efficiently search the latent space of state-of-the-art GAN models, with the aim to automatically generate novel faces, to maximize an individual subject's response. We present results from a web-based proof-of-principle study, where participants rated images of themselves generated via performing Bayesian optimization over the latent space of a GAN. We show how the algorithm can efficiently locate an individual's optimal face while mapping out their response across different semantic transformations of a face; inter-individual analyses suggest how the approach can provide rich information about individual differences in face processing.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-20T10:27:18Z</published>\n",
      "    <arxiv:comment>Accepted at ICML2020 workshop track</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Pedro F. da Costa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Romy Lorenz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ricardo Pio Monti</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Emily Jones</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Robert Leech</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.10022v1</id>\n",
      "    <title>Learning Sparse Filters in Deep Convolutional Neural Networks with a l1/l2 Pseudo-Norm</title>\n",
      "    <updated>2020-07-20T11:56:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.10022v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.10022v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>While deep neural networks (DNNs) have proven to be efficient for numerous tasks, they come at a high memory and computation cost, thus making them impractical on resource-limited devices. However, these networks are known to contain a large number of parameters. Recent research has shown that their structure can be more compact without compromising their performance. In this paper, we present a sparsity-inducing regularization term based on the ratio l1/l2 pseudo-norm defined on the filter coefficients. By defining this pseudo-norm appropriately for the different filter kernels, and removing irrelevant filters, the number of kernels in each layer can be drastically reduced leading to very compact Deep Convolutional Neural Networks (DCNN) structures. Unlike numerous existing methods, our approach does not require an iterative retraining process and, using this regularization term, directly produces a sparse model during the training process. Furthermore, our approach is also much easier and simpler to implement than existing methods. Experimental results on MNIST and CIFAR-10 show that our approach significantly reduces the number of filters of classical models such as LeNet and VGG while reaching the same or even better accuracy than the baseline models. Moreover, the trade-off between the sparsity and the accuracy is compared to other loss regularization terms based on the l1 or l2 norm as well as the SSL, NISP and GAL methods and shows that our approach is outperforming them.</summary>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-20T11:56:12Z</published>\n",
      "    <arxiv:comment>8 pages, 7 figures, under review for ICPR 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.NE\"/>\n",
      "    <author>\n",
      "      <name>Anthony Berthelier</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yongzhe Yan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Thierry Chateau</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christophe Blanc</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stefan Duffner</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christophe Garcia</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.10055v2</id>\n",
      "    <title>Morphological Skip-Gram: Using morphological knowledge to improve word representation</title>\n",
      "    <updated>2020-07-21T09:01:52Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.10055v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.10055v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Natural language processing models have attracted much interest in the deep learning community. This branch of study is composed of some applications such as machine translation, sentiment analysis, named entity recognition, question and answer, and others. Word embeddings are continuous word representations, they are an essential module for those applications and are generally used as input word representation to the deep learning models. Word2Vec and GloVe are two popular methods to learn word embeddings. They achieve good word representations, however, they learn representations with limited information because they ignore the morphological information of the words and consider only one representation vector for each word. This approach implies that Word2Vec and GloVe are unaware of the word inner structure. To mitigate this problem, the FastText model represents each word as a bag of characters n-grams. Hence, each n-gram has a continuous vector representation, and the final word representation is the sum of its characters n-grams vectors. Nevertheless, the use of all n-grams character of a word is a poor approach since some n-grams have no semantic relation with their words and increase the amount of potentially useless information. This approach also increases the training phase time. In this work, we propose a new method for training word embeddings, and its goal is to replace the FastText bag of character n-grams for a bag of word morphemes through the morphological analysis of the word. Thus, words with similar context and morphemes are represented by vectors close to each other. To evaluate our new approach, we performed intrinsic evaluations considering 15 different tasks, and the results show a competitive performance compared to FastText.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-20T12:47:36Z</published>\n",
      "    <arxiv:comment>11 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Flávio Santos</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hendrik Macedo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Thiago Bispo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cleber Zanchettin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.09536v1</id>\n",
      "    <title>Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding</title>\n",
      "    <updated>2020-07-18T23:30:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.09536v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.09536v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-18T23:30:47Z</published>\n",
      "    <arxiv:comment>KDD 2020 Research Track. (Code: https://github.com/yumeng5/JoSH)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Yu Meng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yunyi Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiaxin Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chao Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiawei Han</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3394486.3403242</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3394486.3403242\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.10261v1</id>\n",
      "    <title>Few-shot link prediction via graph neural networks for Covid-19 drug-repurposing</title>\n",
      "    <updated>2020-07-20T16:48:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.10261v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.10261v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Predicting interactions among heterogenous graph structured data has numerous applications such as knowledge graph completion, recommendation systems and drug discovery. Often times, the links to be predicted belong to rare types such as the case in repurposing drugs for novel diseases. This motivates the task of few-shot link prediction. Typically, GCNs are ill-equipped in learning such rare link types since the relation embedding is not learned in an inductive fashion. This paper proposes an inductive RGCN for learning informative relation embeddings even in the few-shot learning regime. The proposed inductive model significantly outperforms the RGCN and state-of-the-art KGE models in few-shot learning tasks. Furthermore, we apply our method on the drug-repurposing knowledge graph (DRKG) for discovering drugs for Covid-19. We pose the drug discovery task as link prediction and learn embeddings for the biological entities that partake in the DRKG. Our initial results corroborate that several drugs used in clinical trials were identified as possible drug candidates. The method in this paper are implemented using the efficient deep graph learning (DGL)</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-20T16:48:51Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Vassilis N. Ioannidis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Da Zheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>George Karypis</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.10307v2</id>\n",
      "    <title>Optimal $\\ell_1$ Column Subset Selection and a Fast PTAS for Low Rank Approximation</title>\n",
      "    <updated>2020-11-16T07:22:43Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.10307v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.10307v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study the problem of entrywise $\\ell_1$ low rank approximation. We give the first polynomial time column subset selection-based $\\ell_1$ low rank approximation algorithm sampling $\\tilde{O}(k)$ columns and achieving an $\\tilde{O}(k^{1/2})$-approximation for any $k$, improving upon the previous best $\\tilde{O}(k)$-approximation and matching a prior lower bound for column subset selection-based $\\ell_1$-low rank approximation which holds for any $\\text{poly}(k)$ number of columns. We extend our results to obtain tight upper and lower bounds for column subset selection-based $\\ell_p$ low rank approximation for any $1 &lt; p &lt; 2$, closing a long line of work on this problem.\n",
      "  We next give a $(1 + \\varepsilon)$-approximation algorithm for entrywise $\\ell_p$ low rank approximation, for $1 \\leq p &lt; 2$, that is not a column subset selection algorithm. First, we obtain an algorithm which, given a matrix $A \\in \\mathbb{R}^{n \\times d}$, returns a rank-$k$ matrix $\\hat{A}$ in $2^{\\text{poly}(k/\\varepsilon)} + \\text{poly}(nd)$ running time such that: $$\\|A - \\hat{A}\\|_p \\leq (1 + \\varepsilon) \\cdot OPT + \\frac{\\varepsilon}{\\text{poly}(k)}\\|A\\|_p$$ where $OPT = \\min_{A_k \\text{ rank }k} \\|A - A_k\\|_p$. Using this algorithm, in the same running time we give an algorithm which obtains error at most $(1 + \\varepsilon) \\cdot OPT$ and outputs a matrix of rank at most $3k$ -- these algorithms significantly improve upon all previous $(1 + \\varepsilon)$- and $O(1)$-approximation algorithms for the $\\ell_p$ low rank approximation problem, which required at least $n^{\\text{poly}(k/\\varepsilon)}$ or $n^{\\text{poly}(k)}$ running time, and either required strong bit complexity assumptions (our algorithms do not) or had bicriteria rank $3k$. Finally, we show hardness results which nearly match our $2^{\\text{poly}(k)} + \\text{poly}(nd)$ running time and the above additive error guarantee.</summary>\n",
      "    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-20T17:50:30Z</published>\n",
      "    <arxiv:comment>To appear in SODA 2021. Changes: (1) Fixed errors in hardness proof for constrained $\\ell_1$ low rank approximation. (2) Simplified analysis of column subset selection algorithm. (3) Improved runtime of $\\text{poly}(k)$-approximation algorithm with output rank $k$ from $2^{O(k\\log k)} + \\text{poly}(nd)$ to $\\text{poly}(nd)$. Results are unchanged aside from (3)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.DS\"/>\n",
      "    <author>\n",
      "      <name>Arvind V. Mahankali</name>\n",
      "      <arxiv:affiliation>Carnegie Mellon University</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David P. Woodruff</name>\n",
      "      <arxiv:affiliation>Carnegie Mellon University</arxiv:affiliation>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.09679v1</id>\n",
      "    <title>One-Shot Learning for Language Modelling</title>\n",
      "    <updated>2020-07-19T14:33:03Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.09679v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.09679v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Humans can infer a great deal about the meaning of a word, using the syntax and semantics of surrounding words even if it is their first time reading or hearing it. We can also generalise the learned concept of the word to new tasks. Despite great progress in achieving human-level performance in certain tasks (Silver et al., 2016), learning from one or few examples remains a key challenge in machine learning, and has not thoroughly been explored in Natural Language Processing (NLP).\n",
      "  In this work we tackle the problem of oneshot learning for an NLP task by employing ideas from recent developments in machine learning: embeddings, attention mechanisms (softmax) and similarity measures (cosine, Euclidean, Poincare, and Minkowski). We adapt the framework suggested in matching networks (Vinyals et al., 2016), and explore the effectiveness of the aforementioned methods in one, two and three-shot learning problems on the task of predicting missing word explored in (Vinyals et al., 2016) by using the WikiText-2 dataset. Our work contributes in two ways: Our first contribution is that we explore the effectiveness of different distance metrics on k-shot learning, and show that there is no single best distance metric for k-shot learning, which challenges common belief. We found that the performance of a distance metric depends on the number of shots used during training. The second contribution of our work is that we establish a benchmark for one, two, and three-shot learning on a language task with a publicly available dataset that can be used to benchmark against in future research.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-19T14:33:03Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Talip Ucar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Adrian Gonzalez-Martin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matthew Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Adrian Daniel Szwarc</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.09712v1</id>\n",
      "    <title>Deep Anomaly Detection for Time-series Data in Industrial IoT: A Communication-Efficient On-device Federated Learning Approach</title>\n",
      "    <updated>2020-07-19T16:47:26Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.09712v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.09712v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Since edge device failures (i.e., anomalies) seriously affect the production of industrial products in Industrial IoT (IIoT), accurately and timely detecting anomalies is becoming increasingly important. Furthermore, data collected by the edge device may contain the user's private data, which is challenging the current detection approaches as user privacy is calling for the public concern in recent years. With this focus, this paper proposes a new communication-efficient on-device federated learning (FL)-based deep anomaly detection framework for sensing time-series data in IIoT. Specifically, we first introduce a FL framework to enable decentralized edge devices to collaboratively train an anomaly detection model, which can improve its generalization ability. Second, we propose an Attention Mechanism-based Convolutional Neural Network-Long Short Term Memory (AMCNN-LSTM) model to accurately detect anomalies. The AMCNN-LSTM model uses attention mechanism-based CNN units to capture important fine-grained features, thereby preventing memory loss and gradient dispersion problems. Furthermore, this model retains the advantages of LSTM unit in predicting time series data. Third, to adapt the proposed framework to the timeliness of industrial anomaly detection, we propose a gradient compression mechanism based on Top-\\textit{k} selection to improve communication efficiency. Extensive experiment studies on four real-world datasets demonstrate that the proposed framework can accurately and timely detect anomalies and also reduce the communication overhead by 50\\% compared to the federated learning framework that does not use a gradient compression scheme.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-19T16:47:26Z</published>\n",
      "    <arxiv:comment>IEEE Internet of Things Journal</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yi Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sahil Garg</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiangtian Nie</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yang Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zehui Xiong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiawen Kang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>M. Shamim Hossain</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/JIOT.2020.3011726</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/JIOT.2020.3011726\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.10560v1</id>\n",
      "    <title>FPGA-Based Hardware Accelerator of Homomorphic Encryption for Efficient Federated Learning</title>\n",
      "    <updated>2020-07-21T01:59:58Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.10560v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.10560v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>With the increasing awareness of privacy protection and data fragmentation problem, federated learning has been emerging as a new paradigm of machine learning. Federated learning tends to utilize various privacy preserving mechanisms to protect the transferred intermediate data, among which homomorphic encryption strikes a balance between security and ease of utilization. However, the complicated operations and large operands impose significant overhead on federated learning. Maintaining accuracy and security more efficiently has been a key problem of federated learning. In this work, we investigate a hardware solution, and design an FPGA-based homomorphic encryption framework, aiming to accelerate the training phase in federated learning. The root complexity lies in searching for a compact architecture for the core operation of homomorphic encryption, to suit the requirement of federated learning about high encryption throughput and flexibility of configuration. Our framework implements the representative Paillier homomorphic cryptosystem with high level synthesis for flexibility and portability, with careful optimization on the modular multiplication operation in terms of processing clock cycle, resource usage and clock frequency. Our accelerator achieves a near-optimal execution clock cycle, with a better DSP-efficiency than existing designs, and reduces the encryption time by up to 71% during training process of various federated learning models.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-21T01:59:58Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Zhaoxiong Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shuihai Hu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kai Chen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.10685v2</id>\n",
      "    <title>Pattern-Guided Integrated Gradients</title>\n",
      "    <updated>2020-09-01T11:33:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.10685v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.10685v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Integrated Gradients (IG) and PatternAttribution (PA) are two established explainability methods for neural networks. Both methods are theoretically well-founded. However, they were designed to overcome different challenges. In this work, we combine the two methods into a new method, Pattern-Guided Integrated Gradients (PGIG). PGIG inherits important properties from both parent methods and passes stress tests that the originals fail. In addition, we benchmark PGIG against nine alternative explainability approaches (including its parent methods) in a large-scale image degradation experiment and find that it outperforms all of them.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-21T09:51:33Z</published>\n",
      "    <arxiv:comment>Presented at the ICML 2020 Workshop on Human Interpretability in Machine Learning (WHI)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Robert Schwarzenberg</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Steffen Castle</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.10740v3</id>\n",
      "    <title>Balanced Meta-Softmax for Long-Tailed Visual Recognition</title>\n",
      "    <updated>2020-11-22T05:27:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.10740v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.10740v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep classifiers have achieved great success in visual recognition. However, real-world data is long-tailed by nature, leading to the mismatch between training and testing distributions. In this paper, we show that the Softmax function, though used in most classification tasks, gives a biased gradient estimation under the long-tailed setup. This paper presents Balanced Softmax, an elegant unbiased extension of Softmax, to accommodate the label distribution shift between training and testing. Theoretically, we derive the generalization bound for multiclass Softmax regression and show our loss minimizes the bound. In addition, we introduce Balanced Meta-Softmax, applying a complementary Meta Sampler to estimate the optimal class sample rate and further improve long-tailed learning. In our experiments, we demonstrate that Balanced Meta-Softmax outperforms state-of-the-art long-tailed classification solutions on both visual recognition and instance segmentation tasks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-21T12:05:00Z</published>\n",
      "    <arxiv:comment>NeurIPS 2020 camera-ready; Code available at https://github.com/jiawei-ren/BalancedMetaSoftmax</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jiawei Ren</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cunjun Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shunan Sheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiao Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Haiyu Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shuai Yi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hongsheng Li</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.10756v1</id>\n",
      "    <title>Understanding Consumer Preferences for Movie Trailers from EEG using Machine Learning</title>\n",
      "    <updated>2020-07-21T12:35:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.10756v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.10756v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Neuromarketing aims to understand consumer behavior using neuroscience. Brain imaging tools such as EEG have been used to better understand consumer behavior that goes beyond self-report measures which can be a more accurate measure to understand how and why consumers prefer choosing one product over another. Previous studies have shown that consumer preferences can be effectively predicted by understanding changes in evoked responses as captured by EEG. However, understanding ordered preference of choices was not studied earlier. In this study, we try to decipher the evoked responses using EEG while participants were presented with naturalistic stimuli i.e. movie trailers. Using Machine Learning tech niques to mine the patterns in EEG signals, we predicted the movie rating with more than above-chance, 72% accuracy. Our research shows that neural correlates can be an effective predictor of consumer choices and can significantly enhance our understanding of consumer behavior.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-21T12:35:18Z</published>\n",
      "    <arxiv:comment>This paper presented at \"The 6th Annual Conference of Cognitive Science\", 10-12 December, 2019 Link: https://drive.google.com/file/d/13gcTFUNg_2jA3sokOXv2f0U1D8xd-hkS/view</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Pankaj Pandey</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Raunak Swarnkar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shobhit Kakaria</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Krishna Prasad Miyapuram</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.10757v1</id>\n",
      "    <title>Inverting the Feature Visualization Process for Feedforward Neural Networks</title>\n",
      "    <updated>2020-07-21T12:44:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.10757v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.10757v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This work sheds light on the invertibility of feature visualization in neural networks. Since the input that is generated by feature visualization using activation maximization does, in general, not yield the feature objective it was optimized for, we investigate optimizing for the feature objective that yields this input. Given the objective function used in activation maximization that measures how closely a given input resembles the feature objective, we exploit that the gradient of this function w.r.t. inputs is---up to a scaling factor---linear in the objective. This observation is used to find the optimal feature objective via computing a closed form solution that minimizes the gradient. By means of Inverse Feature Visualization, we intend to provide an alternative view on a networks sensitivity to certain inputs that considers feature objectives rather than activations.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-21T12:44:46Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Christian Reinbold</name>\n",
      "      <arxiv:affiliation>Chair of Computer Graphics and Visualization, Technical University of Munich, Bavaria, Germany</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rüdiger Westermann</name>\n",
      "      <arxiv:affiliation>Chair of Computer Graphics and Visualization, Technical University of Munich, Bavaria, Germany</arxiv:affiliation>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.10760v3</id>\n",
      "    <title>Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review</title>\n",
      "    <updated>2020-08-02T08:38:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.10760v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.10760v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This work provides the community with a timely comprehensive review of backdoor attacks and countermeasures on deep learning. According to the attacker's capability and affected stage of the machine learning pipeline, the attack surfaces are recognized to be wide and then formalized into six categorizations: code poisoning, outsourcing, pretrained, data collection, collaborative learning and post-deployment. Accordingly, attacks under each categorization are combed. The countermeasures are categorized into four general classes: blind backdoor removal, offline backdoor inspection, online backdoor inspection, and post backdoor removal. Accordingly, we review countermeasures, and compare and analyze their advantages and disadvantages. We have also reviewed the flip side of backdoor attacks, which are explored for i) protecting intellectual property of deep learning models, ii) acting as a honeypot to catch adversarial example attacks, and iii) verifying data deletion requested by the data contributor.Overall, the research on defense is far behind the attack, and there is no single defense that can prevent all types of backdoor attacks. In some cases, an attacker can intelligently bypass existing defenses with an adaptive attack. Drawing the insights from the systematic review, we also present key areas for future research on the backdoor, such as empirical security evaluations from physical trigger attacks, and in particular, more efficient and practical countermeasures are solicited.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-21T12:49:12Z</published>\n",
      "    <arxiv:comment>29 pages, 9 figures, 2 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Yansong Gao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bao Gia Doan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhi Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Siqi Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiliang Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anmin Fu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Surya Nepal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hyoungshick Kim</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.10791v3</id>\n",
      "    <title>Learning to Match Distributions for Domain Adaptation</title>\n",
      "    <updated>2020-07-27T01:44:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.10791v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.10791v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>When the training and test data are from different distributions, domain adaptation is needed to reduce dataset bias to improve the model's generalization ability. Since it is difficult to directly match the cross-domain joint distributions, existing methods tend to reduce the marginal or conditional distribution divergence using predefined distances such as MMD and adversarial-based discrepancies. However, it remains challenging to determine which method is suitable for a given application since they are built with certain priors or bias. Thus they may fail to uncover the underlying relationship between transferable features and joint distributions. This paper proposes Learning to Match (L2M) to automatically learn the cross-domain distribution matching without relying on hand-crafted priors on the matching loss. Instead, L2M reduces the inductive bias by using a meta-network to learn the distribution matching loss in a data-driven way. L2M is a general framework that unifies task-independent and human-designed matching features. We design a novel optimization algorithm for this challenging objective with self-supervised label propagation. Experiments on public datasets substantiate the superiority of L2M over SOTA methods. Moreover, we apply L2M to transfer from pneumonia to COVID-19 chest X-ray images with remarkable performance. L2M can also be extended in other distribution matching applications where we show in a trial experiment that L2M generates more realistic and sharper MNIST samples.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-17T03:26:13Z</published>\n",
      "    <arxiv:comment>Preprint. 20 Pages. Code available at https://github.com/jindongwang/transferlearning/tree/master/code/deep/Learning-to-Match</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Chaohui Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jindong Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chang Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tao Qin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Renjun Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wenjie Feng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yiqiang Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tie-Yan Liu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.10099v2</id>\n",
      "    <title>Early Stopping in Deep Networks: Double Descent and How to Eliminate it</title>\n",
      "    <updated>2020-09-19T22:21:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.10099v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.10099v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Over-parameterized models, such as large deep networks, often exhibit a double descent phenomenon, whereas a function of model size, error first decreases, increases, and decreases at last. This intriguing double descent behavior also occurs as a function of training epochs and has been conjectured to arise because training epochs control the model complexity. In this paper, we show that such epoch-wise double descent arises for a different reason: It is caused by a superposition of two or more bias-variance tradeoffs that arise because different parts of the network are learned at different epochs, and eliminating this by proper scaling of stepsizes can significantly improve the early stopping performance. We show this analytically for i) linear regression, where differently scaled features give rise to a superposition of bias-variance tradeoffs, and for ii) a two-layer neural network, where the first and second layer each govern a bias-variance tradeoff. Inspired by this theory, we study two standard convolutional networks empirically and show that eliminating epoch-wise double descent through adjusting stepsizes of different layers improves the early stopping performance significantly.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-20T13:43:33Z</published>\n",
      "    <arxiv:comment>37 pages, 8 figures; changes from version 1: additional numerical results and clarifications</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Reinhard Heckel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fatih Furkan Yilmaz</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07192v4</id>\n",
      "    <title>How to Put Users in Control of their Data in Federated Top-N Recommendation with Learning to Rank</title>\n",
      "    <updated>2020-12-22T10:14:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07192v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07192v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recommendation services are extensively adopted in several user-centered applications as a tool to alleviate the information overload problem and help users in orienteering in a vast space of possible choices. In such scenarios, data ownership is a crucial concern since users may not be willing to share their sensitive preferences (e.g., visited locations) with a central server. Unfortunately, data harvesting and collection is at the basis of modern, state-of-the-art approaches to recommendation. To address this issue, we present FPL, an architecture in which users collaborate in training a central factorization model while controlling the amount of sensitive data leaving their devices. The proposed approach implements pair-wise learning-to-rank optimization by following the Federated Learning principles, originally conceived to mitigate the privacy risks of traditional machine learning. The public implementation is available at https://split.to/sisinflab-fpl.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-17T10:13:15Z</published>\n",
      "    <arxiv:comment>Accepted at the 36th ACM/SIGAPP Symposium on Applied Computing (SAC '21)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Vito Walter Anelli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yashar Deldjoo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tommaso Di Noia</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Antonio Ferrara</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fedelucio Narducci</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3412841.3442010</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3412841.3442010\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07231v1</id>\n",
      "    <title>StoRIR: Stochastic Room Impulse Response Generation for Audio Data Augmentation</title>\n",
      "    <updated>2020-08-17T11:56:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07231v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07231v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper we introduce StoRIR - a stochastic room impulse response generation method dedicated to audio data augmentation in machine learning applications. This technique, in contrary to geometrical methods like image-source or ray tracing, does not require prior definition of room geometry, absorption coefficients or microphone and source placement and is dependent solely on the acoustic parameters of the room. The method is intuitive, easy to implement and allows to generate RIRs of very complicated enclosures. We show that StoRIR, when used for audio data augmentation in a speech enhancement task, allows deep learning models to achieve better results on a wide range of metrics than when using the conventional image-source method, effectively improving many of them by more than 5 %. We publish a Python implementation of StoRIR online</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-17T11:56:47Z</published>\n",
      "    <arxiv:comment>Accepted for INTERSPEECH 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Piotr Masztalski</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mateusz Matuszewski</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Karol Piaskowski</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michał Romaniuk</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07247v1</id>\n",
      "    <title>Deep Learning Based Open Set Acoustic Scene Classification</title>\n",
      "    <updated>2020-08-17T12:23:27Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07247v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07247v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this work, we compare the performance of three selected techniques in open set acoustic scenes classification (ASC). We test thresholding of the softmax output of a deep network classifier, which is the most popular technique nowadays employed in ASC. Further we compare the results with the Openmax classifier which is derived from the computer vision field. As the third model, we use the Adapted Class-Conditioned Autoencoder (Adapted C2AE) which is our variation of another computer vision related technique called C2AE. Adapted C2AE encompasses a more fair comparison of the given experiments and simplifies the original inference procedure, making it more applicable in the real-life scenarios. We also analyse two training scenarios: without additional knowledge of unknown classes and another where a limited subset of examples from the unknown classes is available. We find that the C2AE based method outperforms the thresholding and Openmax, obtaining $85.5\\%$ Area Under the Receiver Operating Characteristic curve (AUROC) and $66\\%$ of open set accuracy on data used in Detection and Classification of Acoustic Scenes and Events Challenge 2019 Task 1C.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-17T12:23:27Z</published>\n",
      "    <arxiv:comment>This paper was submitted to conference INTERSPEECH 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Zuzanna Kwiatkowska</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Beniamin Kalinowski</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michał Kośmider</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Krzysztof Rykaczewski</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07280v1</id>\n",
      "    <title>Assessing Viewer's Mental Health by Detecting Depression in YouTube Videos</title>\n",
      "    <updated>2020-07-29T16:17:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07280v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07280v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Depression is one of the most prevalent mental health issues around the world, proving to be one of the leading causes of suicide and placing large economic burdens on families and society. In this paper, we develop and test the efficacy of machine learning techniques applied to the content of YouTube videos captured through their transcripts and determine if the videos are depressive or have a depressing trigger. Our model can detect depressive videos with an accuracy of 83%. We also introduce a real-life evaluation technique to validate our classification based on the comments posted on a video by calculating the CES-D scores of the comments. This work conforms greatly with the UN Sustainable Goal of ensuring Good Health and Well Being with major conformity with section UN SDG 3.4.</summary>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-29T16:17:35Z</published>\n",
      "    <arxiv:comment>AI for Social Good workshop at NeurIPS (2019), Vancouver, Canada</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CY\"/>\n",
      "    <author>\n",
      "      <name>Shanya Sharma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Manan Dey</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07330v1</id>\n",
      "    <title>Optimal Posteriors for Chi-squared Divergence based PAC-Bayesian Bounds and Comparison with KL-divergence based Optimal Posteriors and Cross-Validation Procedure</title>\n",
      "    <updated>2020-08-14T03:15:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07330v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07330v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We investigate optimal posteriors for recently introduced \\cite{begin2016pac} chi-squared divergence based PAC-Bayesian bounds in terms of nature of their distribution, scalability of computations, and test set performance. For a finite classifier set, we deduce bounds for three distance functions: KL-divergence, linear and squared distances. Optimal posterior weights are proportional to deviations of empirical risks, usually with subset support. For uniform prior, it is sufficient to search among posteriors on classifier subsets ordered by these risks. We show the bound minimization for linear distance as a convex program and obtain a closed-form expression for its optimal posterior. Whereas that for squared distance is a quasi-convex program under a specific condition, and the one for KL-divergence is non-convex optimization (a difference of convex functions). To compute such optimal posteriors, we derive fast converging fixed point (FP) equations. We apply these approaches to a finite set of SVM regularization parameter values to yield stochastic SVMs with tight bounds. We perform a comprehensive performance comparison between our optimal posteriors and known KL-divergence based posteriors on a variety of UCI datasets with varying ranges and variances in risk values, etc. Chi-squared divergence based posteriors have weaker bounds and worse test errors, hinting at an underlying regularization by KL-divergence based posteriors. Our study highlights the impact of divergence function on the performance of PAC-Bayesian classifiers. We compare our stochastic classifiers with cross-validation based deterministic classifier. The latter has better test errors, but ours is more sample robust, has quantifiable generalization guarantees, and is computationally much faster.</summary>\n",
      "    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-14T03:15:23Z</published>\n",
      "    <arxiv:comment>arXiv admin note: text overlap with arXiv:1912.06803</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"math.ST\"/>\n",
      "    <author>\n",
      "      <name>Puja Sahu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nandyala Hemachandra</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07338v1</id>\n",
      "    <title>Predicting United States policy outcomes with Random Forests</title>\n",
      "    <updated>2020-08-02T18:06:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07338v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07338v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Two decades of U.S. government legislative outcomes, as well as the policy preferences of rich people, the general population, and diverse interest groups, were captured in a detailed dataset curated and analyzed by Gilens, Page et al. (2014). They found that the preferences of the rich correlated strongly with policy outcomes, while the preferences of the general population did not, except via a linkage with rich people's preferences. Their analysis applied the tools of classical statistical inference, in particular logistic regression. In this paper we analyze the Gilens dataset using the complementary tools of Random Forest classifiers (RFs), from Machine Learning. We present two primary findings, concerning respectively prediction and inference: (i) Holdout test sets can be predicted with approximately 70% balanced accuracy by models that consult only the preferences of rich people and a small number of powerful interest groups, as well as policy area labels. These results include retrodiction, where models trained on pre-1997 cases predicted \"future\" (post-1997) cases. The 20% gain in accuracy over baseline (chance), in this detailed but noisy dataset, indicates the high importance of a few wealthy players in U.S. policy outcomes, and aligns with a body of research indicating that the U.S. government has significant plutocratic tendencies. (ii) The feature selection methods of RF models identify especially salient subsets of interest groups (economic players). These can be used to further investigate the dynamics of governmental policy making, and also offer an example of the potential value of RF feature selection methods for inference on datasets such as this.</summary>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-02T18:06:57Z</published>\n",
      "    <arxiv:comment>13 pages, 2 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CY\"/>\n",
      "    <author>\n",
      "      <name>Shawn McGuire</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Charles Delahunt</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07405v1</id>\n",
      "    <title>Network Intrusion Detection Using Wrapper-based Decision Tree for Feature Selection</title>\n",
      "    <updated>2020-08-11T04:00:58Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07405v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07405v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>One of the key challenges of machine learning (ML) based intrusion detection system (IDS) is the expensive computational complexity which is largely due to redundant, incomplete, and irrelevant features contain in the IDS datasets. To overcome such challenge and ensure building an efficient and more accurate IDS models, many researchers utilize preprocessing techniques such as normalization and feature selection in a hybrid modeling approach. In this work, we propose a hybrid IDS modeling approach with an algorithm for feature selection (FS) and another for building an IDS. The FS algorithm is a wrapper-based with a decision tree as the feature evaluator. The propose FS method is used in combination with some selected ML algorithms to build IDS models using the UNSW-NB15 dataset. Some IDS models are built as a baseline in a single modeling approach using the full features of the dataset. We evaluate the effectiveness of our propose method by comparing it with the baseline models and also with state-of-the-art works. Our method achieves the best DR of 97.95% and shown to be quite effective in comparison to state-of-the-art works. We, therefore, recommend its usage especially in IDS modeling with the UNSW-NB15 dataset.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-11T04:00:58Z</published>\n",
      "    <arxiv:comment>8 pages, 3 figures, Presented at ICICSE 2020 Conference Proceedings, which will be published in the International Conference Proceedings Series by ACM, and will be archived in the ACM Digital Library</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Mubarak Albarka Umar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chen Zhanfang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yan Liu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07426v1</id>\n",
      "    <title>Hey Human, If your Facial Emotions are Uncertain, You Should Use Bayesian Neural Networks!</title>\n",
      "    <updated>2020-08-17T15:50:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07426v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07426v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Facial emotion recognition is the task to classify human emotions in face images. It is a difficult task due to high aleatoric uncertainty and visual ambiguity. A large part of the literature aims to show progress by increasing accuracy on this task, but this ignores the inherent uncertainty and ambiguity in the task. In this paper we show that Bayesian Neural Networks, as approximated using MC-Dropout, MC-DropConnect, or an Ensemble, are able to model the aleatoric uncertainty in facial emotion recognition, and produce output probabilities that are closer to what a human expects. We also show that calibration metrics show strange behaviors for this task, due to the multiple classes that can be considered correct, which motivates future work. We believe our work will motivate other researchers to move away from Classical and into Bayesian Neural Networks.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-17T15:50:40Z</published>\n",
      "    <arxiv:comment>10 pages, 7 figures, Women in Computer Vision @ ECCV 2020 camera ready</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Maryam Matin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matias Valdenegro-Toro</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07599v1</id>\n",
      "    <title>Learning from Irregularly-Sampled Time Series: A Missing Data Perspective</title>\n",
      "    <updated>2020-08-17T20:01:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07599v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07599v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Irregularly-sampled time series occur in many domains including healthcare. They can be challenging to model because they do not naturally yield a fixed-dimensional representation as required by many standard machine learning models. In this paper, we consider irregular sampling from the perspective of missing data. We model observed irregularly-sampled time series data as a sequence of index-value pairs sampled from a continuous but unobserved function. We introduce an encoder-decoder framework for learning from such generic indexed sequences. We propose learning methods for this framework based on variational autoencoders and generative adversarial networks. For continuous irregularly-sampled time series, we introduce continuous convolutional layers that can efficiently interface with existing neural network architectures. Experiments show that our models are able to achieve competitive or better classification results on irregularly-sampled multivariate time series compared to recent RNN models while offering significantly faster training times.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-17T20:01:55Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Steven Cheng-Xian Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Benjamin M. Marlin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07633v1</id>\n",
      "    <title>SF-GRASS: Solver-Free Graph Spectral Sparsification</title>\n",
      "    <updated>2020-08-17T21:37:19Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07633v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07633v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent spectral graph sparsification techniques have shown promising performance in accelerating many numerical and graph algorithms, such as iterative methods for solving large sparse matrices, spectral partitioning of undirected graphs, vectorless verification of power/thermal grids, representation learning of large graphs, etc. However, prior spectral graph sparsification methods rely on fast Laplacian matrix solvers that are usually challenging to implement in practice. This work, for the first time, introduces a solver-free approach (SF-GRASS) for spectral graph sparsification by leveraging emerging spectral graph coarsening and graph signal processing (GSP) techniques. We introduce a local spectral embedding scheme for efficiently identifying spectrally-critical edges that are key to preserving graph spectral properties, such as the first few Laplacian eigenvalues and eigenvectors. Since the key kernel functions in SF-GRASS can be efficiently implemented using sparse-matrix-vector-multiplications (SpMVs), the proposed spectral approach is simple to implement and inherently parallel friendly. Our extensive experimental results show that the proposed method can produce a hierarchy of high-quality spectral sparsifiers in nearly-linear time for a variety of real-world, large-scale graphs and circuit networks when compared with the prior state-of-the-art spectral method.</summary>\n",
      "    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-17T21:37:19Z</published>\n",
      "    <arxiv:primary_category term=\"cs.DS\"/>\n",
      "    <author>\n",
      "      <name>Ying Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhiqiang Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhuo Feng</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06626v1</id>\n",
      "    <title>Safe Reinforcement Learning in Constrained Markov Decision Processes</title>\n",
      "    <updated>2020-08-15T02:20:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06626v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06626v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Safe reinforcement learning has been a promising approach for optimizing the policy of an agent that operates in safety-critical applications. In this paper, we propose an algorithm, SNO-MDP, that explores and optimizes Markov decision processes under unknown safety constraints. Specifically, we take a stepwise approach for optimizing safety and cumulative reward. In our method, the agent first learns safety constraints by expanding the safe region, and then optimizes the cumulative reward in the certified safe region. We provide theoretical guarantees on both the satisfaction of the safety constraint and the near-optimality of the cumulative reward under proper regularity assumptions. In our experiments, we demonstrate the effectiveness of SNO-MDP through two experiments: one uses a synthetic data in a new, openly-available environment named GP-SAFETY-GYM, and the other simulates Mars surface exploration by using real observation data.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-15T02:20:23Z</published>\n",
      "    <arxiv:comment>10 pages, 6 figures, Accepted to ICML2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Akifumi Wachi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yanan Sui</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06696v1</id>\n",
      "    <title>Autonomous Braking and Throttle System: A Deep Reinforcement Learning Approach for Naturalistic Driving</title>\n",
      "    <updated>2020-08-15T10:37:07Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06696v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06696v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Autonomous Braking and Throttle control is key in developing safe driving systems for the future. There exists a need for autonomous vehicles to negotiate a multi-agent environment while ensuring safety and comfort. A Deep Reinforcement Learning based autonomous throttle and braking system is presented. For each time step, the proposed system makes a decision to apply the brake or throttle. The throttle and brake are modelled as continuous action space values. We demonstrate 2 scenarios where there is a need for a sophisticated braking and throttle system, i.e when there is a static obstacle in front of our agent like a car, stop sign. The second scenario consists of 2 vehicles approaching an intersection. The policies for brake and throttle control are learned through computer simulation using Deep deterministic policy gradients. The experiment shows that the system not only avoids a collision, but also it ensures that there is smooth change in the values of throttle/brake as it gets out of the emergency situation and abides by the speed regulations, i.e the system resembles human driving.</summary>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-15T10:37:07Z</published>\n",
      "    <arxiv:primary_category term=\"cs.AI\"/>\n",
      "    <author>\n",
      "      <name>Varshit S. Dubey</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruhshad Kasad</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Karan Agrawal</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06729v1</id>\n",
      "    <title>Reliable Uncertainties for Bayesian Neural Networks using Alpha-divergences</title>\n",
      "    <updated>2020-08-15T15:03:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06729v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06729v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Bayesian Neural Networks (BNNs) often result uncalibrated after training, usually tending towards overconfidence. Devising effective calibration methods with low impact in terms of computational complexity is thus of central interest. In this paper we present calibration methods for BNNs based on the alpha divergences from Information Geometry. We compare the use of alpha divergence in training and in calibration, and we show how the use in calibration provides better calibrated uncertainty estimates for specific choices of alpha and is more efficient especially for complex network architectures. We empirically demonstrate the advantages of alpha calibration in regression problems involving parameter estimation and inferred correlations between output uncertainties.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-15T15:03:46Z</published>\n",
      "    <arxiv:comment>Accepted at the ICML 2020: Workshop on Uncertainty and Robustness in Deep Learning</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Hector J. Hortua</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luigi Malago</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Riccardo Volpi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06736v1</id>\n",
      "    <title>Obtaining Adjustable Regularization for Free via Iterate Averaging</title>\n",
      "    <updated>2020-08-15T15:28:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06736v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06736v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Regularization for optimization is a crucial technique to avoid overfitting in machine learning. In order to obtain the best performance, we usually train a model by tuning the regularization parameters. It becomes costly, however, when a single round of training takes significant amount of time. Very recently, Neu and Rosasco show that if we run stochastic gradient descent (SGD) on linear regression problems, then by averaging the SGD iterates properly, we obtain a regularized solution. It left open whether the same phenomenon can be achieved for other optimization problems and algorithms. In this paper, we establish an averaging scheme that provably converts the iterates of SGD on an arbitrary strongly convex and smooth objective function to its regularized counterpart with an adjustable regularization parameter. Our approaches can be used for accelerated and preconditioned optimization methods as well. We further show that the same methods work empirically on more general optimization objectives including neural networks. In sum, we obtain adjustable regularization for free for a large class of optimization problems and resolve an open question raised by Neu and Rosasco.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-15T15:28:05Z</published>\n",
      "    <arxiv:comment>ICML 2020 camera ready</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jingfeng Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vladimir Braverman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lin F. Yang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06775v1</id>\n",
      "    <title>Model Patching: Closing the Subgroup Performance Gap with Data Augmentation</title>\n",
      "    <updated>2020-08-15T20:01:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06775v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06775v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Classifiers in machine learning are often brittle when deployed. Particularly concerning are models with inconsistent performance on specific subgroups of a class, e.g., exhibiting disparities in skin cancer classification in the presence or absence of a spurious bandage. To mitigate these performance differences, we introduce model patching, a two-stage framework for improving robustness that encourages the model to be invariant to subgroup differences, and focus on class information shared by subgroups. Model patching first models subgroup features within a class and learns semantic transformations between them, and then trains a classifier with data augmentations that deliberately manipulate subgroup features. We instantiate model patching with CAMEL, which (1) uses a CycleGAN to learn the intra-class, inter-subgroup augmentations, and (2) balances subgroup performance using a theoretically-motivated subgroup consistency regularizer, accompanied by a new robust objective. We demonstrate CAMEL's effectiveness on 3 benchmark datasets, with reductions in robust error of up to 33% relative to the best baseline. Lastly, CAMEL successfully patches a model that fails due to spurious features on a real-world skin cancer dataset.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-15T20:01:23Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Karan Goel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Albert Gu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yixuan Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christopher Ré</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06786v1</id>\n",
      "    <title>The Neural Tangent Kernel in High Dimensions: Triple Descent and a Multi-Scale Theory of Generalization</title>\n",
      "    <updated>2020-08-15T20:55:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06786v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06786v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Modern deep learning models employ considerably more parameters than required to fit the training data. Whereas conventional statistical wisdom suggests such models should drastically overfit, in practice these models generalize remarkably well. An emerging paradigm for describing this unexpected behavior is in terms of a \\emph{double descent} curve, in which increasing a model's capacity causes its test error to first decrease, then increase to a maximum near the interpolation threshold, and then decrease again in the overparameterized regime. Recent efforts to explain this phenomenon theoretically have focused on simple settings, such as linear regression or kernel regression with unstructured random features, which we argue are too coarse to reveal important nuances of actual neural networks. We provide a precise high-dimensional asymptotic analysis of generalization under kernel regression with the Neural Tangent Kernel, which characterizes the behavior of wide neural networks optimized with gradient descent. Our results reveal that the test error has non-monotonic behavior deep in the overparameterized regime and can even exhibit additional peaks and descents when the number of parameters scales quadratically with the dataset size.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-15T20:55:40Z</published>\n",
      "    <arxiv:comment>Published as a conference paper in the Proceedings of the 37th International Conference on Machine Learning; 31 pages; 4 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Ben Adlam</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jeffrey Pennington</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07001v2</id>\n",
      "    <title>Learning Disentangled Expression Representations from Facial Images</title>\n",
      "    <updated>2020-08-18T06:58:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07001v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07001v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Face images are subject to many different factors of variation, especially in unconstrained in-the-wild scenarios. For most tasks involving such images, e.g. expression recognition from video streams, having enough labeled data is prohibitively expensive. One common strategy to tackle such a problem is to learn disentangled representations for the different factors of variation of the observed data using adversarial learning. In this paper, we use a formulation of the adversarial loss to learn disentangled representations for face images. The used model facilitates learning on single-task datasets and improves the state-of-the-art in expression recognition with an accuracy of60.53%on the AffectNetdataset, without using any additional data.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-16T21:23:32Z</published>\n",
      "    <arxiv:comment>Accepted at ECCV2020 workshops</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Marah Halawa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Manuel Wöllhaf</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eduardo Vellasques</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Urko Sánchez Sanz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Olaf Hellwich</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07711v1</id>\n",
      "    <title>One-pixel Signature: Characterizing CNN Models for Backdoor Detection</title>\n",
      "    <updated>2020-08-18T02:54:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07711v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07711v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We tackle the convolution neural networks (CNNs) backdoor detection problem by proposing a new representation called one-pixel signature. Our task is to detect/classify if a CNN model has been maliciously inserted with an unknown Trojan trigger or not. Here, each CNN model is associated with a signature that is created by generating, pixel-by-pixel, an adversarial value that is the result of the largest change to the class prediction. The one-pixel signature is agnostic to the design choice of CNN architectures, and how they were trained. It can be computed efficiently for a black-box CNN model without accessing the network parameters. Our proposed one-pixel signature demonstrates a substantial improvement (by around 30% in the absolute detection accuracy) over the existing competing methods for backdoored CNN detection/classification. One-pixel signature is a general representation that can be used to characterize CNN models beyond backdoor detection.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-18T02:54:47Z</published>\n",
      "    <arxiv:comment>Accepted at ECCV 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Shanjiaoyang Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Weiqi Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhiwei Jia</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhuowen Tu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07724v1</id>\n",
      "    <title>Domain Generalizer: A Few-shot Meta Learning Framework for Domain Generalization in Medical Imaging</title>\n",
      "    <updated>2020-08-18T03:35:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07724v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07724v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep learning models perform best when tested on target (test) data domains whose distribution is similar to the set of source (train) domains. However, model generalization can be hindered when there is significant difference in the underlying statistics between the target and source domains. In this work, we adapt a domain generalization method based on a model-agnostic meta-learning framework to biomedical imaging. The method learns a domain-agnostic feature representation to improve generalization of models to the unseen test distribution. The method can be used for any imaging task, as it does not depend on the underlying model architecture. We validate the approach through a computed tomography (CT) vertebrae segmentation task across healthy and pathological cases on three datasets. Next, we employ few-shot learning, i.e. training the generalized model using very few examples from the unseen domain, to quickly adapt the model to new unseen data distribution. Our results suggest that the method could help generalize models across different medical centers, image acquisition protocols, anatomies, different regions in a given scan, healthy and diseased populations across varied imaging modalities.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-18T03:35:56Z</published>\n",
      "    <arxiv:comment>Medical Image Computing and Computer Assisted Interventions (MICCAI) 2020 to be presented at DART 2020. Supplementary material and link to code included</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Pulkit Khandelwal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paul Yushkevich</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07788v1</id>\n",
      "    <title>CinC-GAN for Effective F0 prediction for Whisper-to-Normal Speech Conversion</title>\n",
      "    <updated>2020-08-18T07:56:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07788v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07788v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recently, Generative Adversarial Networks (GAN)-based methods have shown remarkable performance for the Voice Conversion and WHiSPer-to-normal SPeeCH (WHSP2SPCH) conversion. One of the key challenges in WHSP2SPCH conversion is the prediction of fundamental frequency (F0). Recently, authors have proposed state-of-the-art method Cycle-Consistent Generative Adversarial Networks (CycleGAN) for WHSP2SPCH conversion. The CycleGAN-based method uses two different models, one for Mel Cepstral Coefficients (MCC) mapping, and another for F0 prediction, where F0 is highly dependent on the pre-trained model of MCC mapping. This leads to additional non-linear noise in predicted F0. To suppress this noise, we propose Cycle-in-Cycle GAN (i.e., CinC-GAN). It is specially designed to increase the effectiveness in F0 prediction without losing the accuracy of MCC mapping. We evaluated the proposed method on a non-parallel setting and analyzed on speaker-specific, and gender-specific tasks. The objective and subjective tests show that CinC-GAN significantly outperforms the CycleGAN. In addition, we analyze the CycleGAN and CinC-GAN for unseen speakers and the results show the clear superiority of CinC-GAN.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-18T07:56:16Z</published>\n",
      "    <arxiv:comment>Accepted in 28th European Signal Processing Conference (EUSIPCO), 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Maitreya Patel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mirali Purohit</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jui Shah</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hemant A. Patil</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07832v1</id>\n",
      "    <title>Tackling the Unannotated: Scene Graph Generation with Bias-Reduced Models</title>\n",
      "    <updated>2020-08-18T10:04:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07832v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07832v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Predicting a scene graph that captures visual entities and their interactions in an image has been considered a crucial step towards full scene comprehension. Recent scene graph generation (SGG) models have shown their capability of capturing the most frequent relations among visual entities. However, the state-of-the-art results are still far from satisfactory, e.g. models can obtain 31% in overall recall R@100, whereas the likewise important mean class-wise recall mR@100 is only around 8% on Visual Genome (VG). The discrepancy between R and mR results urges to shift the focus from pursuing a high R to a high mR with a still competitive R. We suspect that the observed discrepancy stems from both the annotation bias and sparse annotations in VG, in which many visual entity pairs are either not annotated at all or only with a single relation when multiple ones could be valid. To address this particular issue, we propose a novel SGG training scheme that capitalizes on self-learned knowledge. It involves two relation classifiers, one offering a less biased setting for the other to base on. The proposed scheme can be applied to most of the existing SGG models and is straightforward to implement. We observe significant relative improvements in mR (between +6.6% and +20.4%) and competitive or better R (between -2.4% and 0.3%) across all standard SGG tasks.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-18T10:04:51Z</published>\n",
      "    <arxiv:comment>accepted to BMVC2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Tzu-Jui Julius Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Selen Pehlivan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jorma Laaksonen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.07015v2</id>\n",
      "    <title>Adversarial Concurrent Training: Optimizing Robustness and Accuracy Trade-off of Deep Neural Networks</title>\n",
      "    <updated>2020-08-18T18:31:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.07015v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.07015v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Adversarial training has been proven to be an effective technique for improving the adversarial robustness of models. However, there seems to be an inherent trade-off between optimizing the model for accuracy and robustness. To this end, we propose Adversarial Concurrent Training (ACT), which employs adversarial training in a collaborative learning framework whereby we train a robust model in conjunction with a natural model in a minimax game. ACT encourages the two models to align their feature space by using the task-specific decision boundaries and explore the input space more broadly. Furthermore, the natural model acts as a regularizer, enforcing priors on features that the robust model should learn. Our analyses on the behavior of the models show that ACT leads to a robust model with lower model complexity, higher information compression in the learned representations, and high posterior entropy solutions indicative of convergence to a flatter minima. We demonstrate the effectiveness of the proposed approach across different datasets and network architectures. On ImageNet, ACT achieves 68.20% standard accuracy and 44.29% robustness accuracy under a 100-iteration untargeted attack, improving upon the standard adversarial training method's 65.70% standard accuracy and 42.36% robustness.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-16T22:14:48Z</published>\n",
      "    <arxiv:comment>Accepted at 31st British Machine Vision Conference (BMVC) 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Elahe Arani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fahad Sarfraz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bahram Zonooz</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.11133v1</id>\n",
      "    <title>Unsupervised Learning of Solutions to Differential Equations with Generative Adversarial Networks</title>\n",
      "    <updated>2020-07-21T23:36:36Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.11133v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.11133v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Solutions to differential equations are of significant scientific and engineering relevance. Recently, there has been a growing interest in solving differential equations with neural networks. This work develops a novel method for solving differential equations with unsupervised neural networks that applies Generative Adversarial Networks (GANs) to \\emph{learn the loss function} for optimizing the neural network. We present empirical results showing that our method, which we call Differential Equation GAN (DEQGAN), can obtain multiple orders of magnitude lower mean squared errors than an alternative unsupervised neural network method based on (squared) $L_2$, $L_1$, and Huber loss functions. Moreover, we show that DEQGAN achieves solution accuracy that is competitive with traditional numerical methods. Finally, we analyze the stability of our approach and find it to be sensitive to the selection of hyperparameters, which we provide in the appendix.\n",
      "  Code available at https://github.com/dylanrandle/denn. Please address any electronic correspondence to dylanrandle@alumni.harvard.edu.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-21T23:36:36Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Dylan Randle</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pavlos Protopapas</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Sondak</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.11167v1</id>\n",
      "    <title>Learning the Latent Space of Robot Dynamics for Cutting Interaction Inference</title>\n",
      "    <updated>2020-07-22T02:29:33Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.11167v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.11167v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Utilization of latent space to capture a lower-dimensional representation of a complex dynamics model is explored in this work. The targeted application is of a robotic manipulator executing a complex environment interaction task, in particular, cutting a wooden object. We train two flavours of Variational Autoencoders---standard and Vector-Quantised---to learn the latent space which is then used to infer certain properties of the cutting operation, such as whether the robot is cutting or not, as well as, material and geometry of the object being cut. The two VAE models are evaluated with reconstruction, prediction and a combined reconstruction/prediction decoders. The results demonstrate the expressiveness of the latent space for robotic interaction inference and the competitive prediction performance against recurrent neural networks.</summary>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-22T02:29:33Z</published>\n",
      "    <arxiv:comment>IROS2020. Copyright 20xx IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.RO\"/>\n",
      "    <author>\n",
      "      <name>Sahand Rezaei-Shoshtari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Meger</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Inna Sharf</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.11201v1</id>\n",
      "    <title>IITK at the FinSim Task: Hypernym Detection in Financial Domain via Context-Free and Contextualized Word Embeddings</title>\n",
      "    <updated>2020-07-22T04:56:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.11201v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.11201v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we present our approaches for the FinSim 2020 shared task on \"Learning Semantic Representations for the Financial Domain\". The goal of this task is to classify financial terms into the most relevant hypernym (or top-level) concept in an external ontology. We leverage both context-dependent and context-independent word embeddings in our analysis. Our systems deploy Word2vec embeddings trained from scratch on the corpus (Financial Prospectus in English) along with pre-trained BERT embeddings. We divide the test dataset into two subsets based on a domain rule. For one subset, we use unsupervised distance measures to classify the term. For the second subset, we use simple supervised classifiers like Naive Bayes, on top of the embeddings, to arrive at a final prediction. Finally, we combine both the results. Our system ranks 1st based on both the metrics, i.e., mean rank and accuracy.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-fin.CP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-22T04:56:23Z</published>\n",
      "    <arxiv:comment>6 pages, 1 figure, 4 tables. Accepted at the Second Workshop on Financial Technology and Natural Language Processing (FinNLP-2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Vishal Keswani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sakshi Singh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ashutosh Modi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.11230v1</id>\n",
      "    <title>MetAL: Active Semi-Supervised Learning on Graphs via Meta Learning</title>\n",
      "    <updated>2020-07-22T06:59:49Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.11230v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.11230v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The objective of active learning (AL) is to train classification models with less number of labeled instances by selecting only the most informative instances for labeling. The AL algorithms designed for other data types such as images and text do not perform well on graph-structured data. Although a few heuristics-based AL algorithms have been proposed for graphs, a principled approach is lacking. In this paper, we propose MetAL, an AL approach that selects unlabeled instances that directly improve the future performance of a classification model. For a semi-supervised learning problem, we formulate the AL task as a bilevel optimization problem. Based on recent work in meta-learning, we use the meta-gradients to approximate the impact of retraining the model with any unlabeled instance on the model performance. Using multiple graph datasets belonging to different domains, we demonstrate that MetAL efficiently outperforms existing state-of-the-art AL algorithms.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-22T06:59:49Z</published>\n",
      "    <arxiv:comment>16 pages, 4 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Kaushalya Madhawa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tsuyoshi Murata</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.09296v1</id>\n",
      "    <title>Towards Deeper Graph Neural Networks</title>\n",
      "    <updated>2020-07-18T01:11:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.09296v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.09296v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Graph neural networks have shown significant success in the field of graph representation learning. Graph convolutions perform neighborhood aggregation and represent one of the most important graph operations. Nevertheless, one layer of these neighborhood aggregation methods only consider immediate neighbors, and the performance decreases when going deeper to enable larger receptive fields. Several recent studies attribute this performance deterioration to the over-smoothing issue, which states that repeated propagation makes node representations of different classes indistinguishable. In this work, we study this observation systematically and develop new insights towards deeper graph neural networks. First, we provide a systematical analysis on this issue and argue that the key factor compromising the performance significantly is the entanglement of representation transformation and propagation in current graph convolution operations. After decoupling these two operations, deeper graph neural networks can be used to learn graph node representations from larger receptive fields. We further provide a theoretical analysis of the above observation when building very deep models, which can serve as a rigorous and gentle description of the over-smoothing issue. Based on our theoretical and empirical analysis, we propose Deep Adaptive Graph Neural Network (DAGNN) to adaptively incorporate information from large receptive fields. A set of experiments on citation, co-authorship, and co-purchase datasets have confirmed our analysis and insights and demonstrated the superiority of our proposed methods.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-18T01:11:14Z</published>\n",
      "    <arxiv:comment>11 pages, KDD2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Meng Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hongyang Gao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shuiwang Ji</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3394486.3403076</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3394486.3403076\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.09334v1</id>\n",
      "    <title>Deep Learning of High-Order Interactions for Protein Interface Prediction</title>\n",
      "    <updated>2020-07-18T05:39:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.09334v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.09334v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Protein interactions are important in a broad range of biological processes. Traditionally, computational methods have been developed to automatically predict protein interface from hand-crafted features. Recent approaches employ deep neural networks and predict the interaction of each amino acid pair independently. However, these methods do not incorporate the important sequential information from amino acid chains and the high-order pairwise interactions. Intuitively, the prediction of an amino acid pair should depend on both their features and the information of other amino acid pairs. In this work, we propose to formulate the protein interface prediction as a 2D dense prediction problem. In addition, we propose a novel deep model to incorporate the sequential information and high-order pairwise interactions to perform interface predictions. We represent proteins as graphs and employ graph neural networks to learn node features. Then we propose the sequential modeling method to incorporate the sequential information and reorder the feature matrix. Next, we incorporate high-order pairwise interactions to generate a 3D tensor containing different pairwise interactions. Finally, we employ convolutional neural networks to perform 2D dense predictions. Experimental results on multiple benchmarks demonstrate that our proposed method can consistently improve the protein interface prediction performance.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.MN\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-18T05:39:35Z</published>\n",
      "    <arxiv:comment>10 pages, 3 figures, 4 tables. KDD2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yi Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hao Yuan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lei Cai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shuiwang Ji</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3394486.3403110</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3394486.3403110\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.09392v1</id>\n",
      "    <title>Distributed Learning via Filtered Hyperinterpolation on Manifolds</title>\n",
      "    <updated>2020-07-18T10:05:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.09392v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.09392v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Learning mappings of data on manifolds is an important topic in contemporary machine learning, with applications in astrophysics, geophysics, statistical physics, medical diagnosis, biochemistry, 3D object analysis. This paper studies the problem of learning real-valued functions on manifolds through filtered hyperinterpolation of input-output data pairs where the inputs may be sampled deterministically or at random and the outputs may be clean or noisy. Motivated by the problem of handling large data sets, it presents a parallel data processing approach which distributes the data-fitting task among multiple servers and synthesizes the fitted sub-models into a global estimator. We prove quantitative relations between the approximation quality of the learned function over the entire manifold, the type of target function, the number of servers, and the number and type of available samples. We obtain the approximation rates of convergence for distributed and non-distributed approaches. For the non-distributed case, the approximation order is optimal.</summary>\n",
      "    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-18T10:05:18Z</published>\n",
      "    <arxiv:comment>50 pages, 4 figures, 2 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"math.NA\"/>\n",
      "    <author>\n",
      "      <name>Guido Montúfar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu Guang Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.09426v1</id>\n",
      "    <title>Improved Convergence Speed of Fully Symmetric Learning Rules for Principal Component Analysis</title>\n",
      "    <updated>2020-07-18T13:41:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.09426v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.09426v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Fully symmetric learning rules for principal component analysis can be derived from a novel objective function suggested in our previous work. We observed that these learning rules suffer from slow convergence for covariance matrices where some principal eigenvalues are close to each other. Here we describe a modified objective function with an additional term which mitigates this convergence problem. We show that the learning rule derived from the modified objective function inherits all fixed points from the original learning rule (but may introduce additional ones). Also the stability of the inherited fixed points remains unchanged. Only the steepness of the objective function is increased in some directions. Simulations confirm that the convergence speed can be noticeably improved, depending on the weight factor of the additional term.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-18T13:41:35Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Ralf Möller</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.11641v1</id>\n",
      "    <title>Attention based Multiple Instance Learning for Classification of Blood Cell Disorders</title>\n",
      "    <updated>2020-07-22T19:29:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.11641v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.11641v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Red blood cells are highly deformable and present in various shapes. In blood cell disorders, only a subset of all cells is morphologically altered and relevant for the diagnosis. However, manually labeling of all cells is laborious, complicated and introduces inter-expert variability. We propose an attention based multiple instance learning method to classify blood samples of patients suffering from blood cell disorders. Cells are detected using an R-CNN architecture. With the features extracted for each cell, a multiple instance learning method classifies patient samples into one out of four blood cell disorders. The attention mechanism provides a measure of the contribution of each cell to the overall classification and significantly improves the network's classification accuracy as well as its interpretability for the medical expert.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-22T19:29:40Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Ario Sadafi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Asya Makhro</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anna Bogdanova</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nassir Navab</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tingying Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shadi Albarqouni</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Carsten Marr</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.11794v1</id>\n",
      "    <title>Applying GPGPU to Recurrent Neural Network Language Model based Fast Network Search in the Real-Time LVCSR</title>\n",
      "    <updated>2020-07-23T05:15:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.11794v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.11794v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recurrent Neural Network Language Models (RNNLMs) have started to be used in various fields of speech recognition due to their outstanding performance. However, the high computational complexity of RNNLMs has been a hurdle in applying the RNNLM to a real-time Large Vocabulary Continuous Speech Recognition (LVCSR). In order to accelerate the speed of RNNLM-based network searches during decoding, we apply the General Purpose Graphic Processing Units (GPGPUs). This paper proposes a novel method of applying GPGPUs to RNNLM-based graph traversals. We have achieved our goal by reducing redundant computations on CPUs and amount of transfer between GPGPUs and CPUs. The proposed approach was evaluated on both WSJ corpus and in-house data. Experiments shows that the proposed approach achieves the real-time speed in various circumstances while maintaining the Word Error Rate (WER) to be relatively 10% lower than that of n-gram models.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-23T05:15:14Z</published>\n",
      "    <arxiv:comment>4 pages, 2 figures, Interspeech2015(Accepted)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Kyungmin Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chiyoun Park</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ilhwan Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Namhoon Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jaewon Lee</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.11888v1</id>\n",
      "    <title>SBAT: Video Captioning with Sparse Boundary-Aware Transformer</title>\n",
      "    <updated>2020-07-23T09:57:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.11888v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.11888v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we focus on the problem of applying the transformer structure to video captioning effectively. The vanilla transformer is proposed for uni-modal language generation task such as machine translation. However, video captioning is a multimodal learning problem, and the video features have much redundancy between different time steps. Based on these concerns, we propose a novel method called sparse boundary-aware transformer (SBAT) to reduce the redundancy in video representation. SBAT employs boundary-aware pooling operation for scores from multihead attention and selects diverse features from different scenarios. Also, SBAT includes a local correlation scheme to compensate for the local information loss brought by sparse operation. Based on SBAT, we further propose an aligned cross-modal encoding scheme to boost the multimodal interaction. Experimental results on two benchmark datasets show that SBAT outperforms the state-of-the-art methods under most of the metrics.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-23T09:57:25Z</published>\n",
      "    <arxiv:comment>Appearing at IJCAI 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Tao Jin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Siyu Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ming Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yingming Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhongfei Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.11893v2</id>\n",
      "    <title>Critically Examining the Claimed Value of Convolutions over User-Item Embedding Maps for Recommender Systems</title>\n",
      "    <updated>2020-08-05T18:53:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.11893v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.11893v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In recent years, algorithm research in the area of recommender systems has shifted from matrix factorization techniques and their latent factor models to neural approaches. However, given the proven power of latent factor models, some newer neural approaches incorporate them within more complex network architectures. One specific idea, recently put forward by several researchers, is to consider potential correlations between the latent factors, i.e., embeddings, by applying convolutions over the user-item interaction map. However, contrary to what is claimed in these articles, such interaction maps do not share the properties of images where Convolutional Neural Networks (CNNs) are particularly useful. In this work, we show through analytical considerations and empirical evaluations that the claimed gains reported in the literature cannot be attributed to the ability of CNNs to model embedding correlations, as argued in the original papers. Moreover, additional performance evaluations show that all of the examined recent CNN-based models are outperformed by existing non-neural machine learning techniques or traditional nearest-neighbor approaches. On a more general level, our work points to major methodological issues in recommender systems research.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-23T10:03:47Z</published>\n",
      "    <arxiv:comment>Source code available here: https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <arxiv:journal_ref>The 29th ACM International Conference on Information and Knowledge Management (CIKM '20), October 19--23, 2020, Virtual Event, Ireland</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Maurizio Ferrari Dacrema</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Federico Parroni</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paolo Cremonesi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dietmar Jannach</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3340531.3411901</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3340531.3411901\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12020v1</id>\n",
      "    <title>Few-shot Visual Reasoning with Meta-analogical Contrastive Learning</title>\n",
      "    <updated>2020-07-23T14:00:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12020v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12020v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>While humans can solve a visual puzzle that requires logical reasoning by observing only few samples, it would require training over large amount of data for state-of-the-art deep reasoning models to obtain similar performance on the same task. In this work, we propose to solve such a few-shot (or low-shot) visual reasoning problem, by resorting to analogical reasoning, which is a unique human ability to identify structural or relational similarity between two sets. Specifically, given training and test sets that contain the same type of visual reasoning problems, we extract the structural relationships between elements in both domains, and enforce them to be as similar as possible with analogical learning. We repeatedly apply this process with slightly modified queries of the same problem under the assumption that it does not affect the relationship between a training and a test sample. This allows to learn the relational similarity between the two samples in an effective manner even with a single pair of samples. We validate our method on RAVEN dataset, on which it outperforms state-of-the-art method, with larger gains when the training data is scarce. We further meta-learn our analogical contrastive learning model over the same tasks with diverse attributes, and show that it generalizes to the same visual reasoning problem with unseen attributes.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-23T14:00:34Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Youngsung Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jinwoo Shin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eunho Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sung Ju Hwang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12475v1</id>\n",
      "    <title>Predicting and Mapping of Soil Organic Carbon Using Machine Learning Algorithms in Northern Iran</title>\n",
      "    <updated>2020-07-12T08:23:24Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12475v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12475v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Estimation of the soil organic carbon content is of utmost importance in understanding the chemical, physical, and biological functions of the soil. This study proposes machine learning algorithms of support vector machines, artificial neural networks, regression tree, random forest, extreme gradient boosting, and conventional deep neural network for advancing prediction models of SOC. Models are trained with 1879 composite surface soil samples, and 105 auxiliary data as predictors. The genetic algorithm is used as a feature selection approach to identify effective variables. The results indicate that precipitation is the most important predictor driving 15 percent of SOC spatial variability followed by the normalized difference vegetation index, day temperature index of moderate resolution imaging spectroradiometer, multiresolution valley bottom flatness and land use, respectively. Based on 10 fold cross validation, the DNN model reported as a superior algorithm with the lowest prediction error and uncertainty. In terms of accuracy, DNN yielded a mean absolute error of 59 percent, a root mean squared error of 75 percent, a coefficient of determination of 0.65, and Lins concordance correlation coefficient of 0.83. The SOC content was the highest in udic soil moisture regime class with mean values of 4 percent, followed by the aquic and xeric classes, respectively. Soils in dense forestlands had the highest SOC contents, whereas soils of younger geological age and alluvial fans had lower SOC. The proposed DNN is a promising algorithm for handling large numbers of auxiliary data at a province scale, and due to its flexible structure and the ability to extract more information from the auxiliary data surrounding the sampled observations, it had high accuracy for the prediction of the SOC baseline map and minimal uncertainty.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-12T08:23:24Z</published>\n",
      "    <arxiv:comment>30pages, 9 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>Remote Sens. 2020, 12, 2234</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Mostafa Emadi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruhollah Taghizadeh-Mehrjardi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ali Cherati</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Majid Danesh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amir Mosavi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Thomas Scholten</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.3390/rs12142234</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.3390/rs12142234\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12528v1</id>\n",
      "    <title>Improved Slice-wise Tumour Detection in Brain MRIs by Computing Dissimilarities between Latent Representations</title>\n",
      "    <updated>2020-07-24T14:02:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12528v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12528v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Anomaly detection for Magnetic Resonance Images (MRIs) can be solved with unsupervised methods by learning the distribution of healthy images and identifying anomalies as outliers. In presence of an additional dataset of unlabelled data containing also anomalies, the task can be framed as a semi-supervised task with negative and unlabelled sample points. Recently, in Albu et al., 2020, we have proposed a slice-wise semi-supervised method for tumour detection based on the computation of a dissimilarity function in the latent space of a Variational AutoEncoder, trained on unlabelled data. The dissimilarity is computed between the encoding of the image and the encoding of its reconstruction obtained through a different autoencoder trained only on healthy images. In this paper we present novel and improved results for our method, obtained by training the Variational AutoEncoders on a subset of the HCP and BRATS-2018 datasets and testing on the remaining individuals. We show that by training the models on higher resolution images and by improving the quality of the reconstructions, we obtain results which are comparable with different baselines, which employ a single VAE trained on healthy individuals. As expected, the performance of our method increases with the size of the threshold used to determine the presence of an anomaly.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-24T14:02:09Z</published>\n",
      "    <arxiv:comment>In 2020 KDD Workshop on Applied Data Science for Healthcare, August 24, 2020, San Diego, CA, USA. ACM, New York, NY, USA, 4 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Alexandra-Ioana Albu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alina Enescu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luigi Malagò</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12620v1</id>\n",
      "    <title>A Novel Ensemble Deep Learning Model for Stock Prediction Based on Stock Prices and News</title>\n",
      "    <updated>2020-07-23T15:25:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12620v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12620v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In recent years, machine learning and deep learning have become popular methods for financial data analysis, including financial textual data, numerical data, and graphical data. This paper proposes to use sentiment analysis to extract useful information from multiple textual data sources and a blending ensemble deep learning model to predict future stock movement. The blending ensemble model contains two levels. The first level contains two Recurrent Neural Networks (RNNs), one Long-Short Term Memory network (LSTM) and one Gated Recurrent Units network (GRU), followed by a fully connected neural network as the second level model. The RNNs, LSTM, and GRU models can effectively capture the time-series events in the input data, and the fully connected neural network is used to ensemble several individual prediction results to further improve the prediction accuracy. The purpose of this work is to explain our design philosophy and show that ensemble deep learning technologies can truly predict future stock price trends more effectively and can better assist investors in making the right investment decision than other traditional methods.</summary>\n",
      "    <category term=\"q-fin.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-23T15:25:37Z</published>\n",
      "    <arxiv:comment>15 pages, 11 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"q-fin.ST\"/>\n",
      "    <author>\n",
      "      <name>Yang Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yi Pan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12640v1</id>\n",
      "    <title>Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning on Graphs</title>\n",
      "    <updated>2020-07-24T16:50:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12640v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12640v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We consider an autonomous exploration problem in which a range-sensing mobile robot is tasked with accurately mapping the landmarks in an a priori unknown environment efficiently in real-time; it must choose sensing actions that both curb localization uncertainty and achieve information gain. For this problem, belief space planning methods that forward-simulate robot sensing and estimation may often fail in real-time implementation, scaling poorly with increasing size of the state, belief and action spaces. We propose a novel approach that uses graph neural networks (GNNs) in conjunction with deep reinforcement learning (DRL), enabling decision-making over graphs containing exploration information to predict a robot's optimal sensing action in belief space. The policy, which is trained in different random environments without human intervention, offers a real-time, scalable decision-making process whose high-performance exploratory sensing actions yield accurate maps and high rates of information gain.</summary>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-24T16:50:41Z</published>\n",
      "    <arxiv:primary_category term=\"cs.RO\"/>\n",
      "    <author>\n",
      "      <name>Fanfei Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>John D. Martin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yewei Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jinkun Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Brendan Englot</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12355v2</id>\n",
      "    <title>Dynamic Knowledge Distillation for Black-box Hypothesis Transfer Learning</title>\n",
      "    <updated>2020-08-07T00:47:11Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12355v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12355v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In real world applications like healthcare, it is usually difficult to build a machine learning prediction model that works universally well across different institutions. At the same time, the available model is often proprietary, i.e., neither the model parameter nor the data set used for model training is accessible. In consequence, leveraging the knowledge hidden in the available model (aka. the hypothesis) and adapting it to a local data set becomes extremely challenging. Motivated by this situation, in this paper we aim to address such a specific case within the hypothesis transfer learning framework, in which 1) the source hypothesis is a black-box model and 2) the source domain data is unavailable. In particular, we introduce a novel algorithm called dynamic knowledge distillation for hypothesis transfer learning (dkdHTL). In this method, we use knowledge distillation with instance-wise weighting mechanism to adaptively transfer the \"dark\" knowledge from the source hypothesis to the target domain.The weighting coefficients of the distillation loss and the standard loss are determined by the consistency between the predicted probability of the source hypothesis and the target ground-truth label.Empirical results on both transfer learning benchmark datasets and a healthcare dataset demonstrate the effectiveness of our method.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-24T05:19:08Z</published>\n",
      "    <arxiv:comment>7 pages, 2 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yiqin Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xu Min</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shiwan Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jing Mei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fei Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dongsheng Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kenney Ng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shaochun Li</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12420v2</id>\n",
      "    <title>Multinomial Sampling for Hierarchical Change-Point Detection</title>\n",
      "    <updated>2020-11-03T17:45:26Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12420v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12420v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Bayesian change-point detection, together with latent variable models, allows to perform segmentation over high-dimensional time-series. We assume that change-points lie on a lower-dimensional manifold where we aim to infer subsets of discrete latent variables. For this model, full inference is computationally unfeasible and pseudo-observations based on point-estimates are used instead. However, if estimation is not certain enough, change-point detection gets affected. To circumvent this problem, we propose a multinomial sampling methodology that improves the detection rate and reduces the delay while keeping complexity stable and inference analytically tractable. Our experiments show results that outperform the baseline method and we also provide an example oriented to a human behavior study.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-24T09:18:17Z</published>\n",
      "    <arxiv:comment>International Workshop on Machine Learning for Signal Processing (MLSP), 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Lorena Romero-Medrano</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pablo Moreno-Muñoz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Antonio Artés-Rodríguez</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12250v1</id>\n",
      "    <title>WiFi-based Crowd Monitoring and Workspace Planning for COVID-19 Recovery</title>\n",
      "    <updated>2020-07-23T20:45:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12250v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12250v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The recovery phase of the COVID-19 pandemic requires careful planning and monitoring while people gradually return to work. Internet-of-Things (IoT) is widely regarded as a crucial tool to help combating COVID-19 pandemic in many areas and societies. In particular, the heterogeneous data captured by IoT solutions can inform policy making and quick responses to community events. This article introduces a novel IoT crowd monitoring solution which uses software defined networks (SDN) assisted WiFi access points as 24/7 sensors to monitor and analyze the use of physical space. Prototypes and crowd behavior models are developed using over 500 million records captured on a university campus. Besides supporting informed decision at institution level, the results can be used by individual visitors to plan or schedule their access to facilities.</summary>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-23T20:45:44Z</published>\n",
      "    <arxiv:primary_category term=\"cs.SI\"/>\n",
      "    <author>\n",
      "      <name>Mu Mu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12076v1</id>\n",
      "    <title>HCMS at SemEval-2020 Task 9: A Neural Approach to Sentiment Analysis for Code-Mixed Texts</title>\n",
      "    <updated>2020-07-23T15:39:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12076v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12076v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Problems involving code-mixed language are often plagued by a lack of resources and an absence of materials to perform sophisticated transfer learning with. In this paper we describe our submission to the Sentimix Hindi-English task involving sentiment classification of code-mixed texts, and with an F1 score of 67.1%, we demonstrate that simple convolution and attention may well produce reasonable results.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-23T15:39:53Z</published>\n",
      "    <arxiv:comment>6 pages, 2 figures, 4 tables, math equations, to be published in the proceedings of the 14th International Workshop on Semantic Evaluation (SemEval) 2020, Association for Computational Linguistics (ACL). Code for the paper is available at https://github.com/IamAdiSri/hcms-semeval20 . Data and task description is available at https://competitions.codalab.org/competitions/20654</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Aditya Srivastava</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>V. Harsha Vardhan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06330v1</id>\n",
      "    <title>Automated detection and quantification of COVID-19 airspace disease on chest radiographs: A novel approach achieving radiologist-level performance using a CNN trained on digital reconstructed radiographs (DRRs) from CT-based ground-truth</title>\n",
      "    <updated>2020-08-13T14:33:03Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06330v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06330v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Purpose: To leverage volumetric quantification of airspace disease (AD) derived from a superior modality (CT) serving as ground truth, projected onto digitally reconstructed radiographs (DRRs) to: 1) train a convolutional neural network to quantify airspace disease on paired CXRs; and 2) compare the DRR-trained CNN to expert human readers in the CXR evaluation of patients with confirmed COVID-19.\n",
      "  Materials and Methods: We retrospectively selected a cohort of 86 COVID-19 patients (with positive RT-PCR), from March-May 2020 at a tertiary hospital in the northeastern USA, who underwent chest CT and CXR within 48 hrs. The ground truth volumetric percentage of COVID-19 related AD (POv) was established by manual AD segmentation on CT. The resulting 3D masks were projected into 2D anterior-posterior digitally reconstructed radiographs (DRR) to compute area-based AD percentage (POa). A convolutional neural network (CNN) was trained with DRR images generated from a larger-scale CT dataset of COVID-19 and non-COVID-19 patients, automatically segmenting lungs, AD and quantifying POa on CXR. CNN POa results were compared to POa quantified on CXR by two expert readers and to the POv ground-truth, by computing correlations and mean absolute errors.\n",
      "  Results: Bootstrap mean absolute error (MAE) and correlations between POa and POv were 11.98% [11.05%-12.47%] and 0.77 [0.70-0.82] for average of expert readers, and 9.56%-9.78% [8.83%-10.22%] and 0.78-0.81 [0.73-0.85] for the CNN, respectively.\n",
      "  Conclusion: Our CNN trained with DRR using CT-derived airspace quantification achieved expert radiologist level of accuracy in the quantification of airspace disease on CXR, in patients with positive RT-PCR for COVID-19.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-13T14:33:03Z</published>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Eduardo Mortani Barbosa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Warren B. Gefter</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rochelle Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Florin C. Ghesu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Siqi Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Boris Mailhe</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Awais Mansoor</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sasa Grbic</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sebastian Piat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Guillaume Chabin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vishwanath R S.</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abishek Balachandran</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sebastian Vogt</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Valentin Ziebandt</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Steffen Kappler</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dorin Comaniciu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06395v3</id>\n",
      "    <title>Supervised Topological Maps</title>\n",
      "    <updated>2020-09-02T09:30:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06395v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06395v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Controlling the internal representation space of a neural network is a desirable feature because it allows to generate new data in a supervised manner. In this paper we will show how this can be achieved while building a low-dimensional mapping of the input stream, by deriving a generalized algorithm starting from Self Organizing Maps (SOMs). SOMs are a kind of neural network which can be trained with unsupervised learning to produce a low-dimensional discretized mapping of the input space. They can be used for the generation of new data through backward propagation of interpolations made from the mapping grid. Unfortunately the final topology of the mapping space of a SOM is not known before learning, so interpolating new data in a supervised way is not an easy task. Here we will show a variation from the SOM algorithm consisting in constraining the update of prototypes so that it is also a function of the distance of its prototypes from extrinsically given targets in the mapping space. We will demonstrate how such variants, that we will call Supervised Topological Maps (STMs), allow for a supervised mapping where the position of internal representations in the mapping space is determined by the experimenter. Controlling the internal representation space in STMs reveals to be an easier task than what is currently done using other algorithms such as variational or adversarial autoencoders.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-14T14:30:16Z</published>\n",
      "    <arxiv:comment>11 pages, 8 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Francesco Mannella</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.05503v1</id>\n",
      "    <title>Multi-level Stress Assessment Using Multi-domain Fusion of ECG Signal</title>\n",
      "    <updated>2020-08-12T18:08:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.05503v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.05503v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Stress analysis and assessment of affective states of mind using ECG as a physiological signal is a burning research topic in biomedical signal processing. However, existing literature provides only binary assessment of stress, while multiple levels of assessment may be more beneficial for healthcare applications. Furthermore, in present research, ECG signal for stress analysis is examined independently in spatial domain or in transform domains but the advantage of fusing these domains has not been fully utilized. To get the maximum advantage of fusing diferent domains, we introduce a dataset with multiple stress levels and then classify these levels using a novel deep learning approach by converting ECG signal into signal images based on R-R peaks without any feature extraction. Moreover, We made signal images multimodal and multidomain by converting them into time-frequency and frequency domain using Gabor wavelet transform (GWT) and Discrete Fourier Transform (DFT) respectively. Convolutional Neural networks (CNNs) are used to extract features from different modalities and then decision level fusion is performed for improving the classification accuracy. The experimental results on an in-house dataset collected with 15 users show that with proposed fusion framework and using ECG signal to image conversion, we reach an average accuracy of 85.45%.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-12T18:08:35Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Zeeshan Ahmad</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Naimul Khan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.05515v1</id>\n",
      "    <title>Synergy between Machine/Deep Learning and Software Engineering: How Far Are We?</title>\n",
      "    <updated>2020-08-12T18:19:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.05515v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.05515v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Since 2009, the deep learning revolution, which was triggered by the introduction of ImageNet, has stimulated the synergy between Machine Learning (ML)/Deep Learning (DL) and Software Engineering (SE). Meanwhile, critical reviews have emerged that suggest that ML/DL should be used cautiously. To improve the quality (especially the applicability and generalizability) of ML/DL-related SE studies, and to stimulate and enhance future collaborations between SE/AI researchers and industry practitioners, we conducted a 10-year Systematic Literature Review (SLR) on 906 ML/DL-related SE papers published between 2009 and 2018. Our trend analysis demonstrated the mutual impacts that ML/DL and SE have had on each other. At the same time, however, we also observed a paucity of replicable and reproducible ML/DL-related SE studies and identified five factors that influence their replicability and reproducibility. To improve the applicability and generalizability of research results, we analyzed what ingredients in a study would facilitate an understanding of why a ML/DL technique was selected for a specific SE problem. In addition, we identified the unique trends of impacts of DL models on SE tasks, as well as five unique challenges that needed to be met in order to better leverage DL to improve the productivity of SE tasks. Finally, we outlined a road-map that we believe can facilitate the transfer of ML/DL-based SE research results into real-world industry practices.</summary>\n",
      "    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-12T18:19:30Z</published>\n",
      "    <arxiv:primary_category term=\"cs.SE\"/>\n",
      "    <author>\n",
      "      <name>Simin Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liguo Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jidong Ge</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tengfei Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Haitao Feng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ming Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>He Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vincent Ng</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.05566v1</id>\n",
      "    <title>An Efficient Confidence Measure-Based Evaluation Metric for Breast Cancer Screening Using Bayesian Neural Networks</title>\n",
      "    <updated>2020-08-12T20:34:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.05566v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.05566v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Screening mammograms is the gold standard for detecting breast cancer early. While a good amount of work has been performed on mammography image classification, especially with deep neural networks, there has not been much exploration into the confidence or uncertainty measurement of the classification. In this paper, we propose a confidence measure-based evaluation metric for breast cancer screening. We propose a modular network architecture, where a traditional neural network is used as a feature extractor with transfer learning, followed by a simple Bayesian neural network. Utilizing a two-stage approach helps reducing the computational complexity, making the proposed framework attractive for wider deployment. We show that by providing the medical practitioners with a tool to tune two hyperparameters of the Bayesian neural network, namely, fraction of sampled number of networks and minimum probability, the framework can be adapted as needed by the domain expert. Finally, we argue that instead of just a single number such as accuracy, a tuple (accuracy, coverage, sampled number of networks, and minimum probability) can be utilized as an evaluation metric of our framework. We provide experimental results on the CBIS-DDSM dataset, where we show the trends in accuracy-coverage tradeoff while tuning the two hyperparameters. We also show that our confidence tuning results in increased accuracy with a reduced set of images with high confidence when compared to the baseline transfer learning. To make the proposed framework readily deployable, we provide (anonymized) source code with reproducible results at https://git.io/JvRqE.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-12T20:34:14Z</published>\n",
      "    <arxiv:comment>To be presented at the IEEE ICHI 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Anika Tabassum</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Naimul Khan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.05567v2</id>\n",
      "    <title>Procedural Urban Forestry</title>\n",
      "    <updated>2020-08-14T00:35:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.05567v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.05567v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The placement of vegetation plays a central role in the realism of virtual scenes. We introduce procedural placement models (PPMs) for vegetation in urban layouts. PPMs are environmentally sensitive to city geometry and allow identifying plausible plant positions based on structural and functional zones in an urban layout. PPMs can either be directly used by defining their parameters or can be learned from satellite images and land register data. Together with approaches for generating buildings and trees, this allows us to populate urban landscapes with complex 3D vegetation. The effectiveness of our framework is shown through examples of large-scale city scenes and close-ups of individually grown tree models; we also validate it by a perceptual user study.</summary>\n",
      "    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-12T20:44:56Z</published>\n",
      "    <arxiv:comment>14 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.GR\"/>\n",
      "    <author>\n",
      "      <name>Till Niese</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sören Pirk</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matthias Albrecht</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bedrich Benes</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Oliver Deussen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.05644v1</id>\n",
      "    <title>A Deep Learning Approach for COVID-19 Trend Prediction</title>\n",
      "    <updated>2020-08-09T16:47:45Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.05644v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.05644v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this work, we developed a deep learning model-based approach to forecast the spreading trend of SARS-CoV-2 in the United States. We implemented the designed model using the United States to confirm cases and state demographic data and achieved promising trend prediction results. The model incorporates demographic information and epidemic time-series data through a Gated Recurrent Unit structure. The identification of dominating demographic factors is delivered in the end.</summary>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.PE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-09T16:47:45Z</published>\n",
      "    <arxiv:comment>7 pages, 11 figures, accepted by KDD 2020 epiDAMIK workshop</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CY\"/>\n",
      "    <author>\n",
      "      <name>Tong Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Long Sha</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Justin Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pengyu Hong</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.05157v1</id>\n",
      "    <title>Towards Geometry Guided Neural Relighting with Flash Photography</title>\n",
      "    <updated>2020-08-12T08:03:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.05157v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.05157v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Previous image based relighting methods require capturing multiple images to acquire high frequency lighting effect under different lighting conditions, which needs nontrivial effort and may be unrealistic in certain practical use scenarios. While such approaches rely entirely on cleverly sampling the color images under different lighting conditions, little has been done to utilize geometric information that crucially influences the high-frequency features in the images, such as glossy highlight and cast shadow. We therefore propose a framework for image relighting from a single flash photograph with its corresponding depth map using deep learning. By incorporating the depth map, our approach is able to extrapolate realistic high-frequency effects under novel lighting via geometry guided image decomposition from the flashlight image, and predict the cast shadow map from the shadow-encoding transformed depth map. Moreover, the single-image based setup greatly simplifies the data capture process. We experimentally validate the advantage of our geometry guided approach over state-of-the-art image-based approaches in intrinsic image decomposition and image relighting, and also demonstrate our performance on real mobile phone photo examples.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-12T08:03:28Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Di Qiu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jin Zeng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhanghan Ke</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wenxiu Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chengxi Yang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.05201v2</id>\n",
      "    <title>OCoR: An Overlapping-Aware Code Retriever</title>\n",
      "    <updated>2020-08-20T12:05:03Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.05201v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.05201v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Code retrieval helps developers reuse the code snippet in the open-source projects. Given a natural language description, code retrieval aims to search for the most relevant code among a set of code. Existing state-of-the-art approaches apply neural networks to code retrieval. However, these approaches still fail to capture an important feature: overlaps. The overlaps between different names used by different people indicate that two different names may be potentially related (e.g., \"message\" and \"msg\"), and the overlaps between identifiers in code and words in natural language descriptions indicate that the code snippet and the description may potentially be related. To address these problems, we propose a novel neural architecture named OCoR, where we introduce two specifically-designed components to capture overlaps: the first embeds identifiers by character to capture the overlaps between identifiers, and the second introduces a novel overlap matrix to represent the degrees of overlaps between each natural language word and each identifier.\n",
      "  The evaluation was conducted on two established datasets. The experimental results show that OCoR significantly outperforms the existing state-of-the-art approaches and achieves 13.1% to 22.3% improvements. Moreover, we also conducted several in-depth experiments to help understand the performance of different components in OCoR.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-12T09:43:35Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <arxiv:journal_ref>ASE 2020: 35th IEEE/ACM International Conference on Automated Software Engineering Proceedings</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Qihao Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zeyu Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiran Liang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yingfei Xiong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lu Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.05892v2</id>\n",
      "    <title>LGNN: A Context-aware Line Segment Detector</title>\n",
      "    <updated>2020-08-29T03:43:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.05892v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.05892v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present a novel real-time line segment detection scheme called Line Graph Neural Network (LGNN). Existing approaches require a computationally expensive verification or postprocessing step. Our LGNN employs a deep convolutional neural network (DCNN) for proposing line segment directly, with a graph neural network (GNN) module for reasoning their connectivities. Specifically, LGNN exploits a new quadruplet representation for each line segment where the GNN module takes the predicted candidates as vertexes and constructs a sparse graph to enforce structural context. Compared with the state-of-the-art, LGNN achieves near real-time performance without compromising accuracy. LGNN further enables time-sensitive 3D applications. When a 3D point cloud is accessible, we present a multi-modal line segment classification technique for extracting a 3D wireframe of the environment robustly and efficiently.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-13T13:23:18Z</published>\n",
      "    <arxiv:comment>9 pages, 7 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Quan Meng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiakai Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qiang Hu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xuming He</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jingyi Yu</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3394171.3413784</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3394171.3413784\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.05932v3</id>\n",
      "    <title>Kullback-Leibler divergence between quantum distributions, and its upper-bound</title>\n",
      "    <updated>2020-12-10T12:39:33Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.05932v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.05932v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This work presents an upper-bound to value that the Kullback-Leibler (KL) divergence can reach for a class of probability distributions called quantum distributions (QD). The aim is to find a distribution $U$ which maximizes the KL divergence from a given distribution $P$ under the assumption that $P$ and $U$ have been generated by distributing a given discrete quantity, a quantum. Quantum distributions naturally represent a wide range of probability distributions that are used in practical applications. Moreover, such a class of distributions can be obtained as an approximation of any probability distribution. The retrieving of an upper-bound for the entropic divergence is here shown to be possible under the condition that the compared distributions are quantum distributions over the same quantum value, thus they become comparable. Thus, entropic divergence acquires a more powerful meaning when it is applied to comparable distributions. This aspect should be taken into account in future developments of divergences. The theoretical findings are used for proposing a notion of normalized KL divergence that is empirically shown to behave differently from already known measures.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-13T14:42:13Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Vincenzo Bonnici</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06020v1</id>\n",
      "    <title>Testing the Safety of Self-driving Vehicles by Simulating Perception and Prediction</title>\n",
      "    <updated>2020-08-13T17:20:02Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06020v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06020v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present a novel method for testing the safety of self-driving vehicles in simulation. We propose an alternative to sensor simulation, as sensor simulation is expensive and has large domain gaps. Instead, we directly simulate the outputs of the self-driving vehicle's perception and prediction system, enabling realistic motion planning testing. Specifically, we use paired data in the form of ground truth labels and real perception and prediction outputs to train a model that predicts what the online system will produce. Importantly, the inputs to our system consists of high definition maps, bounding boxes, and trajectories, which can be easily sketched by a test engineer in a matter of minutes. This makes our approach a much more scalable solution. Quantitative results on two large-scale datasets demonstrate that we can realistically test motion planning using our simulations.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-13T17:20:02Z</published>\n",
      "    <arxiv:comment>ECCV 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Kelvin Wong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qiang Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ming Liang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bin Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Renjie Liao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abbas Sadat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Raquel Urtasun</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.05650v1</id>\n",
      "    <title>MLNET: An Adaptive Multiple Receptive-field Attention Neural Network for Voice Activity Detection</title>\n",
      "    <updated>2020-08-13T02:24:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.05650v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.05650v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Voice activity detection (VAD) makes a distinction between speech and non-speech and its performance is of crucial importance for speech based services. Recently, deep neural network (DNN)-based VADs have achieved better performance than conventional signal processing methods. The existed DNNbased models always handcrafted a fixed window to make use of the contextual speech information to improve the performance of VAD. However, the fixed window of contextual speech information can't handle various unpredicatable noise environments and highlight the critical speech information to VAD task. In order to solve this problem, this paper proposed an adaptive multiple receptive-field attention neural network, called MLNET, to finish VAD task. The MLNET leveraged multi-branches to extract multiple contextual speech information and investigated an effective attention block to weight the most crucial parts of the context for final classification. Experiments in real-world scenarios demonstrated that the proposed MLNET-based model outperformed other baselines.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-13T02:24:28Z</published>\n",
      "    <arxiv:comment>will be presented in INTERSPEECH 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Zhenpeng Zheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jianzong Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ning Cheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jian Luo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jing Xiao</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.05756v1</id>\n",
      "    <title>Metrics for Multi-Class Classification: an Overview</title>\n",
      "    <updated>2020-08-13T08:41:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.05756v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.05756v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Classification tasks in machine learning involving more than two classes are known by the name of \"multi-class classification\". Performance indicators are very useful when the aim is to evaluate and compare different classification models or machine learning techniques. Many metrics come in handy to test the ability of a multi-class classifier. Those metrics turn out to be useful at different stage of the development process, e.g. comparing the performance of two different models or analysing the behaviour of the same model by tuning different parameters. In this white paper we review a list of the most promising multi-class metrics, we highlight their advantages and disadvantages and show their possible usages during the development of a classification model.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-13T08:41:44Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Margherita Grandini</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Enrico Bagli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Giorgio Visani</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.05789v1</id>\n",
      "    <title>Look, Listen, and Attend: Co-Attention Network for Self-Supervised Audio-Visual Representation Learning</title>\n",
      "    <updated>2020-08-13T10:08:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.05789v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.05789v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>When watching videos, the occurrence of a visual event is often accompanied by an audio event, e.g., the voice of lip motion, the music of playing instruments. There is an underlying correlation between audio and visual events, which can be utilized as free supervised information to train a neural network by solving the pretext task of audio-visual synchronization. In this paper, we propose a novel self-supervised framework with co-attention mechanism to learn generic cross-modal representations from unlabelled videos in the wild, and further benefit downstream tasks. Specifically, we explore three different co-attention modules to focus on discriminative visual regions correlated to the sounds and introduce the interactions between them. Experiments show that our model achieves state-of-the-art performance on the pretext task while having fewer parameters compared with existing methods. To further evaluate the generalizability and transferability of our approach, we apply the pre-trained model on two downstream tasks, i.e., sound source localization and action recognition. Extensive experiments demonstrate that our model provides competitive results with other self-supervised methods, and also indicate that our approach can tackle the challenging scenes which contain multiple sound sources.</summary>\n",
      "    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-13T10:08:12Z</published>\n",
      "    <arxiv:comment>Accepted by the 28th ACM International Conference on Multimedia (ACM MM 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.MM\"/>\n",
      "    <author>\n",
      "      <name>Ying Cheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruize Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhihao Pan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rui Feng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuejie Zhang</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3394171.3413869</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3394171.3413869\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.05808v1</id>\n",
      "    <title>Small Towers Make Big Differences</title>\n",
      "    <updated>2020-08-13T10:45:31Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.05808v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.05808v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Multi-task learning aims at solving multiple machine learning tasks at the same time. A good solution to a multi-task learning problem should be generalizable in addition to being Pareto optimal. In this paper, we provide some insights on understanding the trade-off between Pareto efficiency and generalization as a result of parameterization in multi-task deep learning models. As a multi-objective optimization problem, enough parameterization is needed for handling task conflicts in a constrained solution space; however, from a multi-task generalization perspective, over-parameterization undermines the benefit of learning a shared representation which helps harder tasks or tasks with limited training examples. A delicate balance between multi-task generalization and multi-objective optimization is therefore needed for finding a better trade-off between efficiency and generalization. To this end, we propose a method of under-parameterized self-auxiliaries for multi-task models to achieve the best of both worlds. It is task-agnostic and works with other multi-task learning algorithms. Empirical results show that small towers of under-parameterized self-auxiliaries can make big differences in improving Pareto efficiency in various multi-task applications.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-13T10:45:31Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yuyan Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhe Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bo Dai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christopher Fifty</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dong Lin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lichan Hong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ed H. Chi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06199v2</id>\n",
      "    <title>Adversary Agnostic Robust Deep Reinforcement Learning</title>\n",
      "    <updated>2020-12-24T06:38:19Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06199v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06199v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep reinforcement learning (DRL) policies have been shown to be deceived by perturbations (e.g., random noise or intensional adversarial attacks) on state observations that appear at test time but are unknown during training. To increase the robustness of DRL policies, previous approaches assume that the knowledge of adversaries can be added into the training process to achieve the corresponding generalization ability on these perturbed observations. However, such an assumption not only makes the robustness improvement more expensive but may also leave a model less effective to other kinds of attacks in the wild. In contrast, we propose an adversary agnostic robust DRL paradigm that does not require learning from adversaries. To this end, we first theoretically derive that robustness could indeed be achieved independently of the adversaries based on a policy distillation setting. Motivated by this finding, we propose a new policy distillation loss with two terms: 1) a prescription gap maximization loss aiming at simultaneously maximizing the likelihood of the action selected by the teacher policy and the entropy over the remaining actions; 2) a corresponding Jacobian regularization loss that minimizes the magnitude of the gradient with respect to the input state. The theoretical analysis shows that our distillation loss guarantees to increase the prescription gap and the adversarial robustness. Furthermore, experiments on five Atari games firmly verify the superiority of our approach in terms of boosting adversarial robustness compared to other state-of-the-art methods.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-14T06:04:15Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xinghua Qu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yew-Soon Ong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abhishek Gupta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhu Sun</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06218v1</id>\n",
      "    <title>Which Strategies Matter for Noisy Label Classification? Insight into Loss and Uncertainty</title>\n",
      "    <updated>2020-08-14T07:34:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06218v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06218v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Label noise is a critical factor that degrades the generalization performance of deep neural networks, thus leading to severe issues in real-world problems. Existing studies have employed strategies based on either loss or uncertainty to address noisy labels, and ironically some strategies contradict each other: emphasizing or discarding uncertain samples or concentrating on high or low loss samples. To elucidate how opposing strategies can enhance model performance and offer insights into training with noisy labels, we present analytical results on how loss and uncertainty values of samples change throughout the training process. From the in-depth analysis, we design a new robust training method that emphasizes clean and informative samples, while minimizing the influence of noise using both loss and uncertainty. We demonstrate the effectiveness of our method with extensive experiments on synthetic and real-world datasets for various deep learning models. The results show that our method significantly outperforms other state-of-the-art methods and can be used generally regardless of neural network architectures.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-14T07:34:32Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Wonyoung Shin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jung-Woo Ha</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shengzhe Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yongwoo Cho</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hoyean Song</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sunyoung Kwon</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06220v1</id>\n",
      "    <title>Kernel Methods for Cooperative Multi-Agent Contextual Bandits</title>\n",
      "    <updated>2020-08-14T07:37:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06220v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06220v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Cooperative multi-agent decision making involves a group of agents cooperatively solving learning problems while communicating over a network with delays. In this paper, we consider the kernelised contextual bandit problem, where the reward obtained by an agent is an arbitrary linear function of the contexts' images in the related reproducing kernel Hilbert space (RKHS), and a group of agents must cooperate to collectively solve their unique decision problems. For this problem, we propose \\textsc{Coop-KernelUCB}, an algorithm that provides near-optimal bounds on the per-agent regret, and is both computationally and communicatively efficient. For special cases of the cooperative problem, we also provide variants of \\textsc{Coop-KernelUCB} that provides optimal per-agent regret. In addition, our algorithm generalizes several existing results in the multi-agent bandit setting. Finally, on a series of both synthetic and real-world multi-agent network benchmarks, we demonstrate that our algorithm significantly outperforms existing benchmarks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-14T07:37:44Z</published>\n",
      "    <arxiv:comment>19 pages including supplement, camera-ready at ICML 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Abhimanyu Dubey</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alex Pentland</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06239v2</id>\n",
      "    <title>Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems</title>\n",
      "    <updated>2020-08-20T10:56:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06239v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06239v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Task-oriented dialogue systems use four connected modules, namely, Natural Language Understanding (NLU), a Dialogue State Tracking (DST), Dialogue Policy (DP) and Natural Language Generation (NLG). A research challenge is to learn each module with the least amount of samples (i.e., few-shots) given the high cost related to the data collection. The most common and effective technique to solve this problem is transfer learning, where large language models, either pre-trained on text or task-specific data, are fine-tuned on the few samples. These methods require fine-tuning steps and a set of parameters for each task. Differently, language models, such as GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020), allow few-shot learning by priming the model with few examples. In this paper, we evaluate the priming few-shot ability of language models in the NLU, DST, DP and NLG tasks. Importantly, we highlight the current limitations of this approach, and we discuss the possible implication for future work.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-14T08:23:21Z</published>\n",
      "    <arxiv:comment>Blog (https://andreamad8.github.io/few-shot-gpt/), Medium (https://medium.com/@madottoandrea/language-model-as-few-shot-learner-for-task-oriented-dialogue-systems-db4765796744) and Code (https://github.com/andreamad8/TASK-ORIENTED-LM-FEWSHOT)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Andrea Madotto</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zihan Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhaojiang Lin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pascale Fung</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06244v1</id>\n",
      "    <title>Cooperative Multi-Agent Bandits with Heavy Tails</title>\n",
      "    <updated>2020-08-14T08:34:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06244v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06244v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study the heavy-tailed stochastic bandit problem in the cooperative multi-agent setting, where a group of agents interact with a common bandit problem, while communicating on a network with delays. Existing algorithms for the stochastic bandit in this setting utilize confidence intervals arising from an averaging-based communication protocol known as~\\textit{running consensus}, that does not lend itself to robust estimation for heavy-tailed settings. We propose \\textsc{MP-UCB}, a decentralized multi-agent algorithm for the cooperative stochastic bandit that incorporates robust estimation with a message-passing protocol. We prove optimal regret bounds for \\textsc{MP-UCB} for several problem settings, and also demonstrate its superiority to existing methods. Furthermore, we establish the first lower bounds for the cooperative bandit problem, in addition to providing efficient algorithms for robust bandit estimation of location.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-14T08:34:32Z</published>\n",
      "    <arxiv:comment>26 pages including appendix, camera-ready for ICML 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Abhimanyu Dubey</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alex Pentland</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06246v1</id>\n",
      "    <title>Graph Polish: A Novel Graph Generation Paradigm for Molecular Optimization</title>\n",
      "    <updated>2020-08-14T08:36:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06246v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06246v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Molecular optimization, which transforms a given input molecule X into another Y with desirable properties, is essential in molecular drug discovery. The traditional translating approaches, generating the molecular graphs from scratch by adding some substructures piece by piece, prone to error because of the large set of candidate substructures in a large number of steps to the final target. In this study, we present a novel molecular optimization paradigm, Graph Polish, which changes molecular optimization from the traditional \"two-language translating\" task into a \"single-language polishing\" task. The key to this optimization paradigm is to find an optimization center subject to the conditions that the preserved areas around it ought to be maximized and thereafter the removed and added regions should be minimized. We then propose an effective and efficient learning framework T&amp;S polish to capture the long-term dependencies in the optimization steps. The T component automatically identifies and annotates the optimization centers and the preservation, removal and addition of some parts of the molecule, and the S component learns these behaviors and applies these actions to a new molecule. Furthermore, the proposed paradigm can offer an intuitive interpretation for each molecular optimization result. Experiments with multiple optimization tasks are conducted on four benchmark datasets. The proposed T&amp;S polish approach achieves significant advantage over the five state-of-the-art baseline methods on all the tasks. In addition, extensive studies are conducted to validate the effectiveness, explainability and time saving of the novel optimization paradigm.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-14T08:36:13Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Chaojie Ji</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yijia Zheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruxin Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yunpeng Cai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hongyan Wu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06072v1</id>\n",
      "    <title>MIXCAPS: A Capsule Network-based Mixture of Experts for Lung Nodule Malignancy Prediction</title>\n",
      "    <updated>2020-08-13T18:16:07Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06072v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06072v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Lung diseases including infections such as Pneumonia, Tuberculosis, and novel Coronavirus (COVID-19), together with Lung Cancer are significantly widespread and are, typically, considered life threatening. In particular, lung cancer is among the most common and deadliest cancers with a low 5-year survival rate. Timely diagnosis of lung cancer is, therefore, of paramount importance as it can save countless lives. In this regard, deep learning radiomics solutions have the promise of extracting the most useful features on their own in an end-to-end fashion without having access to the annotated boundaries. Among different deep learning models, Capsule Networks are proposed to overcome shortcomings of the Convolutional Neural Networks (CNN) such as their inability to recognize detailed spatial relations. Capsule networks have so far shown satisfying performance in medical imaging problems. Capitalizing on their success, in this study, we propose a novel capsule network-based mixture of experts, referred to as the MIXCAPS. The proposed MIXCAPS architecture takes advantage of not only the capsule network's capabilities to handle small datasets, but also automatically splitting dataset through a convolutional gating network. MIXCAPS enables capsule network experts to specialize on different subsets of the data. Our results show that MIXCAPS outperforms a single capsule network and a mixture of CNNs, with an accuracy of 92.88%, sensitivity of 93.2%, specificity of 92.3% and area under the curve of 0.963. Our experiments also show that there is a relation between the gate outputs and a couple of hand-crafted features, illustrating explainable nature of the proposed MIXCAPS. To further evaluate generalization capabilities of the proposed MIXCAPS architecture, additional experiments on a brain tumor dataset are performed showing potentials of MIXCAPS for detection of tumors related to other organs.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-13T18:16:07Z</published>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Parnian Afshar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Farnoosh Naderkhani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anastasia Oikonomou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Moezedin Javad Rafiee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arash Mohammadi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Konstantinos N. Plataniotis</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06073v1</id>\n",
      "    <title>Visuomotor Mechanical Search: Learning to Retrieve Target Objects in Clutter</title>\n",
      "    <updated>2020-08-13T18:23:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06073v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06073v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>When searching for objects in cluttered environments, it is often necessary to perform complex interactions in order to move occluding objects out of the way and fully reveal the object of interest and make it graspable. Due to the complexity of the physics involved and the lack of accurate models of the clutter, planning and controlling precise predefined interactions with accurate outcome is extremely hard, when not impossible. In problems where accurate (forward) models are lacking, Deep Reinforcement Learning (RL) has shown to be a viable solution to map observations (e.g. images) to good interactions in the form of close-loop visuomotor policies. However, Deep RL is sample inefficient and fails when applied directly to the problem of unoccluding objects based on images. In this work we present a novel Deep RL procedure that combines i) teacher-aided exploration, ii) a critic with privileged information, and iii) mid-level representations, resulting in sample efficient and effective learning for the problem of uncovering a target object occluded by a heap of unknown objects. Our experiments show that our approach trains faster and converges to more efficient uncovering solutions than baselines and ablations, and that our uncovering policies lead to an average improvement in the graspability of the target object, facilitating downstream retrieval applications.</summary>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-13T18:23:00Z</published>\n",
      "    <arxiv:primary_category term=\"cs.AI\"/>\n",
      "    <author>\n",
      "      <name>Andrey Kurenkov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joseph Taglic</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rohun Kulkarni</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marcus Dominguez-Kuhne</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Animesh Garg</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Roberto Martín-Martín</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Silvio Savarese</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.06141v1</id>\n",
      "    <title>Analytical bounds on the local Lipschitz constants of affine-ReLU functions</title>\n",
      "    <updated>2020-08-14T00:23:21Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.06141v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.06141v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we determine analytical bounds on the local Lipschitz constants of of affine functions composed with rectified linear units (ReLUs). Affine-ReLU functions represent a widely used layer in deep neural networks, due to the fact that convolution, fully-connected, and normalization functions are all affine, and are often followed by a ReLU activation function. Using an analytical approach, we mathematically determine upper bounds on the local Lipschitz constant of an affine-ReLU function, show how these bounds can be combined to determine a bound on an entire network, and discuss how the bounds can be efficiently computed, even for larger layers and networks. We show several examples by applying our results to AlexNet, as well as several smaller networks based on the MNIST and CIFAR-10 datasets. The results show that our method produces tighter bounds than the standard conservative bound (i.e. the product of the spectral norms of the layers' linear matrices), especially for small perturbations.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-14T00:23:21Z</published>\n",
      "    <arxiv:comment>14 pages, 5 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Trevor Avant</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kristi A. Morgansen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.04988v1</id>\n",
      "    <title>Asymptotic Convergence Rate of Alternating Minimization for Rank One Matrix Completion</title>\n",
      "    <updated>2020-08-11T19:56:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.04988v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.04988v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study alternating minimization for matrix completion in the simplest possible setting: completing a rank-one matrix from a revealed subset of the entries. We bound the asymptotic convergence rate by the variational characterization of eigenvalues of a reversible consensus problem. This leads to a polynomial upper bound on the asymptotic rate in terms of number of nodes as well as the largest degree of the graph of revealed entries.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-11T19:56:35Z</published>\n",
      "    <arxiv:comment>6 pages, 4 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Rui Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alex Olshevsky</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.08331v1</id>\n",
      "    <title>Probing Neural Dialog Models for Conversational Understanding</title>\n",
      "    <updated>2020-06-07T17:32:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.08331v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.08331v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The predominant approach to open-domain dialog generation relies on end-to-end training of neural models on chat datasets. However, this approach provides little insight as to what these models learn (or do not learn) about engaging in dialog. In this study, we analyze the internal representations learned by neural open-domain dialog systems and evaluate the quality of these representations for learning basic conversational skills. Our results suggest that standard open-domain dialog systems struggle with answering questions, inferring contradiction, and determining the topic of conversation, among other tasks. We also find that the dyadic, turn-taking nature of dialog is not fully leveraged by these models. By exploring these limitations, we highlight the need for additional research into architectures and training methods that can better capture high-level information about dialog.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-07T17:32:00Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Abdelrhman Saleh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tovly Deutsch</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stephen Casper</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yonatan Belinkov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stuart Shieber</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.18653/v1/2020.nlp4convai-1.15</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.18653/v1/2020.nlp4convai-1.15\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.08376v1</id>\n",
      "    <title>Generating Master Faces for Use in Performing Wolf Attacks on Face Recognition Systems</title>\n",
      "    <updated>2020-06-15T12:59:49Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.08376v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.08376v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Due to its convenience, biometric authentication, especial face authentication, has become increasingly mainstream and thus is now a prime target for attackers. Presentation attacks and face morphing are typical types of attack. Previous research has shown that finger-vein- and fingerprint-based authentication methods are susceptible to wolf attacks, in which a wolf sample matches many enrolled user templates. In this work, we demonstrated that wolf (generic) faces, which we call \"master faces,\" can also compromise face recognition systems and that the master face concept can be generalized in some cases. Motivated by recent similar work in the fingerprint domain, we generated high-quality master faces by using the state-of-the-art face generator StyleGAN in a process called latent variable evolution. Experiments demonstrated that even attackers with limited resources using only pre-trained models available on the Internet can initiate master face attacks. The results, in addition to demonstrating performance from the attacker's point of view, can also be used to clarify and improve the performance of face recognition systems and harden face authentication systems.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-15T12:59:49Z</published>\n",
      "    <arxiv:comment>Accepted to be Published in Proceedings of the 2020 International Joint Conference on Biometrics (IJCB 2020), Houston, USA</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Huy H. Nguyen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Junichi Yamagishi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Isao Echizen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sébastien Marcel</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.08465v1</id>\n",
      "    <title>Neural Certificates for Safe Control Policies</title>\n",
      "    <updated>2020-06-15T15:14:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.08465v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.08465v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper develops an approach to learn a policy of a dynamical system that is guaranteed to be both provably safe and goal-reaching. Here, the safety means that a policy must not drive the state of the system to any unsafe region, while the goal-reaching requires the trajectory of the controlled system asymptotically converges to a goal region (a generalization of stability). We obtain the safe and goal-reaching policy by jointly learning two additional certificate functions: a barrier function that guarantees the safety and a developed Lyapunov-like function to fulfill the goal-reaching requirement, both of which are represented by neural networks. We show the effectiveness of the method to learn both safe and goal-reaching policies on various systems, including pendulums, cart-poles, and UAVs.</summary>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-15T15:14:18Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SY\"/>\n",
      "    <author>\n",
      "      <name>Wanxin Jin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhaoran Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhuoran Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shaoshuai Mou</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.06704v1</id>\n",
      "    <title>End-to-end Sinkhorn Autoencoder with Noise Generator</title>\n",
      "    <updated>2020-06-11T18:04:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.06704v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.06704v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this work, we propose a novel end-to-end sinkhorn autoencoder with noise generator for efficient data collection simulation. Simulating processes that aim at collecting experimental data is crucial for multiple real-life applications, including nuclear medicine, astronomy and high energy physics. Contemporary methods, such as Monte Carlo algorithms, provide high-fidelity results at a price of high computational cost. Multiple attempts are taken to reduce this burden, e.g. using generative approaches based on Generative Adversarial Networks or Variational Autoencoders. Although such methods are much faster, they are often unstable in training and do not allow sampling from an entire data distribution. To address these shortcomings, we introduce a novel method dubbed end-to-end Sinkhorn Autoencoder, that leverages sinkhorn algorithm to explicitly align distribution of encoded real data examples and generated noise. More precisely, we extend autoencoder architecture by adding a deterministic neural network trained to map noise from a known distribution onto autoencoder latent space representing data distribution. We optimise the entire model jointly. Our method outperforms competing approaches on a challenging dataset of simulation data from Zero Degree Calorimeters of ALICE experiment in LHC. as well as standard benchmarks, such as MNIST and CelebA.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-11T18:04:10Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Kamil Deja</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jan Dubiński</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Piotr Nowak</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sandro Wenzel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tomasz Trzciński</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.06733v1</id>\n",
      "    <title>IDEAL: Inexact DEcentralized Accelerated Augmented Lagrangian Method</title>\n",
      "    <updated>2020-06-11T18:49:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.06733v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.06733v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We introduce a framework for designing primal methods under the decentralized optimization setting where local functions are smooth and strongly convex. Our approach consists of approximately solving a sequence of sub-problems induced by the accelerated augmented Lagrangian method, thereby providing a systematic way for deriving several well-known decentralized algorithms including EXTRA arXiv:1404.6264 and SSDA arXiv:1702.08704. When coupled with accelerated gradient descent, our framework yields a novel primal algorithm whose convergence rate is optimal and matched by recently derived lower bounds. We provide experimental results that demonstrate the effectiveness of the proposed algorithm on highly ill-conditioned problems.</summary>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-11T18:49:06Z</published>\n",
      "    <arxiv:primary_category term=\"math.OC\"/>\n",
      "    <author>\n",
      "      <name>Yossi Arjevani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joan Bruna</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bugra Can</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mert Gürbüzbalaban</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stefanie Jegelka</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hongzhou Lin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.06743v2</id>\n",
      "    <title>Faster DBSCAN via subsampled similarity queries</title>\n",
      "    <updated>2020-10-22T01:19:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.06743v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.06743v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>DBSCAN is a popular density-based clustering algorithm. It computes the $ε$-neighborhood graph of a dataset and uses the connected components of the high-degree nodes to decide the clusters. However, the full neighborhood graph may be too costly to compute with a worst-case complexity of $O(n^2)$. In this paper, we propose a simple variant called SNG-DBSCAN, which clusters based on a subsampled $ε$-neighborhood graph, only requires access to similarity queries for pairs of points and in particular avoids any complex data structures which need the embeddings of the data points themselves. The runtime of the procedure is $O(sn^2)$, where $s$ is the sampling rate. We show under some natural theoretical assumptions that $s \\approx \\log n/n$ is sufficient for statistical cluster recovery guarantees leading to an $O(n\\log n)$ complexity. We provide an extensive experimental analysis showing that on large datasets, one can subsample as little as $0.1\\%$ of the neighborhood graph, leading to as much as over 200x speedup and 250x reduction in RAM consumption compared to scikit-learn's implementation of DBSCAN, while still maintaining competitive clustering performance.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-11T18:57:54Z</published>\n",
      "    <arxiv:comment>34th Conference on Neural Information Processing Systems (NeurIPS 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Heinrich Jiang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jennifer Jang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jakub Łącki</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.07985v2</id>\n",
      "    <title>Explaining Predictions by Approximating the Local Decision Boundary</title>\n",
      "    <updated>2020-10-22T18:22:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.07985v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.07985v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Constructing accurate model-agnostic explanations for opaque machine learning models remains a challenging task. Classification models for high-dimensional data, like images, are often inherently complex. To reduce this complexity, individual predictions may be explained locally, either in terms of a simpler local surrogate model or by communicating how the predictions contrast with those of another class. However, existing approaches still fall short in the following ways: a) they measure locality using a (Euclidean) metric that is not meaningful for non-linear high-dimensional data; or b) they do not attempt to explain the decision boundary, which is the most relevant characteristic of classifiers that are optimized for classification accuracy; or c) they do not give the user any freedom in specifying attributes that are meaningful to them. We address these issues in a new procedure for local decision boundary approximation (DBA). To construct a meaningful metric, we train a variational autoencoder to learn a Euclidean latent space of encoded data representations. We impose interpretability by exploiting attribute annotations to map the latent space to attributes that are meaningful to the user. A difficulty in evaluating explainability approaches is the lack of a ground truth. We address this by introducing a new benchmark data set with artificially generated Iris images, and showing that we can recover the latent attributes that locally determine the class. We further evaluate our approach on tabular data and on the CelebA image data set.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-14T19:12:42Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Georgios Vlassopoulos</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tim van Erven</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Henry Brighton</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vlado Menkovski</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.08089v3</id>\n",
      "    <title>Generalized Adversarially Learned Inference</title>\n",
      "    <updated>2020-12-21T15:34:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.08089v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.08089v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Allowing effective inference of latent vectors while training GANs can greatly increase their applicability in various downstream tasks. Recent approaches, such as ALI and BiGAN frameworks, develop methods of inference of latent variables in GANs by adversarially training an image generator along with an encoder to match two joint distributions of image and latent vector pairs. We generalize these approaches to incorporate multiple layers of feedback on reconstructions, self-supervision, and other forms of supervision based on prior or learned knowledge about the desired solutions. We achieve this by modifying the discriminator's objective to correctly identify more than two joint distributions of tuples of an arbitrary number of random variables consisting of images, latent vectors, and other variables generated through auxiliary tasks, such as reconstruction and inpainting or as outputs of suitable pre-trained models. We design a non-saturating maximization objective for the generator-encoder pair and prove that the resulting adversarial game corresponds to a global optimum that simultaneously matches all the distributions. Within our proposed framework, we introduce a novel set of techniques for providing self-supervised feedback to the model based on properties, such as patch-level correspondence and cycle consistency of reconstructions. Through comprehensive experiments, we demonstrate the efficacy, scalability, and flexibility of the proposed approach for a variety of tasks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-15T02:18:13Z</published>\n",
      "    <arxiv:comment>AAAI 2021 (accepted for publication)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yatin Dandi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Homanga Bharadhwaj</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abhishek Kumar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Piyush Rai</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.08129v1</id>\n",
      "    <title>Emotion Recognition in Audio and Video Using Deep Neural Networks</title>\n",
      "    <updated>2020-06-15T04:50:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.08129v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.08129v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Humans are able to comprehend information from multiple domains for e.g. speech, text and visual. With advancement of deep learning technology there has been significant improvement of speech recognition. Recognizing emotion from speech is important aspect and with deep learning technology emotion recognition has improved in accuracy and latency. There are still many challenges to improve accuracy. In this work, we attempt to explore different neural networks to improve accuracy of emotion recognition. With different architectures explored, we find (CNN+RNN) + 3DCNN multi-model architecture which processes audio spectrograms and corresponding video frames giving emotion prediction accuracy of 54.0% among 4 emotions and 71.75% among 3 emotions using IEMOCAP[2] dataset.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-15T04:50:18Z</published>\n",
      "    <arxiv:comment>9 pages, 9 figures, 3 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Mandeep Singh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuan Fang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05822v2</id>\n",
      "    <title>Description and Discussion on DCASE2020 Challenge Task2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring</title>\n",
      "    <updated>2020-08-08T06:38:07Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05822v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05822v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we present the task description and discuss the results of the DCASE 2020 Challenge Task 2: Unsupervised Detection of Anomalous Sounds for Machine Condition Monitoring. The goal of anomalous sound detection (ASD) is to identify whether the sound emitted from a target machine is normal or anomalous. The main challenge of this task is to detect unknown anomalous sounds under the condition that only normal sound samples have been provided as training data. We have designed this challenge as the first benchmark of ASD research, which includes a large-scale dataset, evaluation metrics, and a simple baseline system. We received 117 submissions from 40 teams, and several novel approaches have been developed as a result of this challenge. On the basis of the analysis of the evaluation results, we discuss two new approaches and their problems.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-10T13:17:36Z</published>\n",
      "    <arxiv:comment>Submitted to DCASE2020 Workshop</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Yuma Koizumi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yohei Kawaguchi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Keisuke Imoto</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Toshiki Nakamura</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuki Nikaido</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ryo Tanabe</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Harsh Purohit</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kaori Suefusa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Takashi Endo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Masahiro Yasuda</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Noboru Harada</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05832v1</id>\n",
      "    <title>Adaptive Reinforcement Learning through Evolving Self-Modifying Neural Networks</title>\n",
      "    <updated>2020-05-22T02:24:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05832v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05832v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The adaptive learning capabilities seen in biological neural networks are largely a product of the self-modifying behavior emerging from online plastic changes in synaptic connectivity. Current methods in Reinforcement Learning (RL) only adjust to new interactions after reflection over a specified time interval, preventing the emergence of online adaptivity. Recent work addressing this by endowing artificial neural networks with neuromodulated plasticity have been shown to improve performance on simple RL tasks trained using backpropagation, but have yet to scale up to larger problems. Here we study the problem of meta-learning in a challenging quadruped domain, where each leg of the quadruped has a chance of becoming unusable, requiring the agent to adapt by continuing locomotion with the remaining limbs. Results demonstrate that agents evolved using self-modifying plastic networks are more capable of adapting to complex meta-learning learning tasks, even outperforming the same network updated using gradient-based algorithms while taking less time to train.</summary>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-22T02:24:44Z</published>\n",
      "    <arxiv:comment>GECCO'2020 Poster: Submitted and accepted</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.NE\"/>\n",
      "    <arxiv:journal_ref>Proc. of GECCO 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Samuel Schmidgall</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3377929.3389901</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3377929.3389901\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05854v3</id>\n",
      "    <title>Learning the geometry of wave-based imaging</title>\n",
      "    <updated>2020-11-10T06:34:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05854v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05854v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose a general physics-based deep learning architecture for wave-based imaging problems. A key difficulty in imaging problems with a varying background wave speed is that the medium \"bends\" the waves differently depending on their position and direction. This space-bending geometry makes the equivariance to translations of convolutional networks an undesired inductive bias. We build an interpretable neural architecture inspired by Fourier integral operators (FIOs) which approximate the wave physics. FIOs model a wide range of imaging modalities, from seismology and radar to Doppler and ultrasound. We focus on learning the geometry of wave propagation captured by FIOs, which is implicit in the data, via a loss based on optimal transport. The proposed FIONet performs significantly better than the usual baselines on a number of imaging inverse problems, especially in out-of-distribution tests.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-10T14:29:54Z</published>\n",
      "    <arxiv:comment>Accepted as spotlight presentation to NeurIPS '20</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Konik Kothari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maarten de Hoop</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ivan Dokmanić</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05882v4</id>\n",
      "    <title>Self-Supervised Learning Aided Class-Incremental Lifelong Learning</title>\n",
      "    <updated>2020-10-07T12:46:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05882v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05882v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Lifelong or continual learning remains to be a challenge for artificial neural network, as it is required to be both stable for preservation of old knowledge and plastic for acquisition of new knowledge. It is common to see previous experience get overwritten, which leads to the well-known issue of catastrophic forgetting, especially in the scenario of class-incremental learning (Class-IL). Recently, many lifelong learning methods have been proposed to avoid catastrophic forgetting. However, models which learn without replay of the input data, would encounter another problem which has been ignored, and we refer to it as prior information loss (PIL). In training procedure of Class-IL, as the model has no knowledge about following tasks, it would only extract features necessary for tasks learned so far, whose information is insufficient for joint classification. In this paper, our empirical results on several image datasets show that PIL limits the performance of current state-of-the-art method for Class-IL, the orthogonal weights modification (OWM) algorithm. Furthermore, we propose to combine self-supervised learning, which can provide effective representations without requiring labels, with Class-IL to partly get around this problem. Experiments show superiority of proposed method to OWM, as well as other strong baselines.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-10T15:15:27Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Song Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gehui Shen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jinsong Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhi-Hong Deng</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05939v1</id>\n",
      "    <title>Is the Skip Connection Provable to Reform the Neural Network Loss Landscape?</title>\n",
      "    <updated>2020-06-10T16:46:19Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05939v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05939v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The residual network is now one of the most effective structures in deep learning, which utilizes the skip connections to ``guarantee\" the performance will not get worse. However, the non-convexity of the neural network makes it unclear whether the skip connections do provably improve the learning ability since the nonlinearity may create many local minima. In some previous works \\cite{freeman2016topology}, it is shown that despite the non-convexity, the loss landscape of the two-layer ReLU network has good properties when the number $m$ of hidden nodes is very large. In this paper, we follow this line to study the topology (sub-level sets) of the loss landscape of deep ReLU neural networks with a skip connection and theoretically prove that the skip connection network inherits the good properties of the two-layer network and skip connections can help to control the connectedness of the sub-level sets, such that any local minima worse than the global minima of some two-layer ReLU network will be very ``shallow\". The ``depth\" of these local minima are at most $O(m^{(η-1)/n})$, where $n$ is the input dimension, $η&lt;1$. This provides a theoretical explanation for the effectiveness of the skip connection in deep learning.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-10T16:46:19Z</published>\n",
      "    <arxiv:comment>Accepted bt IJCAI2020. SOLE copyright holder is IJCAI (international Joint Conferences on Artificial Intelligence), all rights reserved. http://static.ijcai.org/2020-accepted_papers.html</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Lifu Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bo Shen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ning Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhiyuan Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05986v2</id>\n",
      "    <title>ClarQ: A large-scale and diverse dataset for Clarification Question Generation</title>\n",
      "    <updated>2020-06-11T17:18:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05986v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05986v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Question answering and conversational systems are often baffled and need help clarifying certain ambiguities. However, limitations of existing datasets hinder the development of large-scale models capable of generating and utilising clarification questions. In order to overcome these limitations, we devise a novel bootstrapping framework (based on self-supervision) that assists in the creation of a diverse, large-scale dataset of clarification questions based on post-comment tuples extracted from stackexchange. The framework utilises a neural network based architecture for classifying clarification questions. It is a two-step method where the first aims to increase the precision of the classifier and second aims to increase its recall. We quantitatively demonstrate the utility of the newly created dataset by applying it to the downstream task of question-answering. The final dataset, ClarQ, consists of ~2M examples distributed across 173 domains of stackexchange. We release this dataset in order to foster research into the field of clarification question generation with the larger goal of enhancing dialog and question answering systems.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-10T17:56:50Z</published>\n",
      "    <arxiv:comment>Accepted at ACL 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Vaibhav Kumar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alan W. black</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.07864v1</id>\n",
      "    <title>Cityscapes 3D: Dataset and Benchmark for 9 DoF Vehicle Detection</title>\n",
      "    <updated>2020-06-14T10:56:27Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.07864v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.07864v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Detecting vehicles and representing their position and orientation in the three dimensional space is a key technology for autonomous driving. Recently, methods for 3D vehicle detection solely based on monocular RGB images gained popularity. In order to facilitate this task as well as to compare and drive state-of-the-art methods, several new datasets and benchmarks have been published. Ground truth annotations of vehicles are usually obtained using lidar point clouds, which often induces errors due to imperfect calibration or synchronization between both sensors. To this end, we propose Cityscapes 3D, extending the original Cityscapes dataset with 3D bounding box annotations for all types of vehicles. In contrast to existing datasets, our 3D annotations were labeled using stereo RGB images only and capture all nine degrees of freedom. This leads to a pixel-accurate reprojection in the RGB image and a higher range of annotations compared to lidar-based approaches. In order to ease multitask learning, we provide a pairing of 2D instance segments with 3D bounding boxes. In addition, we complement the Cityscapes benchmark suite with 3D vehicle detection based on the new annotations as well as metrics presented in this work. Dataset and benchmark are available online.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-14T10:56:27Z</published>\n",
      "    <arxiv:comment>2020 \"Scalability in Autonomous Driving\" CVPR Workshop</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Nils Gählert</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nicolas Jourdan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marius Cordts</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Uwe Franke</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joachim Denzler</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.07885v2</id>\n",
      "    <title>Optical Music Recognition: State of the Art and Major Challenges</title>\n",
      "    <updated>2020-06-22T16:33:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.07885v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.07885v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Optical Music Recognition (OMR) is concerned with transcribing sheet music into a machine-readable format. The transcribed copy should allow musicians to compose, play and edit music by taking a picture of a music sheet. Complete transcription of sheet music would also enable more efficient archival. OMR facilitates examining sheet music statistically or searching for patterns of notations, thus helping use cases in digital musicology too. Recently, there has been a shift in OMR from using conventional computer vision techniques towards a deep learning approach. In this paper, we review relevant works in OMR, including fundamental methods and significant outcomes, and highlight different stages of the OMR pipeline. These stages often lack standard input and output representation and standardised evaluation. Therefore, comparing different approaches and evaluating the impact of different processing methods can become rather complex. This paper provides recommendations for future work, addressing some of the highlighted issues and represents a position in furthering this important field of research.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-14T12:40:17Z</published>\n",
      "    <arxiv:comment>Author manuscript for TENOR 2020 conference. 11 pages with references, 3 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Elona Shatri</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>György Fazekas</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.07900v1</id>\n",
      "    <title>ResOT: Resource-Efficient Oblique Trees for Neural Signal Classification</title>\n",
      "    <updated>2020-06-14T13:29:02Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.07900v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.07900v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Classifiers that can be implemented on chip with minimal computational and memory resources are essential for edge computing in emerging applications such as medical and IoT devices. This paper introduces a machine learning model based on oblique decision trees to enable resource-efficient classification on a neural implant. By integrating model compression with probabilistic routing and implementing cost-aware learning, our proposed model could significantly reduce the memory and hardware cost compared to state-of-the-art models, while maintaining the classification accuracy. We trained the resource-efficient oblique tree with power-efficient regularization (ResOT-PE) on three neural classification tasks to evaluate the performance, memory, and hardware requirements. On seizure detection task, we were able to reduce the model size by 3.4X and the feature extraction cost by 14.6X compared to the ensemble of boosted trees, using the intracranial EEG from 10 epilepsy patients. In a second experiment, we tested the ResOT-PE model on tremor detection for Parkinson's disease, using the local field potentials from 12 patients implanted with a deep-brain stimulation (DBS) device. We achieved a comparable classification performance as the state-of-the-art boosted tree ensemble, while reducing the model size and feature extraction cost by 10.6X and 6.8X, respectively. We also tested on a 6-class finger movement detection task using ECoG recordings from 9 subjects, reducing the model size by 17.6X and feature computation cost by 5.1X. The proposed model can enable a low-power and memory-efficient implementation of classifiers for real-time neurological disease detection and motor decoding.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-14T13:29:02Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Bingzhao Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Masoud Farivar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mahsa Shoaran</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.07909v2</id>\n",
      "    <title>Leveraging Multimodal Behavioral Analytics for Automated Job Interview Performance Assessment and Feedback</title>\n",
      "    <updated>2020-06-16T14:18:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.07909v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.07909v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Behavioral cues play a significant part in human communication and cognitive perception. In most professional domains, employee recruitment policies are framed such that both professional skills and personality traits are adequately assessed. Hiring interviews are structured to evaluate expansively a potential employee's suitability for the position - their professional qualifications, interpersonal skills, ability to perform in critical and stressful situations, in the presence of time and resource constraints, etc. Therefore, candidates need to be aware of their positive and negative attributes and be mindful of behavioral cues that might have adverse effects on their success. We propose a multimodal analytical framework that analyzes the candidate in an interview scenario and provides feedback for predefined labels such as engagement, speaking rate, eye contact, etc. We perform a comprehensive analysis that includes the interviewee's facial expressions, speech, and prosodic information, using the video, audio, and text transcripts obtained from the recorded interview. We use these multimodal data sources to construct a composite representation, which is used for training machine learning classifiers to predict the class labels. Such analysis is then used to provide constructive feedback to the interviewee for their behavioral cues and body language. Experimental validation showed that the proposed methodology achieved promising results.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-14T14:20:42Z</published>\n",
      "    <arxiv:comment>9 pages, ACL 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Anumeha Agrawal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rosa Anil George</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Selvan Sunitha Ravi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sowmya Kamath S</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anand Kumar M</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.07911v1</id>\n",
      "    <title>Loss Rate Forecasting Framework Based on Macroeconomic Changes: Application to US Credit Card Industry</title>\n",
      "    <updated>2020-06-14T14:22:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.07911v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.07911v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A major part of the balance sheets of the largest US banks consists of credit card portfolios. Hence, managing the charge-off rates is a vital task for the profitability of the credit card industry. Different macroeconomic conditions affect individuals' behavior in paying down their debts. In this paper, we propose an expert system for loss forecasting in the credit card industry using macroeconomic indicators. We select the indicators based on a thorough review of the literature and experts' opinions covering all aspects of the economy, consumer, business, and government sectors. The state of the art machine learning models are used to develop the proposed expert system framework. We develop two versions of the forecasting expert system, which utilize different approaches to select between the lags added to each indicator. Among 19 macroeconomic indicators that were used as the input, six were used in the model with optimal lags, and seven indicators were selected by the model using all lags. The features that were selected by each of these models covered all three sectors of the economy. Using the charge-off data for the top 100 US banks ranked by assets from the first quarter of 1985 to the second quarter of 2019, we achieve mean squared error values of 1.15E-03 and 1.04E-03 using the model with optimal lags and the model with all lags, respectively. The proposed expert system gives a holistic view of the economy to the practitioners in the credit card industry and helps them to see the impact of different macroeconomic conditions on their future loss.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"econ.GN\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-14T14:22:59Z</published>\n",
      "    <arxiv:comment>45 pages, 16 figures, 16 tables, submitted to Expert Systems with Applications Journal</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Sajjad Taghiyeh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David C Lengacher</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Robert B Handfield</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.07917v2</id>\n",
      "    <title>Robust Recursive Partitioning for Heterogeneous Treatment Effects with Uncertainty Quantification</title>\n",
      "    <updated>2020-10-18T03:00:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.07917v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.07917v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Subgroup analysis of treatment effects plays an important role in applications from medicine to public policy to recommender systems. It allows physicians (for example) to identify groups of patients for whom a given drug or treatment is likely to be effective and groups of patients for which it is not. Most of the current methods of subgroup analysis begin with a particular algorithm for estimating individualized treatment effects (ITE) and identify subgroups by maximizing the difference across subgroups of the average treatment effect in each subgroup. These approaches have several weaknesses: they rely on a particular algorithm for estimating ITE, they ignore (in)homogeneity within identified subgroups, and they do not produce good confidence estimates. This paper develops a new method for subgroup analysis, R2P, that addresses all these weaknesses. R2P uses an arbitrary, exogenously prescribed algorithm for estimating ITE and quantifies the uncertainty of the ITE estimation, using a construction that is more robust than other methods. Experiments using synthetic and semi-synthetic datasets (based on real data) demonstrate that R2P constructs partitions that are simultaneously more homogeneous within groups and more heterogeneous across groups than the partitions produced by other methods. Moreover, because R2P can employ any ITE estimator, it also produces much narrower confidence intervals with a prescribed coverage guarantee than other methods.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-14T14:50:02Z</published>\n",
      "    <arxiv:comment>19 pages, 7 figures, NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Hyun-Suk Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yao Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>William Zame</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cong Shen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jang-Won Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mihaela van der Schaar</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.07934v1</id>\n",
      "    <title>Adversarial Attacks and Detection on Reinforcement Learning-Based Interactive Recommender Systems</title>\n",
      "    <updated>2020-06-14T15:41:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.07934v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.07934v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Adversarial attacks pose significant challenges for detecting adversarial attacks at an early stage. We propose attack-agnostic detection on reinforcement learning-based interactive recommendation systems. We first craft adversarial examples to show their diverse distributions and then augment recommendation systems by detecting potential attacks with a deep learning-based classifier based on the crafted data. Finally, we study the attack strength and frequency of adversarial examples and evaluate our model on standard datasets with multiple crafting methods. Our extensive experiments show that most adversarial attacks are effective, and both attack strength and attack frequency impact the attack performance. The strategically-timed attack achieves comparative attack performance with only 1/3 to 1/2 attack frequency. Besides, our black-box detector trained with one crafting method has the generalization ability over several crafting methods.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-14T15:41:47Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yuanjiang Cao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaocong Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lina Yao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xianzhi Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wei Emma Zhang</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3397271.3401196</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3397271.3401196\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.06519v2</id>\n",
      "    <title>Reserve Price Optimization for First Price Auctions</title>\n",
      "    <updated>2020-06-28T19:25:33Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.06519v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.06519v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The display advertising industry has recently transitioned from second- to first-price auctions as its primary mechanism for ad allocation and pricing. In light of this, publishers need to re-evaluate and optimize their auction parameters, notably reserve prices. In this paper, we propose a gradient-based algorithm to adaptively update and optimize reserve prices based on estimates of bidders' responsiveness to experimental shocks in reserves. Our key innovation is to draw on the inherent structure of the revenue objective in order to reduce the variance of gradient estimates and improve convergence rates in both theory and practice. We show that revenue in a first-price auction can be usefully decomposed into a \\emph{demand} component and a \\emph{bidding} component, and introduce techniques to reduce the variance of each component. We characterize the bias-variance trade-offs of these techniques and validate the performance of our proposed algorithm through experiments on synthetic data and real display ad auctions data from Google ad exchange.</summary>\n",
      "    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"econ.EM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-11T15:35:19Z</published>\n",
      "    <arxiv:primary_category term=\"cs.GT\"/>\n",
      "    <author>\n",
      "      <name>Zhe Feng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sébastien Lahaie</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jon Schneider</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jinchao Ye</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.06526v1</id>\n",
      "    <title>Recurrent Neural Networks for Handover Management in Next-Generation Self-Organized Networks</title>\n",
      "    <updated>2020-06-11T15:41:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.06526v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.06526v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we discuss a handover management scheme for Next Generation Self-Organized Networks. We propose to extract experience from full protocol stack data, to make smart handover decisions in a multi-cell scenario, where users move and are challenged by deep zones of an outage. Traditional handover schemes have the drawback of taking into account only the signal strength from the serving, and the target cell, before the handover. However, we believe that the expected Quality of Experience (QoE) resulting from the decision of target cell to handover to, should be the driving principle of the handover decision. In particular, we propose two models based on multi-layer many-to-one LSTM architecture, and a multi-layer LSTM AutoEncoder (AE) in conjunction with a MultiLayer Perceptron (MLP) neural network. We show that using experience extracted from data, we can improve the number of users finalizing the download by 18%, and we can reduce the time to download, with respect to a standard event-based handover benchmark scheme. Moreover, for the sake of generalization, we test the LSTM Autoencoder in a different scenario, where it maintains its performance improvements with a slight degradation, compared to the original scenario.</summary>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-11T15:41:12Z</published>\n",
      "    <arxiv:primary_category term=\"cs.NI\"/>\n",
      "    <author>\n",
      "      <name>Zoraze Ali</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marco Miozzo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lorenza Giupponi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paolo Dini</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stojan Denic</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stavroula Vassaki</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.06599v2</id>\n",
      "    <title>Robust model training and generalisation with Studentising flows</title>\n",
      "    <updated>2020-07-11T12:50:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.06599v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.06599v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Normalising flows are tractable probabilistic models that leverage the power of deep learning to describe a wide parametric family of distributions, all while remaining trainable using maximum likelihood. We discuss how these methods can be further improved based on insights from robust (in particular, resistant) statistics. Specifically, we propose to endow flow-based models with fat-tailed latent distributions such as multivariate Student's $t$, as a simple drop-in replacement for the Gaussian distribution used by conventional normalising flows. While robustness brings many advantages, this paper explores two of them: 1) We describe how using fatter-tailed base distributions can give benefits similar to gradient clipping, but without compromising the asymptotic consistency of the method. 2) We also discuss how robust ideas lead to models with reduced generalisation gap and improved held-out data likelihood. Experiments on several different datasets confirm the efficacy of the proposed approach in both regards.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-11T16:47:01Z</published>\n",
      "    <arxiv:comment>9 pages, 8 figures, accepted for publication at INNF+ 2020 (Second ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Simon Alexanderson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gustav Eje Henter</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.08212v2</id>\n",
      "    <title>Tight Nonparametric Convergence Rates for Stochastic Gradient Descent under the Noiseless Linear Model</title>\n",
      "    <updated>2020-10-27T08:38:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.08212v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.08212v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In the context of statistical supervised learning, the noiseless linear model assumes that there exists a deterministic linear relation $Y = \\langle θ_*, X \\rangle$ between the random output $Y$ and the random feature vector $Φ(U)$, a potentially non-linear transformation of the inputs $U$. We analyze the convergence of single-pass, fixed step-size stochastic gradient descent on the least-square risk under this model. The convergence of the iterates to the optimum $θ_*$ and the decay of the generalization error follow polynomial convergence rates with exponents that both depend on the regularities of the optimum $θ_*$ and of the feature vectors $Φ(u)$. We interpret our result in the reproducing kernel Hilbert space framework. As a special case, we analyze an online algorithm for estimating a real function on the unit interval from the noiseless observation of its value at randomly sampled points; the convergence depends on the Sobolev smoothness of the function and of a chosen kernel. Finally, we apply our analysis beyond the supervised learning setting to obtain convergence rates for the averaging process (a.k.a. gossip algorithm) on a graph depending on its spectral dimension.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-15T08:25:50Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Raphaël Berthier</name>\n",
      "      <arxiv:affiliation>PSL, SIERRA</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Francis Bach</name>\n",
      "      <arxiv:affiliation>SIERRA, PSL</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pierre Gaillard</name>\n",
      "      <arxiv:affiliation>SIERRA, PSL, Thoth</arxiv:affiliation>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.08256v5</id>\n",
      "    <title>Markov-Lipschitz Deep Learning</title>\n",
      "    <updated>2020-09-30T09:17:19Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.08256v5\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.08256v5\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose a novel framework, called Markov-Lipschitz deep learning (MLDL), to tackle geometric deterioration caused by collapse, twisting, or crossing in vector-based neural network transformations for manifold-based representation learning and manifold data generation. A prior constraint, called locally isometric smoothness (LIS), is imposed across-layers and encoded into a Markov random field (MRF)-Gibbs distribution. This leads to the best possible solutions for local geometry preservation and robustness as measured by locally geometric distortion and locally bi-Lipschitz continuity. Consequently, the layer-wise vector transformations are enhanced into well-behaved, LIS-constrained metric homeomorphisms. Extensive experiments, comparisons, and ablation study demonstrate significant advantages of MLDL for manifold learning and manifold data generation. MLDL is general enough to enhance any vector transformation-based networks. The code is available at https://github.com/westlake-cairi/Markov-Lipschitz-Deep-Learning.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-15T09:46:42Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Stan Z. Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zelin Zang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lirong Wu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.08274v2</id>\n",
      "    <title>Exploration of End-to-End ASR for OpenSTT -- Russian Open Speech-to-Text Dataset</title>\n",
      "    <updated>2020-07-26T20:21:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.08274v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.08274v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper presents an exploration of end-to-end automatic speech recognition systems (ASR) for the largest open-source Russian language data set -- OpenSTT. We evaluate different existing end-to-end approaches such as joint CTC/Attention, RNN-Transducer, and Transformer. All of them are compared with the strong hybrid ASR system based on LF-MMI TDNN-F acoustic model. For the three available validation sets (phone calls, YouTube, and books), our best end-to-end model achieves word error rate (WER) of 34.8%, 19.1%, and 18.1%, respectively. Under the same conditions, the hybridASR system demonstrates 33.5%, 20.9%, and 18.6% WER.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-15T10:35:31Z</published>\n",
      "    <arxiv:comment>Accepted by SPECOM 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Andrei Andrusenko</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aleksandr Laptev</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ivan Medennikov</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1007/978-3-030-60276-5_4</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1007/978-3-030-60276-5_4\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.08292v1</id>\n",
      "    <title>Robust Locality-Aware Regression for Labeled Data Classification</title>\n",
      "    <updated>2020-06-15T11:36:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.08292v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.08292v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>With the dramatic increase of dimensions in the data representation, extracting latent low-dimensional features becomes of the utmost importance for efficient classification. Aiming at the problems of unclear margin representation and difficulty in revealing the data manifold structure in most of the existing linear discriminant methods, we propose a new discriminant feature extraction framework, namely Robust Locality-Aware Regression (RLAR). In our model, we introduce a retargeted regression to perform the marginal representation learning adaptively instead of using the general average inter-class margin. Besides, we formulate a new strategy for enhancing the local intra-class compactness of the data manifold, which can achieve the joint learning of locality-aware graph structure and desirable projection matrix. To alleviate the disturbance of outliers and prevent overfitting, we measure the regression term and locality-aware term together with the regularization term by the L2,1 norm. Further, forcing the row sparsity on the projection matrix through the L2,1 norm achieves the cooperation of feature selection and feature extraction. Then, we derive an effective iterative algorithm for solving the proposed model. The experimental results over a range of UCI data sets and other benchmark databases demonstrate that the proposed RLAR outperforms some state-of-the-art approaches.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-15T11:36:59Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Liangchen Hu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wensheng Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.08299v1</id>\n",
      "    <title>Cryptotree: fast and accurate predictions on encrypted structured data</title>\n",
      "    <updated>2020-06-15T11:48:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.08299v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.08299v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Applying machine learning algorithms to private data, such as financial or medical data, while preserving their confidentiality, is a difficult task. Homomorphic Encryption (HE) is acknowledged for its ability to allow computation on encrypted data, where both the input and output are encrypted, which therefore enables secure inference on private data. Nonetheless, because of the constraints of HE, such as its inability to evaluate non-polynomial functions or to perform arbitrary matrix multiplication efficiently, only inference of linear models seem usable in practice in the HE paradigm so far.\n",
      "  In this paper, we propose Cryptotree, a framework that enables the use of Random Forests (RF), a very powerful learning procedure compared to linear regression, in the context of HE. To this aim, we first convert a regular RF to a Neural RF, then adapt this to fit the HE scheme CKKS, which allows HE operations on real values. Through SIMD operations, we are able to have quick inference and prediction results better than the original RF on encrypted data.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-15T11:48:01Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Daniel Huynh</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.08305v2</id>\n",
      "    <title>Inner Ensemble Networks: Average Ensemble as an Effective Regularizer</title>\n",
      "    <updated>2020-10-09T05:59:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.08305v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.08305v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We introduce Inner Ensemble Networks (IENs) which reduce the variance within the neural network itself without an increase in the model complexity. IENs utilize ensemble parameters during the training phase to reduce the network variance. While in the testing phase, these parameters are removed without a change in the enhanced performance. IENs reduce the variance of an ordinary deep model by a factor of $1/m^{L-1}$, where $m$ is the number of inner ensembles and $L$ is the depth of the model. Also, we show empirically and theoretically that IENs lead to a greater variance reduction in comparison with other similar approaches such as dropout and maxout. Our results show a decrease of error rates between 1.7\\% and 17.3\\% in comparison with an ordinary deep model. We also show that IEN was preferred by Neural Architecture Search (NAS) methods over prior approaches. Code is available at https://github.com/abduallahmohamed/inner_ensemble_nets.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-15T11:56:11Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Abduallah Mohamed</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Muhammed Mohaimin Sadiq</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ehab AlBadawy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mohamed Elhoseiny</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christian Claudel</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05635v1</id>\n",
      "    <title>Data Augmentation for Training Dialog Models Robust to Speech Recognition Errors</title>\n",
      "    <updated>2020-06-10T03:18:15Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05635v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05635v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Speech-based virtual assistants, such as Amazon Alexa, Google assistant, and Apple Siri, typically convert users' audio signals to text data through automatic speech recognition (ASR) and feed the text to downstream dialog models for natural language understanding and response generation. The ASR output is error-prone; however, the downstream dialog models are often trained on error-free text data, making them sensitive to ASR errors during inference time. To bridge the gap and make dialog models more robust to ASR errors, we leverage an ASR error simulator to inject noise into the error-free text data, and subsequently train the dialog models with the augmented data. Compared to other approaches for handling ASR errors, such as using ASR lattice or end-to-end methods, our data augmentation approach does not require any modification to the ASR or downstream dialog models; our approach also does not introduce any additional latency during inference time. We perform extensive experiments on benchmark data and show that our approach improves the performance of downstream dialog models in the presence of ASR errors, and it is particularly effective in the low-resource situations where there are constraints on model size or the training data is scarce.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-10T03:18:15Z</published>\n",
      "    <arxiv:comment>To be presented at 2nd Workshop on NLP for ConvAI, ACL 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Longshaokan Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maryam Fazel-Zarandi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aditya Tiwari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Spyros Matsoukas</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lazaros Polymenakos</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05664v2</id>\n",
      "    <title>OpEvo: An Evolutionary Method for Tensor Operator Optimization</title>\n",
      "    <updated>2020-12-21T08:02:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05664v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05664v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Training and inference efficiency of deep neural networks highly rely on the performance of tensor operators on hardware platforms. Manually optimizing tensor operators has limitations in terms of supporting new operators or hardware platforms. Therefore, automatically optimizing device code configurations of tensor operators is getting increasingly attractive. However, current methods for tensor operator optimization usually suffer from poor sample-efficiency due to the combinatorial search space. In this work, we propose a novel evolutionary method, OpEvo, which efficiently explores the search spaces of tensor operators by introducing a topology-aware mutation operation based on q-random walk to leverage the topological structures over the search spaces. Our comprehensive experiment results show that compared with state-of-the-art (SOTA) methods OpEvo can find the best configuration with the lowest variance and least efforts in the number of trials and wall-clock time. All code of this work is available online.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-10T05:33:33Z</published>\n",
      "    <arxiv:comment>Accepted at AAAI 2021</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xiaotian Gao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cui Wei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lintao Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mao Yang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05676v1</id>\n",
      "    <title>Position Masking for Language Models</title>\n",
      "    <updated>2020-06-02T23:40:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05676v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05676v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Masked language modeling (MLM) pre-training models such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. This is an effective technique which has led to good results on all NLP benchmarks. We propose to expand upon this idea by masking the positions of some tokens along with the masked input token ids. We follow the same standard approach as BERT masking a percentage of the tokens positions and then predicting their original values using an additional fully connected classifier stage. This approach has shown good performance gains (.3\\% improvement) for the SQUAD additional improvement in convergence times. For the Graphcore IPU the convergence of BERT Base with position masking requires only 50\\% of the tokens from the original BERT paper.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-02T23:40:41Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Andy Wagner</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tiyasa Mitra</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mrinal Iyer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Godfrey Da Costa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marc Tremblay</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05698v1</id>\n",
      "    <title>Rendering Natural Camera Bokeh Effect with Deep Learning</title>\n",
      "    <updated>2020-06-10T07:28:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05698v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05698v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Bokeh is an important artistic effect used to highlight the main object of interest on the photo by blurring all out-of-focus areas. While DSLR and system camera lenses can render this effect naturally, mobile cameras are unable to produce shallow depth-of-field photos due to a very small aperture diameter of their optics. Unlike the current solutions simulating bokeh by applying Gaussian blur to image background, in this paper we propose to learn a realistic shallow focus technique directly from the photos produced by DSLR cameras. For this, we present a large-scale bokeh dataset consisting of 5K shallow / wide depth-of-field image pairs captured using the Canon 7D DSLR with 50mm f/1.8 lenses. We use these images to train a deep learning model to reproduce a natural bokeh effect based on a single narrow-aperture image. The experimental results show that the proposed approach is able to render a plausible non-uniform bokeh even in case of complex input data with multiple objects. The dataset, pre-trained models and codes used in this paper are available on the project website.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-10T07:28:06Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Andrey Ignatov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jagruti Patel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Radu Timofte</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05702v1</id>\n",
      "    <title>Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network</title>\n",
      "    <updated>2020-06-10T07:50:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05702v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05702v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot). Few-shot slot tagging faces a unique challenge compared to the other few-shot classification problems as it calls for modeling the dependencies between labels. But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets. To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label dependency patterns as transition scores. In the few-shot setting, the emission score of CRF can be calculated as a word's similarity to the representation of each label. To calculate such similarity, we propose a Label-enhanced Task-Adaptive Projection Network (L-TapNet) based on the state-of-the-art few-shot classification model -- TapNet, by leveraging label name semantics in representing labels. Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64 F1 scores in the one-shot setting.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-10T07:50:44Z</published>\n",
      "    <arxiv:comment>Accepted by ACL2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Yutai Hou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wanxiang Che</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yongkui Lai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhihan Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yijia Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Han Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ting Liu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05711v2</id>\n",
      "    <title>Use of Machine Learning for unraveling hidden correlations between Particle Size Distributions and the Mechanical Behavior of Granular Materials</title>\n",
      "    <updated>2020-06-20T18:49:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05711v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05711v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A data-driven framework was used to predict the macroscopic mechanical behavior of dense packings of polydisperse granular materials. The Discrete Element Method, DEM, was used to generate 92,378 sphere packings that covered many different kinds of particle size distributions, PSD, lying within 2 particle sizes. These packings were subjected to triaxial compression and the corresponding stress-strain curves were fitted to Duncan-Chang hyperbolic models. A multivariate statistical analysis was unsuccessful to relate the model parameters with common geotechnical and statistical descriptors derived from the PSD. In contrast, an artificial Neural Network (NN) scheme, trained with a few hundred DEM simulations, was able to anticipate the value of the model parameters for all these PSDs, with considerable accuracy. This was achieved in spite of the presence of noise in the training data. The NN revealed the existence of hidden correlations between PSD of granular materials and their macroscopic mechanical behavior.</summary>\n",
      "    <category term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-10T08:04:59Z</published>\n",
      "    <arxiv:comment>26 pages, 24 figures, 4 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cond-mat.dis-nn\"/>\n",
      "    <author>\n",
      "      <name>Ignacio G. Tejada</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pablo Antolin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.05722v1</id>\n",
      "    <title>Interferometric Graph Transform: a Deep Unsupervised Graph Representation</title>\n",
      "    <updated>2020-06-10T08:27:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.05722v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.05722v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose the Interferometric Graph Transform (IGT), which is a new class of deep unsupervised graph convolutional neural network for building graph representations. Our first contribution is to propose a generic, complex-valued spectral graph architecture obtained from a generalization of the Euclidean Fourier transform. We show that our learned representation consists of both discriminative and invariant features, thanks to a novel greedy concave objective. From our experiments, we conclude that our learning procedure exploits the topology of the spectral domain, which is normally a flaw of spectral methods, and in particular our method can recover an analytic operator for vision tasks. We test our algorithm on various and challenging tasks such as image classification (MNIST, CIFAR-10), community detection (Authorship, Facebook graph) and action recognition from 3D skeletons videos (SBU, NTU), exhibiting a new state-of-the-art in spectral graph unsupervised settings.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-10T08:27:53Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>International Conference on Machine Learning (ICML), 2020, Online, Austria</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Edouard Oyallon</name>\n",
      "      <arxiv:affiliation>MLIA</arxiv:affiliation>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.04929v2</id>\n",
      "    <title>Learning Graph Structure With A Finite-State Automaton Layer</title>\n",
      "    <updated>2020-11-06T18:26:49Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.04929v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.04929v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Graph-based neural network models are producing strong results in a number of domains, in part because graphs provide flexibility to encode domain knowledge in the form of relational structure (edges) between nodes in the graph. In practice, edges are used both to represent intrinsic structure (e.g., abstract syntax trees of programs) and more abstract relations that aid reasoning for a downstream task (e.g., results of relevant program analyses). In this work, we study the problem of learning to derive abstract relations from the intrinsic graph structure. Motivated by their power in program analyses, we consider relations defined by paths on the base graph accepted by a finite-state automaton. We show how to learn these relations end-to-end by relaxing the problem into learning finite-state automata policies on a graph-based POMDP and then training these policies using implicit differentiation. The result is a differentiable Graph Finite-State Automaton (GFSA) layer that adds a new edge type (expressed as a weighted adjacency matrix) to a base graph. We demonstrate that this layer can find shortcuts in grid-world graphs and reproduce simple static analyses on Python programs. Additionally, we combine the GFSA layer with a larger graph-based model trained end-to-end on the variable misuse program understanding task, and find that using the GFSA layer leads to better performance than using hand-engineered semantic edges or other baseline methods for adding learned edge types.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-09T17:01:34Z</published>\n",
      "    <arxiv:comment>Accepted at NeurIPS 2020 (spotlight)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Daniel D. Johnson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hugo Larochelle</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniel Tarlow</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.04979v1</id>\n",
      "    <title>A Cordial Sync: Going Beyond Marginal Policies for Multi-Agent Embodied Tasks</title>\n",
      "    <updated>2020-07-09T17:59:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.04979v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.04979v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Autonomous agents must learn to collaborate. It is not scalable to develop a new centralized agent every time a task's difficulty outpaces a single agent's abilities. While multi-agent collaboration research has flourished in gridworld-like environments, relatively little work has considered visually rich domains. Addressing this, we introduce the novel task FurnMove in which agents work together to move a piece of furniture through a living room to a goal. Unlike existing tasks, FurnMove requires agents to coordinate at every timestep. We identify two challenges when training agents to complete FurnMove: existing decentralized action sampling procedures do not permit expressive joint action policies and, in tasks requiring close coordination, the number of failed actions dominates successful actions. To confront these challenges we introduce SYNC-policies (synchronize your actions coherently) and CORDIAL (coordination loss). Using SYNC-policies and CORDIAL, our agents achieve a 58% completion rate on FurnMove, an impressive absolute gain of 25 percentage points over competitive decentralized baselines. Our dataset, code, and pretrained models are available at https://unnat.github.io/cordial-sync .</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-09T17:59:57Z</published>\n",
      "    <arxiv:comment>Accepted to ECCV 2020 (spotlight); Project page: https://unnat.github.io/cordial-sync</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Unnat Jain</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luca Weihs</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eric Kolve</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ali Farhadi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Svetlana Lazebnik</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aniruddha Kembhavi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexander Schwing</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05896v1</id>\n",
      "    <title>Learning Abstract Models for Strategic Exploration and Fast Reward Transfer</title>\n",
      "    <updated>2020-07-12T03:33:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05896v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05896v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Model-based reinforcement learning (RL) is appealing because (i) it enables planning and thus more strategic exploration, and (ii) by decoupling dynamics from rewards, it enables fast transfer to new reward functions. However, learning an accurate Markov Decision Process (MDP) over high-dimensional states (e.g., raw pixels) is extremely challenging because it requires function approximation, which leads to compounding errors. Instead, to avoid compounding errors, we propose learning an abstract MDP over abstract states: low-dimensional coarse representations of the state (e.g., capturing agent position, ignoring other objects). We assume access to an abstraction function that maps the concrete states to abstract states. In our approach, we construct an abstract MDP, which grows through strategic exploration via planning. Similar to hierarchical RL approaches, the abstract actions of the abstract MDP are backed by learned subpolicies that navigate between abstract states. Our approach achieves strong results on three of the hardest Arcade Learning Environment games (Montezuma's Revenge, Pitfall!, and Private Eye), including superhuman performance on Pitfall! without demonstrations. After training on one task, we can reuse the learned abstract MDP for new reward functions, achieving higher reward in 1000x fewer samples than model-free methods trained from scratch.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-12T03:33:50Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Evan Zheran Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ramtin Keramati</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sudarshan Seshadri</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kelvin Guu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Panupong Pasupat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Emma Brunskill</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Percy Liang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05954v1</id>\n",
      "    <title>Changing Clusters of Indian States with respect to number of Cases of COVID-19 using incrementalKMN Method</title>\n",
      "    <updated>2020-07-12T10:27:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05954v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05954v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The novel Coronavirus (COVID-19) incidence in India is currently experiencing exponential rise but with apparent spatial variation in growth rate and doubling time rate. We classify the states into five clusters with low to the high-risk category and study how the different states moved from one cluster to the other since the onset of the first case on $30^{th}$ January 2020 till the end of unlock 1 that is $30^{th}$ June 2020. We have implemented a new clustering technique called the incrementalKMN (Prasad, R. K., Sarmah, R., Chakraborty, S.(2019))</summary>\n",
      "    <category term=\"q-bio.PE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-12T10:27:38Z</published>\n",
      "    <arxiv:primary_category term=\"q-bio.PE\"/>\n",
      "    <author>\n",
      "      <name>Rabinder Kumar Prasad</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rosy Sarmah</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Subrata Chakraborty</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06002v1</id>\n",
      "    <title>Multi-Modality Information Fusion for Radiomics-based Neural Architecture Search</title>\n",
      "    <updated>2020-07-12T14:35:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06002v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06002v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>'Radiomics' is a method that extracts mineable quantitative features from radiographic images. These features can then be used to determine prognosis, for example, predicting the development of distant metastases (DM). Existing radiomics methods, however, require complex manual effort including the design of hand-crafted radiomic features and their extraction and selection. Recent radiomics methods, based on convolutional neural networks (CNNs), also require manual input in network architecture design and hyper-parameter tuning. Radiomic complexity is further compounded when there are multiple imaging modalities, for example, combined positron emission tomography - computed tomography (PET-CT) where there is functional information from PET and complementary anatomical localization information from computed tomography (CT). Existing multi-modality radiomics methods manually fuse the data that are extracted separately. Reliance on manual fusion often results in sub-optimal fusion because they are dependent on an 'expert's' understanding of medical images. In this study, we propose a multi-modality neural architecture search method (MM-NAS) to automatically derive optimal multi-modality image features for radiomics and thus negate the dependence on a manual process. We evaluated our MM-NAS on the ability to predict DM using a public PET-CT dataset of patients with soft-tissue sarcomas (STSs). Our results show that our MM-NAS had a higher prediction accuracy when compared to state-of-the-art radiomics methods.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-12T14:35:13Z</published>\n",
      "    <arxiv:comment>Accepted by MICCAI 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Yige Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lei Bi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michael Fulham</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dagan Feng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jinman Kim</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06037v4</id>\n",
      "    <title>Estimating Stochastic Poisson Intensities Using Deep Latent Models</title>\n",
      "    <updated>2020-07-23T02:07:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06037v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06037v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present methodology for estimating the stochastic intensity of a doubly stochastic Poisson process. Statistical and theoretical analyses of traffic traces show that these processes are appropriate models of high intensity traffic arriving at an array of service systems. The statistical estimation of the underlying latent stochastic intensity process driving the traffic model involves a rather complicated nonlinear filtering problem. We develop a novel simulation methodology, using deep neural networks to approximate the path measures induced by the stochastic intensity process, for solving this nonlinear filtering problem. Our simulation studies demonstrate that the method is quite accurate on both in-sample estimation and on an out-of-sample performance prediction task for an infinite server queue.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-12T16:57:53Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Ruixin Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Prateek Jaiwal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Harsha Honnappa</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06045v1</id>\n",
      "    <title>Augmenting Differentiable Simulators with Neural Networks to Close the Sim2Real Gap</title>\n",
      "    <updated>2020-07-12T17:27:11Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06045v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06045v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present a differentiable simulation architecture for articulated rigid-body dynamics that enables the augmentation of analytical models with neural networks at any point of the computation. Through gradient-based optimization, identification of the simulation parameters and network weights is performed efficiently in preliminary experiments on a real-world dataset and in sim2sim transfer applications, while poor local optima are overcome through a random search approach.</summary>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-12T17:27:11Z</published>\n",
      "    <arxiv:primary_category term=\"cs.RO\"/>\n",
      "    <author>\n",
      "      <name>Eric Heiden</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Millard</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Erwin Coumans</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gaurav S. Sukhatme</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05493v1</id>\n",
      "    <title>Learning from Data to Optimize Control in Precision Farming</title>\n",
      "    <updated>2020-07-07T12:44:17Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05493v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05493v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Precision farming is one way of many to meet a 70 percent increase in global demand for agricultural products on current agricultural land by 2050 at reduced need of fertilizers and efficient use of water resources. The catalyst for the emergence of precision farming has been satellite positioning and navigation followed by Internet-of-Things, generating vast information that can be used to optimize farming processes in real-time. Statistical tools from data mining, predictive modeling, and machine learning analyze pattern in historical data, to make predictions about future events as well as intelligent actions. This special issue presents the latest development in statistical inference, machine learning and optimum control for precision farming.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-07T12:44:17Z</published>\n",
      "    <arxiv:comment>Editorial of \"Statistical Tools in Precision Farming\", MDPI/Stats</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Alexander Kocian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luca Incrocci</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05494v1</id>\n",
      "    <title>Automatic Detection of COVID-19 Cases on X-ray images Using Convolutional Neural Networks</title>\n",
      "    <updated>2020-07-02T00:46:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05494v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05494v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In recent months the world has been surprised by the rapid advance of COVID-19. In order to face this disease and minimize its socio-economic impacts, in addition to surveillance and treatment, diagnosis is a crucial procedure. However, the realization of this is hampered by the delay and the limited access to laboratory tests, demanding new strategies to carry out case triage. In this scenario, deep learning models are being proposed as a possible option to assist the diagnostic process based on chest X-ray and computed tomography images. Therefore, this research aims to automate the process of detecting COVID-19 cases from chest images, using convolutional neural networks (CNN) through deep learning techniques. The results can contribute to expand access to other forms of detection of COVID-19 and to speed up the process of identifying this disease. All databases used, the codes built, and the results obtained from the models' training are available for open access. This action facilitates the involvement of other researchers in enhancing these models since this can contribute to the improvement of results and, consequently, the progress in confronting COVID-19.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-02T00:46:13Z</published>\n",
      "    <arxiv:comment>6 pages, 4 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Lucas P. Soares</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cesar P. Soares</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05500v2</id>\n",
      "    <title>Scientific Discovery by Generating Counterfactuals using Image Translation</title>\n",
      "    <updated>2020-07-19T23:38:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05500v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05500v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Model explanation techniques play a critical role in understanding the source of a model's performance and making its decisions transparent. Here we investigate if explanation techniques can also be used as a mechanism for scientific discovery. We make three contributions: first, we propose a framework to convert predictions from explanation techniques to a mechanism of discovery. Second, we show how generative models in combination with black-box predictors can be used to generate hypotheses (without human priors) that can be critically examined. Third, with these techniques we study classification models for retinal images predicting Diabetic Macular Edema (DME), where recent work showed that a CNN trained on these images is likely learning novel features in the image. We demonstrate that the proposed framework is able to explain the underlying scientific mechanism, thus bridging the gap between the model's performance and human understanding.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-10T17:25:52Z</published>\n",
      "    <arxiv:comment>Accepted at MICCAI 2020. This version combines camera-ready and supplement</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <arxiv:journal_ref>MICCAI 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Arunachalam Narayanaswamy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Subhashini Venugopalan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dale R. Webster</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lily Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Greg Corrado</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paisan Ruamviboonsuk</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pinal Bavishi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rory Sayres</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abigail Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Siva Balasubramanian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michael Brenner</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Philip Nelson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Avinash V. Varadarajan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05549v2</id>\n",
      "    <title>Meta-Learning Requires Meta-Augmentation</title>\n",
      "    <updated>2020-11-04T00:03:33Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05549v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05549v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Meta-learning algorithms aim to learn two components: a model that predicts targets for a task, and a base learner that quickly updates that model when given examples from a new task. This additional level of learning can be powerful, but it also creates another potential source for overfitting, since we can now overfit in either the model or the base learner. We describe both of these forms of metalearning overfitting, and demonstrate that they appear experimentally in common meta-learning benchmarks. We then use an information-theoretic framework to discuss meta-augmentation, a way to add randomness that discourages the base learner and model from learning trivial solutions that do not generalize to new tasks. We demonstrate that meta-augmentation produces large complementary benefits to recently proposed meta-regularization techniques.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-10T18:04:04Z</published>\n",
      "    <arxiv:comment>14 pages, 8 figures. NeurIPS 2020 camera ready. Code at https://github.com/google-research/google-research/tree/master/meta_augmentation</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Janarthanan Rajendran</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alex Irpan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eric Jang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05566v1</id>\n",
      "    <title>Contrastive Training for Improved Out-of-Distribution Detection</title>\n",
      "    <updated>2020-07-10T18:40:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05566v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05566v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Reliable detection of out-of-distribution (OOD) inputs is increasingly understood to be a precondition for deployment of machine learning systems. This paper proposes and investigates the use of contrastive training to boost OOD detection performance. Unlike leading methods for OOD detection, our approach does not require access to examples labeled explicitly as OOD, which can be difficult to collect in practice. We show in extensive experiments that contrastive training significantly helps OOD detection performance on a number of common benchmarks. By introducing and employing the Confusion Log Probability (CLP) score, which quantifies the difficulty of the OOD detection task by capturing the similarity of inlier and outlier datasets, we show that our method especially improves performance in the `near OOD' classes -- a particularly challenging setting for previous methods.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-10T18:40:37Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jim Winkens</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rudy Bunel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abhijit Guha Roy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Robert Stanforth</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vivek Natarajan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joseph R. Ledsam</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Patricia MacWilliams</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pushmeet Kohli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alan Karthikesalingam</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Simon Kohl</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Taylan Cemgil</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>S. M. Ali Eslami</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Olaf Ronneberger</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05593v2</id>\n",
      "    <title>Attention-guided Quality Assessment for Automated Cryo-EM Grid Screening</title>\n",
      "    <updated>2020-07-21T21:55:17Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05593v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05593v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Cryogenic electron microscopy (cryo-EM) has become an enabling technology in drug discovery and in understanding molecular bases of disease by producing near-atomic resolution (less than 0.4 nm) 3D reconstructions of biological macromolecules. The imaging process required for 3D reconstructions involves a highly iterative and empirical screening process, starting with the acquisition of low magnification images of the cryo-EM grids. These images are inspected for squares that are likely to contain useful molecular signals. Potentially useful squares within the grid are then imaged at progressively higher magnifications, with the goal of identifying sub-micron areas within circular holes (bounded by the squares) for imaging at high magnification. This arduous, multi-step data acquisition process represents a bottleneck for obtaining a high throughput data collection. Here, we focus on automating the early decision making for the microscope operator, scoring low magnification images of squares, and proposing the first deep learning framework, XCryoNet, for automated cryo-EM grid screening. XCryoNet is a semi-supervised, attention-guided deep learning approach that provides explainable scoring of automatically extracted square images using limited amounts of labeled data. Results show up to 8% and 37% improvements over a fully supervised and a no-attention solution, respectively, when labeled data is scarce.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-10T20:11:43Z</published>\n",
      "    <arxiv:comment>Accepted for publication in MICCAI 2020, the 23rd International Conference on Medical Image Computing and Computer Assisted Intervention</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Hong Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David E. Timm</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shireen Y. Elhabian</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05627v1</id>\n",
      "    <title>A Performance Guarantee for Spectral Clustering</title>\n",
      "    <updated>2020-07-10T22:03:43Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05627v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05627v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The two-step spectral clustering method, which consists of the Laplacian eigenmap and a rounding step, is a widely used method for graph partitioning. It can be seen as a natural relaxation to the NP-hard minimum ratio cut problem. In this paper we study the central question: when is spectral clustering able to find the global solution to the minimum ratio cut problem? First we provide a condition that naturally depends on the intra- and inter-cluster connectivities of a given partition under which we may certify that this partition is the solution to the minimum ratio cut problem. Then we develop a deterministic two-to-infinity norm perturbation bound for the the invariant subspace of the graph Laplacian that corresponds to the $k$ smallest eigenvalues. Finally by combining these two results we give a condition under which spectral clustering is guaranteed to output the global solution to the minimum ratio cut problem, which serves as a performance guarantee for spectral clustering.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-10T22:03:43Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>March Boedihardjo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shaofeng Deng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Thomas Strohmer</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05651v2</id>\n",
      "    <title>Deep or Simple Models for Semantic Tagging? It Depends on your Data [Experiments]</title>\n",
      "    <updated>2020-10-08T22:45:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05651v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05651v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Semantic tagging, which has extensive applications in text mining, predicts whether a given piece of text conveys the meaning of a given semantic tag. The problem of semantic tagging is largely solved with supervised learning and today, deep learning models are widely perceived to be better for semantic tagging. However, there is no comprehensive study supporting the popular belief. Practitioners often have to train different types of models for each semantic tagging task to identify the best model. This process is both expensive and inefficient.\n",
      "  We embark on a systematic study to investigate the following question: Are deep models the best performing model for all semantic tagging tasks? To answer this question, we compare deep models against \"simple models\" over datasets with varying characteristics. Specifically, we select three prevalent deep models (i.e. CNN, LSTM, and BERT) and two simple models (i.e. LR and SVM), and compare their performance on the semantic tagging task over 21 datasets. Results show that the size, the label ratio, and the label cleanliness of a dataset significantly impact the quality of semantic tagging. Simple models achieve similar tagging quality to deep models on large datasets, but the runtime of simple models is much shorter. Moreover, simple models can achieve better tagging quality than deep models when targeting datasets show worse label cleanliness and/or more severe imbalance. Based on these findings, our study can systematically guide practitioners in selecting the right learning model for their semantic tagging task.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-11T00:05:50Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Jinfeng Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuliang Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaolan Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wang-Chiew Tan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05432v1</id>\n",
      "    <title>Reactive Soft Prototype Computing for Concept Drift Streams</title>\n",
      "    <updated>2020-07-10T15:07:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05432v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05432v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The amount of real-time communication between agents in an information system has increased rapidly since the beginning of the decade. This is because the use of these systems, e. g. social media, has become commonplace in today's society. This requires analytical algorithms to learn and predict this stream of information in real-time. The nature of these systems is non-static and can be explained, among other things, by the fast pace of trends. This creates an environment in which algorithms must recognize changes and adapt. Recent work shows vital research in the field, but mainly lack stable performance during model adaptation. In this work, a concept drift detection strategy followed by a prototype-based adaptation strategy is proposed. Validated through experimental results on a variety of typical non-static data, our solution provides stable and quick adjustments in times of change.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-10T15:07:46Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>Neurocomputing, 2020, ISSN 0925-2312</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Christoph Raab</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Moritz Heusinger</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Frank-Michael Schleif</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1016/j.neucom.2019.11.111</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1016/j.neucom.2019.11.111\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05441v1</id>\n",
      "    <title>Impression Space from Deep Template Network</title>\n",
      "    <updated>2020-07-10T15:29:33Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05441v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05441v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>It is an innate ability for humans to imagine something only according to their impression, without having to memorize all the details of what they have seen. In this work, we would like to demonstrate that a trained convolutional neural network also has the capability to \"remember\" its input images. To achieve this, we propose a simple but powerful framework to establish an {\\emph{Impression Space}} upon an off-the-shelf pretrained network. This network is referred to as the {\\emph{Template Network}} because its filters will be used as templates to reconstruct images from the impression. In our framework, the impression space and image space are bridged by a layer-wise encoding and iterative decoding process. It turns out that the impression space indeed captures the salient features from images, and it can be directly applied to tasks such as unpaired image translation and image synthesis through impression matching without further network training. Furthermore, the impression naturally constructs a high-level common space for different data. Based on this, we propose a mechanism to model the data relations inside the impression space, which is able to reveal the feature similarity between images. Our code will be released.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-10T15:29:33Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Gongfan Fang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xinchao Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Haofei Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jie Song</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mingli Song</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05446v1</id>\n",
      "    <title>Evaluation of Big Data based CNN Models in Classification of Skin Lesions with Melanoma</title>\n",
      "    <updated>2020-07-10T15:39:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05446v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05446v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This chapter presents a methodology for diagnosis of pigmented skin lesions using convolutional neural networks. The architecture is based on convolu-tional neural networks and it is evaluated using new CNN models as well as re-trained modification of pre-existing CNN models were used. The experi-mental results showed that CNN models pre-trained on big datasets for gen-eral purpose image classification when re-trained in order to identify skin le-sion types offer more accurate results when compared to convolutional neural network models trained explicitly from the dermatoscopic images. The best performance was achieved by re-training a modified version of ResNet-50 convolutional neural network with accuracy equal to 93.89%. Analysis on skin lesion pathology type was also performed with classification accuracy for melanoma and basal cell carcinoma being equal to 79.13% and 82.88%, respectively.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-10T15:39:32Z</published>\n",
      "    <arxiv:comment>Series Title: Studies in Computational Intelligence, Book Title: Deep Learning for Cancer Diagnosis, Series Volume: 908, DOI: 10.1007/978-981-15-6321-8, eBook ISBN: 978-981-15-6321-8</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Prasitthichai Naronglerdrit</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Iosif Mporas</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1007/978-981-15-6321-8</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1007/978-981-15-6321-8\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06271v2</id>\n",
      "    <title>RATT: Recurrent Attention to Transient Tasks for Continual Image Captioning</title>\n",
      "    <updated>2020-10-29T11:20:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06271v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06271v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Research on continual learning has led to a variety of approaches to mitigating catastrophic forgetting in feed-forward classification networks. Until now surprisingly little attention has been focused on continual learning of recurrent models applied to problems like image captioning. In this paper we take a systematic look at continual learning of LSTM-based models for image captioning. We propose an attention-based approach that explicitly accommodates the transient nature of vocabularies in continual image captioning tasks -- i.e. that task vocabularies are not disjoint. We call our method Recurrent Attention to Transient Tasks (RATT), and also show how to adapt continual learning approaches based on weight egularization and knowledge distillation to recurrent continual learning problems. We apply our approaches to incremental image captioning problem on two new continual learning benchmarks we define using the MS-COCO and Flickr30 datasets. Our results demonstrate that RATT is able to sequentially learn five captioning tasks while incurring no forgetting of previously learned ones.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-13T09:52:37Z</published>\n",
      "    <arxiv:comment>9 pages, 4 figures, 8 supplementary pages, 12 supplementary images, to be published in NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Riccardo Del Chiaro</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bartłomiej Twardowski</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrew D. Bagdanov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joost van de Weijer</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06434v1</id>\n",
      "    <title>Towards Automated Neural Interaction Discovery for Click-Through Rate Prediction</title>\n",
      "    <updated>2020-06-29T04:33:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06434v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06434v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Click-Through Rate (CTR) prediction is one of the most important machine learning tasks in recommender systems, driving personalized experience for billions of consumers. Neural architecture search (NAS), as an emerging field, has demonstrated its capabilities in discovering powerful neural network architectures, which motivates us to explore its potential for CTR predictions. Due to 1) diverse unstructured feature interactions, 2) heterogeneous feature space, and 3) high data volume and intrinsic data randomness, it is challenging to construct, search, and compare different architectures effectively for recommendation models. To address these challenges, we propose an automated interaction architecture discovering framework for CTR prediction named AutoCTR. Via modularizing simple yet representative interactions as virtual building blocks and wiring them into a space of direct acyclic graphs, AutoCTR performs evolutionary architecture exploration with learning-to-rank guidance at the architecture level and achieves acceleration using low-fidelity model. Empirical analysis demonstrates the effectiveness of AutoCTR on different datasets comparing to human-crafted architectures. The discovered architecture also enjoys generalizability and transferability among different datasets.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-29T04:33:01Z</published>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Qingquan Song</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dehua Cheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hanning Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiyan Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuandong Tian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xia Hu</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3394486.3403137</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3394486.3403137\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06059v2</id>\n",
      "    <title>It Is Likely That Your Loss Should be a Likelihood</title>\n",
      "    <updated>2020-10-02T14:39:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06059v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06059v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Many common loss functions such as mean-squared-error, cross-entropy, and reconstruction loss are unnecessarily rigid. Under a probabilistic interpretation, these common losses correspond to distributions with fixed shapes and scales. We instead argue for optimizing full likelihoods that include parameters like the normal variance and softmax temperature. Joint optimization of these \"likelihood parameters\" with model parameters can adaptively tune the scales and shapes of losses in addition to the strength of regularization. We explore and systematically evaluate how to parameterize and apply likelihood parameters for robust modeling, outlier-detection, and re-calibration. Additionally, we propose adaptively tuning $L_2$ and $L_1$ weights by fitting the scale parameters of normal and Laplace priors and introduce more flexible element-wise regularizers.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-12T18:25:17Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Mark Hamilton</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Evan Shelhamer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>William T. Freeman</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06063v1</id>\n",
      "    <title>Exploiting Uncertainties from Ensemble Learners to Improve Decision-Making in Healthcare AI</title>\n",
      "    <updated>2020-07-12T18:33:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06063v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06063v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Ensemble learning is widely applied in Machine Learning (ML) to improve model performance and to mitigate decision risks. In this approach, predictions from a diverse set of learners are combined to obtain a joint decision. Recently, various methods have been explored in literature for estimating decision uncertainties using ensemble learning; however, determining which metrics are a better fit for certain decision-making applications remains a challenging task. In this paper, we study the following key research question in the selection of uncertainty metrics: when does an uncertainty metric outperforms another? We answer this question via a rigorous analysis of two commonly used uncertainty metrics in ensemble learning, namely ensemble mean and ensemble variance. We show that, under mild assumptions on the ensemble learners, ensemble mean is preferable with respect to ensemble variance as an uncertainty metric for decision making. We empirically validate our assumptions and theoretical results via an extensive case study: the diagnosis of referable diabetic retinopathy.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-12T18:33:09Z</published>\n",
      "    <arxiv:comment>Preprint of submission to NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yingshui Tan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Baihong Jin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiangyu Yue</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuxin Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alberto Sangiovanni Vincentelli</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06120v2</id>\n",
      "    <title>Fisher Auto-Encoders</title>\n",
      "    <updated>2020-10-23T12:45:31Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06120v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06120v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>It has been conjectured that the Fisher divergence is more robust to model uncertainty than the conventional Kullback-Leibler (KL) divergence. This motivates the design of a new class of robust generative auto-encoders (AE) referred to as Fisher auto-encoders. Our approach is to design Fisher AEs by minimizing the Fisher divergence between the intractable joint distribution of observed data and latent variables, with that of the postulated/modeled joint distribution. In contrast to KL-based variational AEs (VAEs), the Fisher AE can exactly quantify the distance between the true and the model-based posterior distributions. Qualitative and quantitative results are provided on both MNIST and celebA datasets demonstrating the competitive performance of Fisher AEs in terms of robustness compared to other AEs such as VAEs and Wasserstein AEs.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-12T22:43:20Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Khalil Elkhalil</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ali Hasan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jie Ding</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sina Farsiu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vahid Tarokh</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06153v1</id>\n",
      "    <title>AI Playground: Unreal Engine-based Data Ablation Tool for Deep Learning</title>\n",
      "    <updated>2020-07-13T02:04:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06153v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06153v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Machine learning requires data, but acquiring and labeling real-world data is challenging, expensive, and time-consuming. More importantly, it is nearly impossible to alter real data post-acquisition (e.g., change the illumination of a room), making it very difficult to measure how specific properties of the data affect performance. In this paper, we present AI Playground (AIP), an open-source, Unreal Engine-based tool for generating and labeling virtual image data. With AIP, it is trivial to capture the same image under different conditions (e.g., fidelity, lighting, etc.) and with different ground truths (e.g., depth or surface normal values). AIP is easily extendable and can be used with or without code. To validate our proposed tool, we generated eight datasets of otherwise identical but varying lighting and fidelity conditions. We then trained deep neural networks to predict (1) depth values, (2) surface normals, or (3) object labels and assessed each network's intra- and cross-dataset performance. Among other insights, we verified that sensitivity to different settings is problem-dependent. We confirmed the findings of other studies that segmentation models are very sensitive to fidelity, but we also found that they are just as sensitive to lighting. In contrast, depth and normal estimation models seem to be less sensitive to fidelity or lighting and more sensitive to the structure of the image. Finally, we tested our trained depth-estimation networks on two real-world datasets and obtained results comparable to training on real data alone, confirming that our virtual environments are realistic enough for real-world tasks.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-13T02:04:39Z</published>\n",
      "    <arxiv:comment>14 pages, 7 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Mehdi Mousavi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aashis Khanal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rolando Estrada</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06183v2</id>\n",
      "    <title>Data-driven geophysics: from dictionary learning to deep learning</title>\n",
      "    <updated>2020-09-29T04:12:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06183v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06183v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Understanding the principles of geophysical phenomena is an essential and challenging task. \"Model-driven\" approaches have supported the development of geophysics for a long time; however, such methods suffer from the curse of dimensionality and may inaccurately model the subsurface. \"Data-driven\" techniques may overcome these issues with increasingly available geophysical data. In this article, we review the basic concepts of and recent advances in data-driven approaches from dictionary learning to deep learning in a variety of geophysical scenarios. Explorational geophysics including data processing, inversion and interpretation will be mainly focused. Artificial intelligence applications on geoscience involving deep Earth, earthquake, water resource, atmospheric science, satellite remoe sensing and space sciences are also reviewed. We present a coding tutorial and a summary of tips for beginners and interested geophysical readers to rapidly explore deep learning. Some promising directions are provided for future research involving deep learning in geophysics, such as unsupervised learning, transfer learning, multimodal deep learning, federated learning, uncertainty estimation, and activate learning.</summary>\n",
      "    <category term=\"physics.geo-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-13T04:39:49Z</published>\n",
      "    <arxiv:primary_category term=\"physics.geo-ph\"/>\n",
      "    <author>\n",
      "      <name>Siwei Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jianwei Ma</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06184v1</id>\n",
      "    <title>Efficient Planning in Large MDPs with Weak Linear Function Approximation</title>\n",
      "    <updated>2020-07-13T04:40:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06184v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06184v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Large-scale Markov decision processes (MDPs) require planning algorithms with runtime independent of the number of states of the MDP. We consider the planning problem in MDPs using linear value function approximation with only weak requirements: low approximation error for the optimal value function, and a small set of \"core\" states whose features span those of other states. In particular, we make no assumptions about the representability of policies or value functions of non-optimal policies. Our algorithm produces almost-optimal actions for any state using a generative oracle (simulator) for the MDP, while its computation time scales polynomially with the number of features, core states, and actions and the effective horizon.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-13T04:40:41Z</published>\n",
      "    <arxiv:comment>12 pages and appendix (10 pages). Submitted to the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Roshan Shariff</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Csaba Szepesvári</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06196v1</id>\n",
      "    <title>Data from Model: Extracting Data from Non-robust and Robust Models</title>\n",
      "    <updated>2020-07-13T05:27:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06196v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06196v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The essence of deep learning is to exploit data to train a deep neural network (DNN) model. This work explores the reverse process of generating data from a model, attempting to reveal the relationship between the data and the model. We repeat the process of Data to Model (DtM) and Data from Model (DfM) in sequence and explore the loss of feature mapping information by measuring the accuracy drop on the original validation dataset. We perform this experiment for both a non-robust and robust origin model. Our results show that the accuracy drop is limited even after multiple sequences of DtM and DfM, especially for robust models. The success of this cycling transformation can be attributed to the shared feature mapping existing in data and model. Using the same data, we observe that different DtM processes result in models having different features, especially for different network architecture families, even though they achieve comparable performance.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-13T05:27:48Z</published>\n",
      "    <arxiv:comment>Accepted at the CVPR 2020 Workshop on Adversarial Machine Learning in Computer Vision</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Philipp Benz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chaoning Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tooba Imtiaz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>In-So Kweon</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06207v1</id>\n",
      "    <title>DinerDash Gym: A Benchmark for Policy Learning in High-Dimensional Action Space</title>\n",
      "    <updated>2020-07-13T06:22:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06207v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06207v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>It has been arduous to assess the progress of a policy learning algorithm in the domain of hierarchical task with high dimensional action space due to the lack of a commonly accepted benchmark. In this work, we propose a new light-weight benchmark task called Diner Dash for evaluating the performance in a complicated task with high dimensional action space. In contrast to the traditional Atari games that only have a flat structure of goals and very few actions, the proposed benchmark task has a hierarchical task structure and size of 57 for the action space and hence can facilitate the development of policy learning in complicated tasks. On top of that, we introduce Decomposed Policy Graph Modelling (DPGM), an algorithm that combines both graph modelling and deep learning to allow explicit domain knowledge embedding and achieves significant improvement comparing to the baseline. In the experiments, we have shown the effectiveness of the domain knowledge injection via a specially designed imitation algorithm as well as results of other popular algorithms.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-13T06:22:55Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Siwei Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiao Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Hsu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06240v1</id>\n",
      "    <title>Expert Training: Task Hardness Aware Meta-Learning for Few-Shot Classification</title>\n",
      "    <updated>2020-07-13T08:49:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06240v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06240v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep neural networks are highly effective when a large number of labeled samples are available but fail with few-shot classification tasks. Recently, meta-learning methods have received much attention, which train a meta-learner on massive additional tasks to gain the knowledge to instruct the few-shot classification. Usually, the training tasks are randomly sampled and performed indiscriminately, often making the meta-learner stuck into a bad local optimum. Some works in the optimization of deep neural networks have shown that a better arrangement of training data can make the classifier converge faster and perform better. Inspired by this idea, we propose an easy-to-hard expert meta-training strategy to arrange the training tasks properly, where easy tasks are preferred in the first phase, then, hard tasks are emphasized in the second phase. A task hardness aware module is designed and integrated into the training procedure to estimate the hardness of a task based on the distinguishability of its categories. In addition, we explore multiple hardness measurements including the semantic relation, the pairwise Euclidean distance, the Hausdorff distance, and the Hilbert-Schmidt independence criterion. Experimental results on the miniImageNet and tieredImageNetSketch datasets show that the meta-learners can obtain better results with our expert training strategy.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-13T08:49:00Z</published>\n",
      "    <arxiv:comment>9 pages, 6 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Yucan Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jianfei Cai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qinghua Hu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Weiping Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05683v1</id>\n",
      "    <title>Batch-level Experience Replay with Review for Continual Learning</title>\n",
      "    <updated>2020-07-11T05:20:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05683v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05683v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Continual learning is a branch of deep learning that seeks to strike a balance between learning stability and plasticity. The CVPR 2020 CLVision Continual Learning for Computer Vision challenge is dedicated to evaluating and advancing the current state-of-the-art continual learning methods using the CORe50 dataset with three different continual learning scenarios. This paper presents our approach, called Batch-level Experience Replay with Review, to this challenge. Our team achieved the 1'st place in all three scenarios out of 79 participated teams. The codebase of our implementation is publicly available at https://github.com/RaptorMai/CVPR20_CLVision_challenge</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-11T05:20:09Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Zheda Mai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hyunwoo Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jihwan Jeong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Scott Sanner</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05828v1</id>\n",
      "    <title>Understanding Object Detection Through An Adversarial Lens</title>\n",
      "    <updated>2020-07-11T18:41:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05828v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05828v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep neural networks based object detection models have revolutionized computer vision and fueled the development of a wide range of visual recognition applications. However, recent studies have revealed that deep object detectors can be compromised under adversarial attacks, causing a victim detector to detect no object, fake objects, or mislabeled objects. With object detection being used pervasively in many security-critical applications, such as autonomous vehicles and smart cities, we argue that a holistic approach for an in-depth understanding of adversarial attacks and vulnerabilities of deep object detection systems is of utmost importance for the research community to develop robust defense mechanisms. This paper presents a framework for analyzing and evaluating vulnerabilities of the state-of-the-art object detectors under an adversarial lens, aiming to analyze and demystify the attack strategies, adverse effects, and costs, as well as the cross-model and cross-resolution transferability of attacks. Using a set of quantitative metrics, extensive experiments are performed on six representative deep object detectors from three popular families (YOLOv3, SSD, and Faster R-CNN) with two benchmark datasets (PASCAL VOC and MS COCO). We demonstrate that the proposed framework can serve as a methodical benchmark for analyzing adversarial behaviors and risks in real-time object detection systems. We conjecture that this framework can also serve as a tool to assess the security risks and the adversarial robustness of deep object detectors to be deployed in real-world applications.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-11T18:41:47Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Ka-Ho Chow</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ling Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mehmet Emre Gursoy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stacey Truex</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wenqi Wei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yanzhao Wu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05120v1</id>\n",
      "    <title>Development and Validation of a Novel Prognostic Model for Predicting AMD Progression Using Longitudinal Fundus Images</title>\n",
      "    <updated>2020-07-10T00:33:19Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05120v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05120v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Prognostic models aim to predict the future course of a disease or condition and are a vital component of personalized medicine. Statistical models make use of longitudinal data to capture the temporal aspect of disease progression; however, these models require prior feature extraction. Deep learning avoids explicit feature extraction, meaning we can develop models for images where features are either unknown or impossible to quantify accurately. Previous prognostic models using deep learning with imaging data require annotation during training or only utilize a single time point. We propose a novel deep learning method to predict the progression of diseases using longitudinal imaging data with uneven time intervals, which requires no prior feature extraction. Given previous images from a patient, our method aims to predict whether the patient will progress onto the next stage of the disease. The proposed method uses InceptionV3 to produce feature vectors for each image. In order to account for uneven intervals, a novel interval scaling is proposed. Finally, a Recurrent Neural Network is used to prognosticate the disease. We demonstrate our method on a longitudinal dataset of color fundus images from 4903 eyes with age-related macular degeneration (AMD), taken from the Age-Related Eye Disease Study, to predict progression to late AMD. Our method attains a testing sensitivity of 0.878, a specificity of 0.887, and an area under the receiver operating characteristic of 0.950. We compare our method to previous methods, displaying superior performance in our model. Class activation maps display how the network reaches the final decision.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-10T00:33:19Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Joshua Bridge</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Simon P. Harding</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yalin Zheng</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05089v1</id>\n",
      "    <title>The Trade-Offs of Private Prediction</title>\n",
      "    <updated>2020-07-09T22:02:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05089v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05089v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Machine learning models leak information about their training data every time they reveal a prediction. This is problematic when the training data needs to remain private. Private prediction methods limit how much information about the training data is leaked by each prediction. Private prediction can also be achieved using models that are trained by private training methods. In private prediction, both private training and private prediction methods exhibit trade-offs between privacy, privacy failure probability, amount of training data, and inference budget. Although these trade-offs are theoretically well-understood, they have hardly been studied empirically. This paper presents the first empirical study into the trade-offs of private prediction. Our study sheds light on which methods are best suited for which learning setting. Perhaps surprisingly, we find private training methods outperform private prediction methods in a wide range of private prediction settings.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-09T22:02:37Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Laurens van der Maaten</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Awni Hannun</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05100v2</id>\n",
      "    <title>SGQuant: Squeezing the Last Bit on Graph Neural Networks with Specialized Quantization</title>\n",
      "    <updated>2020-09-16T07:13:58Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05100v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05100v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>With the increasing popularity of graph-based learning, Graph Neural Networks (GNNs) win lots of attention from the research and industry field because of their high accuracy. However, existing GNNs suffer from high memory footprints (e.g., node embedding features). This high memory footprint hurdles the potential applications towards memory-constrained devices, such as the widely-deployed IoT devices. To this end, we propose a specialized GNN quantization scheme, SGQuant, to systematically reduce the GNN memory consumption. Specifically, we first propose a GNN-tailored quantization algorithm design and a GNN quantization fine-tuning scheme to reduce memory consumption while maintaining accuracy. Then, we investigate the multi-granularity quantization strategy that operates at different levels (components, graph topology, and layers) of GNN computation. Moreover, we offer an automatic bit-selecting (ABS) to pinpoint the most appropriate quantization bits for the above multi-granularity quantizations. Intensive experiments show that SGQuant can effectively reduce the memory footprint from 4.25x to 31.9x compared with the original full-precision GNNs while limiting the accuracy drop to 0.4% on average.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-09T22:42:34Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Boyuan Feng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuke Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xu Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shu Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xueqiao Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yufei Ding</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.05104v1</id>\n",
      "    <title>$n$-Reference Transfer Learning for Saliency Prediction</title>\n",
      "    <updated>2020-07-09T23:20:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.05104v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.05104v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Benefiting from deep learning research and large-scale datasets, saliency prediction has achieved significant success in the past decade. However, it still remains challenging to predict saliency maps on images in new domains that lack sufficient data for data-hungry models. To solve this problem, we propose a few-shot transfer learning paradigm for saliency prediction, which enables efficient transfer of knowledge learned from the existing large-scale saliency datasets to a target domain with limited labeled examples. Specifically, very few target domain examples are used as the reference to train a model with a source domain dataset such that the training process can converge to a local minimum in favor of the target domain. Then, the learned model is further fine-tuned with the reference. The proposed framework is gradient-based and model-agnostic. We conduct comprehensive experiments and ablation study on various source domain and target domain pairs. The results show that the proposed framework achieves a significant performance improvement. The code is publicly available at \\url{https://github.com/luoyan407/n-reference}.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-09T23:20:44Z</published>\n",
      "    <arxiv:comment>ECCV 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Yan Luo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yongkang Wong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mohan S. Kankanhalli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qi Zhao</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.08092v1</id>\n",
      "    <title>Using LSTM and SARIMA Models to Forecast Cluster CPU Usage</title>\n",
      "    <updated>2020-07-16T03:29:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.08092v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.08092v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>As large scale cloud computing centers become more popular than individual servers, predicting future resource demand need has become an important problem. Forecasting resource need allows public cloud providers to proactively allocate or deallocate resources for cloud services. This work seeks to predict one resource, CPU usage, over both a short term and long term time scale.\n",
      "  To gain insight into the model characteristics that best support specific tasks, we consider two vastly different architectures: the historically relevant SARIMA model and the more modern neural network, LSTM model. We apply these models to Azure data resampled to 20 minutes per data point with the goal of predicting usage over the next hour for the short-term task and for the next three days for the long-term task. The SARIMA model outperformed the LSTM for the long term prediction task, but performed poorer on the short term task. Furthermore, the LSTM model was more robust, whereas the SARIMA model relied on the data meeting certain assumptions about seasonality.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-16T03:29:13Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Langston Nashold</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rayan Krishnan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.08095v2</id>\n",
      "    <title>Synthesize, Execute and Debug: Learning to Repair for Neural Program Synthesis</title>\n",
      "    <updated>2020-10-22T07:23:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.08095v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.08095v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The use of deep learning techniques has achieved significant progress for program synthesis from input-output examples. However, when the program semantics become more complex, it still remains a challenge to synthesize programs that are consistent with the specification. In this work, we propose SED, a neural program generation framework that incorporates synthesis, execution, and debugging stages. Instead of purely relying on the neural program synthesizer to generate the final program, SED first produces initial programs using the neural program synthesizer component, then utilizes a neural program debugger to iteratively repair the generated programs. The integration of the debugger component enables SED to modify the programs based on the execution results and specification, which resembles the coding process of human programmers. On Karel, a challenging input-output program synthesis benchmark, SED reduces the error rate of the neural program synthesizer itself by a considerable margin, and outperforms the standard beam search for decoding.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-16T04:15:47Z</published>\n",
      "    <arxiv:comment>Published in NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Kavi Gupta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peter Ebert Christensen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xinyun Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dawn Song</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.07677v1</id>\n",
      "    <title>Fast Differentiable Clipping-Aware Normalization and Rescaling</title>\n",
      "    <updated>2020-07-15T13:43:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.07677v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.07677v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Rescaling a vector $\\vecδ \\in \\mathbb{R}^n$ to a desired length is a common operation in many areas such as data science and machine learning. When the rescaled perturbation $η\\vecδ$ is added to a starting point $\\vec{x} \\in D$ (where $D$ is the data domain, e.g. $D = [0, 1]^n$), the resulting vector $\\vec{v} = \\vec{x} + η\\vecδ$ will in general not be in $D$. To enforce that the perturbed vector $v$ is in $D$, the values of $\\vec{v}$ can be clipped to $D$. This subsequent element-wise clipping to the data domain does however reduce the effective perturbation size and thus interferes with the rescaling of $\\vecδ$. The optimal rescaling $η$ to obtain a perturbation with the desired norm after the clipping can be iteratively approximated using a binary search. However, such an iterative approach is slow and non-differentiable. Here we show that the optimal rescaling can be found analytically using a fast and differentiable algorithm. Our algorithm works for any p-norm and can be used to train neural networks on inputs with normalized perturbations. We provide native implementations for PyTorch, TensorFlow, JAX, and NumPy based on EagerPy.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-15T13:43:22Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jonas Rauber</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matthias Bethge</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.07682v2</id>\n",
      "    <title>FetchSGD: Communication-Efficient Federated Learning with Sketching</title>\n",
      "    <updated>2020-10-08T00:37:29Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.07682v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.07682v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Existing approaches to federated learning suffer from a communication bottleneck as well as convergence issues due to sparse client participation. In this paper we introduce a novel algorithm, called FetchSGD, to overcome these challenges. FetchSGD compresses model updates using a Count Sketch, and then takes advantage of the mergeability of sketches to combine model updates from many workers. A key insight in the design of FetchSGD is that, because the Count Sketch is linear, momentum and error accumulation can both be carried out within the sketch. This allows the algorithm to move momentum and error accumulation from clients to the central aggregator, overcoming the challenges of sparse client participation while still achieving high compression rates and good convergence. We prove that FetchSGD has favorable convergence guarantees, and we demonstrate its empirical effectiveness by training two residual networks and a transformer model.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-15T13:46:34Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Daniel Rothchild</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ashwinee Panda</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Enayat Ullah</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nikita Ivkin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ion Stoica</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vladimir Braverman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joseph Gonzalez</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Raman Arora</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.07740v1</id>\n",
      "    <title>Deep Representation Learning and Clustering of Traffic Scenarios</title>\n",
      "    <updated>2020-07-15T15:12:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.07740v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.07740v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Determining the traffic scenario space is a major challenge for the homologation and coverage assessment of automated driving functions. In contrast to current approaches that are mainly scenario-based and rely on expert knowledge, we introduce two data driven autoencoding models that learn a latent representation of traffic scenes. First is a CNN based spatio-temporal model that autoencodes a grid of traffic participants' positions. Secondly, we develop a pure temporal RNN based model that auto-encodes a sequence of sets. To handle the unordered set data, we had to incorporate the permutation invariance property. Finally, we show how the latent scenario embeddings can be used for clustering traffic scenarios and similarity retrieval.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-15T15:12:23Z</published>\n",
      "    <arxiv:comment>Workshop on AI for Autonomous Driving, International Conference on Machine Learning (ICML) 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Nick Harmening</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marin Biloš</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stephan Günnemann</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.07757v2</id>\n",
      "    <title>Two-Level Adversarial Visual-Semantic Coupling for Generalized Zero-shot Learning</title>\n",
      "    <updated>2020-11-30T11:00:45Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.07757v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.07757v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The performance of generative zero-shot methods mainly depends on the quality of generated features and how well the model facilitates knowledge transfer between visual and semantic domains. The quality of generated features is a direct consequence of the ability of the model to capture the several modes of the underlying data distribution. To address these issues, we propose a new two-level joint maximization idea to augment the generative network with an inference network during training which helps our model capture the several modes of the data and generate features that better represent the underlying data distribution. This provides strong cross-modal interaction for effective transfer of knowledge between visual and semantic domains. Furthermore, existing methods train the zero-shot classifier either on generate synthetic image features or latent embeddings produced by leveraging representation learning. In this work, we unify these paradigms into a single model which in addition to synthesizing image features, also utilizes the representation learning capabilities of the inference network to provide discriminative features for the final zero-shot recognition task. We evaluate our approach on four benchmark datasets i.e. CUB, FLO, AWA1 and AWA2 against several state-of-the-art methods, and show its performance. We also perform ablation studies to analyze and understand our method more carefully for the Generalized Zero-shot Learning task.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-15T15:34:09Z</published>\n",
      "    <arxiv:comment>Under Submission</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Shivam Chandhok</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vineeth N Balasubramanian</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.08501v1</id>\n",
      "    <title>Accelerating 3D Deep Learning with PyTorch3D</title>\n",
      "    <updated>2020-07-16T17:53:02Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.08501v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.08501v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep learning has significantly improved 2D image recognition. Extending into 3D may advance many new applications including autonomous vehicles, virtual and augmented reality, authoring 3D content, and even improving 2D recognition. However despite growing interest, 3D deep learning remains relatively underexplored. We believe that some of this disparity is due to the engineering challenges involved in 3D deep learning, such as efficiently processing heterogeneous data and reframing graphics operations to be differentiable. We address these challenges by introducing PyTorch3D, a library of modular, efficient, and differentiable operators for 3D deep learning. It includes a fast, modular differentiable renderer for meshes and point clouds, enabling analysis-by-synthesis approaches. Compared with other differentiable renderers, PyTorch3D is more modular and efficient, allowing users to more easily extend it while also gracefully scaling to large meshes and images. We compare the PyTorch3D operators and renderer with other implementations and demonstrate significant speed and memory improvements. We also use PyTorch3D to improve the state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D images on ShapeNet. PyTorch3D is open-source and we hope it will help accelerate research in 3D deep learning.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-16T17:53:02Z</published>\n",
      "    <arxiv:comment>tech report</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Nikhila Ravi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jeremy Reizenstein</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Novotny</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Taylor Gordon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wan-Yen Lo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Justin Johnson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Georgia Gkioxari</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.08517v1</id>\n",
      "    <title>Detecting Deepfake Videos: An Analysis of Three Techniques</title>\n",
      "    <updated>2020-07-15T20:36:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.08517v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.08517v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent advances in deepfake generating algorithms that produce manipulated media have had dangerous implications in privacy, security and mass communication. Efforts to combat this issue have risen in the form of competitions and funding for research to detect deepfakes. This paper presents three techniques and algorithms: convolutional LSTM, eye blink detection and grayscale histograms-pursued while participating in the Deepfake Detection Challenge. We assessed the current knowledge about deepfake videos, a more severe version of manipulated media, and previous methods used, and found relevance in the grayscale histogram technique over others. We discussed the implications of each method developed and provided further steps to improve the given findings.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-15T20:36:23Z</published>\n",
      "    <arxiv:comment>11 pages, 8 figures, 2 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Armaan Pishori</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Brittany Rollins</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nicolas van Houten</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nisha Chatwani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Omar Uraimov</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.08520v2</id>\n",
      "    <title>Accelerating Robustness Verification of Deep Neural Networks Guided by Target Labels</title>\n",
      "    <updated>2020-07-27T00:04:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.08520v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.08520v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep Neural Networks (DNNs) have become key components of many safety-critical applications such as autonomous driving and medical diagnosis. However, DNNs have been shown suffering from poor robustness because of their susceptibility to adversarial examples such that small perturbations to an input result in misprediction. Addressing to this concern, various approaches have been proposed to formally verify the robustness of DNNs. Most of these approaches reduce the verification problem to optimization problems of searching an adversarial example for a given input so that it is not correctly classified to the original label. However, they are limited in accuracy and scalability. In this paper, we propose a novel approach that can accelerate the robustness verification techniques by guiding the verification with target labels. The key insight of our approach is that the robustness verification problem of DNNs can be solved by verifying sub-problems of DNNs, one per target label. Fixing the target label during verification can drastically reduce the search space and thus improve the efficiency. We also propose an approach by leveraging symbolic interval propagation and linear relaxation techniques to sort the target labels in terms of chances that adversarial examples exist. This often allows us to quickly falsify the robustness of DNNs and the verification for remaining target labels could be avoided. Our approach is orthogonal to, and can be integrated with, many existing verification techniques. For evaluation purposes, we integrate it with three recent promising DNN verification tools, i.e., MipVerify, DeepZ, and Neurify. Experimental results show that our approach can significantly improve these tools by 36X speedup when the perturbation distance is set in a reasonable range.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-16T00:51:52Z</published>\n",
      "    <arxiv:comment>20 pages, 3 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Wenjie Wan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhaodi Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yiwei Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Min Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fu Song</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.08557v1</id>\n",
      "    <title>Unsupervised Text Generation by Learning from Search</title>\n",
      "    <updated>2020-07-09T04:34:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.08557v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.08557v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this work, we present TGLS, a novel framework to unsupervised Text Generation by Learning from Search. We start by applying a strong search algorithm (in particular, simulated annealing) towards a heuristically defined objective that (roughly) estimates the quality of sentences. Then, a conditional generative model learns from the search results, and meanwhile smooth out the noise of search. The alternation between search and learning can be repeated for performance bootstrapping. We demonstrate the effectiveness of TGLS on two real-world natural language generation tasks, paraphrase generation and text formalization. Our model significantly outperforms unsupervised baseline methods in both tasks. Especially, it achieves comparable performance with the state-of-the-art supervised methods in paraphrase generation.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-09T04:34:48Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Jingjing Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zichao Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lili Mou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xin Jiang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michael R. Lyu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Irwin King</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.08649v1</id>\n",
      "    <title>Graph Neural Networks for Node-Level Predictions</title>\n",
      "    <updated>2020-06-22T11:57:03Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.08649v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.08649v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The success of deep learning has revolutionized many fields of research including areas of computer vision, text and speech processing. Enormous research efforts have led to numerous methods that are capable of efficiently analyzing data, especially in the Euclidean space. However, many problems are posed in non-Euclidean domains modeled as general graphs with complex connection patterns. Increased problem complexity and computational power constraints have limited early approaches to static and small-sized graphs. In recent years, a rising interest in machine learning on graph-structured data has been accompanied by improved methods that overcome the limitations of their predecessors. These methods paved the way for dealing with large-scale and time-dynamic graphs. This work aims to provide an overview of early and modern graph neural network based machine learning methods for node-level prediction tasks. Under the umbrella of taxonomies already established in the literature, we explain the core concepts and provide detailed explanations for convolutional methods that have had strong impact. In addition, we introduce common benchmarks and present selected applications from various areas. Finally, we discuss open problems for further research.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-22T11:57:03Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Christoph Heindl</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06869v1</id>\n",
      "    <title>Robust Identifiability in Linear Structural Equation Models of Causal Inference</title>\n",
      "    <updated>2020-07-14T07:32:36Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06869v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06869v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this work, we consider the problem of robust parameter estimation from observational data in the context of linear structural equation models (LSEMs). LSEMs are a popular and well-studied class of models for inferring causality in the natural and social sciences. One of the main problems related to LSEMs is to recover the model parameters from the observational data. Under various conditions on LSEMs and the model parameters the prior work provides efficient algorithms to recover the parameters. However, these results are often about generic identifiability. In practice, generic identifiability is not sufficient and we need robust identifiability: small changes in the observational data should not affect the parameters by a huge amount. Robust identifiability has received far less attention and remains poorly understood. Sankararaman et al. (2019) recently provided a set of sufficient conditions on parameters under which robust identifiability is feasible. However, a limitation of their work is that their results only apply to a small sub-class of LSEMs, called ``bow-free paths.'' In this work, we significantly extend their work along multiple dimensions. First, for a large and well-studied class of LSEMs, namely ``bow free'' models, we provide a sufficient condition on model parameters under which robust identifiability holds, thereby removing the restriction of paths required by prior work. We then show that this sufficient condition holds with high probability which implies that for a large set of parameters robust identifiability holds and that for such parameters, existing algorithms already achieve robust identifiability. Finally, we validate our results on both simulated and real-world datasets.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-14T07:32:36Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Karthik Abinav Sankararaman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anand Louis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Navin Goyal</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06918v1</id>\n",
      "    <title>Lifelong Learning using Eigentasks: Task Separation, Skill Acquisition, and Selective Transfer</title>\n",
      "    <updated>2020-07-14T09:06:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06918v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06918v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We introduce the eigentask framework for lifelong learning. An eigentask is a pairing of a skill that solves a set of related tasks, paired with a generative model that can sample from the skill's input space. The framework extends generative replay approaches, which have mainly been used to avoid catastrophic forgetting, to also address other lifelong learning goals such as forward knowledge transfer. We propose a wake-sleep cycle of alternating task learning and knowledge consolidation for learning in our framework, and instantiate it for lifelong supervised learning and lifelong RL. We achieve improved performance over the state-of-the-art in supervised continual learning, and show evidence of forward knowledge transfer in a lifelong RL application in the game Starcraft2.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-14T09:06:13Z</published>\n",
      "    <arxiv:comment>Accepted at the 4th Lifelong Machine Learning Workshop at the Thirty-seventh International Conference on Machine Learning (ICML) 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Aswin Raghavan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jesse Hostetler</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Indranil Sur</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abrar Rahman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ajay Divakaran</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.07011v2</id>\n",
      "    <title>Lifelong Policy Gradient Learning of Factored Policies for Faster Training Without Forgetting</title>\n",
      "    <updated>2020-10-21T20:36:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.07011v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.07011v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Policy gradient methods have shown success in learning control policies for high-dimensional dynamical systems. Their biggest downside is the amount of exploration they require before yielding high-performing policies. In a lifelong learning setting, in which an agent is faced with multiple consecutive tasks over its lifetime, reusing information from previously seen tasks can substantially accelerate the learning of new tasks. We provide a novel method for lifelong policy gradient learning that trains lifelong function approximators directly via policy gradients, allowing the agent to benefit from accumulated knowledge throughout the entire training process. We show empirically that our algorithm learns faster and converges to better policies than single-task and lifelong learning baselines, and completely avoids catastrophic forgetting on a variety of challenging domains.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-14T13:05:42Z</published>\n",
      "    <arxiv:comment>To appear in Advances in Neural Information Processing Systems 33 (NeurIPS-20)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jorge A. Mendez</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Boyu Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eric Eaton</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.07170v2</id>\n",
      "    <title>Goal-Aware Prediction: Learning to Model What Matters</title>\n",
      "    <updated>2020-08-10T23:15:15Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.07170v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.07170v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Learned dynamics models combined with both planning and policy learning algorithms have shown promise in enabling artificial agents to learn to perform many diverse tasks with limited supervision. However, one of the fundamental challenges in using a learned forward dynamics model is the mismatch between the objective of the learned model (future state reconstruction), and that of the downstream planner or policy (completing a specified task). This issue is exacerbated by vision-based control tasks in diverse real-world environments, where the complexity of the real world dwarfs model capacity. In this paper, we propose to direct prediction towards task relevant information, enabling the model to be aware of the current task and encouraging it to only model relevant quantities of the state space, resulting in a learning objective that more closely matches the downstream task. Further, we do so in an entirely self-supervised manner, without the need for a reward function or image labels. We find that our method more effectively models the relevant parts of the scene conditioned on the goal, and as a result outperforms standard task-agnostic dynamics models and model-free reinforcement learning.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-14T16:42:59Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Suraj Nair</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Silvio Savarese</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chelsea Finn</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.08337v1</id>\n",
      "    <title>Transferred Energy Management Strategies for Hybrid Electric Vehicles Based on Driving Conditions Recognition</title>\n",
      "    <updated>2020-07-16T13:57:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.08337v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.08337v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Energy management strategies (EMSs) are the most significant components in hybrid electric vehicles (HEVs) because they decide the potential of energy conservation and emission reduction. This work presents a transferred EMS for a parallel HEV via combining the reinforcement learning method and driving conditions recognition. First, the Markov decision process (MDP) and the transition probability matrix are utilized to differentiate the driving conditions. Then, reinforcement learning algorithms are formulated to achieve power split controls, in which Q-tables are tuned by current driving situations. Finally, the proposed transferred framework is estimated and validated in a parallel hybrid topology. Its advantages in computational efficiency and fuel economy are summarized and proved.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-16T13:57:46Z</published>\n",
      "    <arxiv:comment>6 pages, 5 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Teng Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaolin Tang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiaxin Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hong Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wenhao Tan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yalian Yang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.08459v2</id>\n",
      "    <title>PC-PG: Policy Cover Directed Exploration for Provable Policy Gradient Learning</title>\n",
      "    <updated>2020-08-13T17:59:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.08459v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.08459v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Direct policy gradient methods for reinforcement learning are a successful approach for a variety of reasons: they are model free, they directly optimize the performance metric of interest, and they allow for richly parameterized policies. Their primary drawback is that, by being local in nature, they fail to adequately explore the environment. In contrast, while model-based approaches and Q-learning directly handle exploration through the use of optimism, their ability to handle model misspecification and function approximation is far less evident. This work introduces the the Policy Cover-Policy Gradient (PC-PG) algorithm, which provably balances the exploration vs. exploitation tradeoff using an ensemble of learned policies (the policy cover). PC-PG enjoys polynomial sample complexity and run time for both tabular MDPs and, more generally, linear MDPs in an infinite dimensional RKHS. Furthermore, PC-PG also has strong guarantees under model misspecification that go beyond the standard worst case $\\ell_{\\infty}$ assumptions; this includes approximation guarantees for state aggregation under an average case error assumption, along with guarantees under a more general assumption where the approximation error under distribution shift is controlled. We complement the theory with empirical evaluation across a variety of domains in both reward-free and reward-driven settings.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-16T16:57:41Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Alekh Agarwal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mikael Henaff</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sham Kakade</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wen Sun</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.08463v1</id>\n",
      "    <title>openDD: A Large-Scale Roundabout Drone Dataset</title>\n",
      "    <updated>2020-07-16T17:01:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.08463v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.08463v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Analyzing and predicting the traffic scene around the ego vehicle has been one of the key challenges in autonomous driving. Datasets including the trajectories of all road users present in a scene, as well as the underlying road topology are invaluable to analyze the behavior of the different traffic participants. The interaction between the various traffic participants is especially high in intersection types that are not regulated by traffic lights, the most common one being the roundabout.\n",
      "  We introduce the openDD dataset, including 84,774 accurately tracked trajectories and HD map data of seven different roundabouts. The openDD dataset is annotated using images taken by a drone in 501 separate flights, totalling in over 62 hours of trajectory data. As of today, openDD is by far the largest publicly available trajectory dataset recorded from a drone perspective, while comparable datasets span 17 hours at most.\n",
      "  The data is available, for both commercial and noncommercial use, at: http://www.l3pilot.eu/openDD.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-16T17:01:44Z</published>\n",
      "    <arxiv:comment>ITSC 2020 Conference Paper</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Antonia Breuer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jan-Aike Termöhlen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Silviu Homoceanu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tim Fingscheidt</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.06634v1</id>\n",
      "    <title>Deep Doubly Supervised Transfer Network for Diagnosis of Breast Cancer with Imbalanced Ultrasound Imaging Modalities</title>\n",
      "    <updated>2020-06-29T07:32:07Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.06634v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.06634v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Elastography ultrasound (EUS) provides additional bio-mechanical in-formation about lesion for B-mode ultrasound (BUS) in the diagnosis of breast cancers. However, joint utilization of both BUS and EUS is not popular due to the lack of EUS devices in rural hospitals, which arouses a novel modality im-balance problem in computer-aided diagnosis (CAD) for breast cancers. Current transfer learning (TL) pay little attention to this special issue of clinical modality imbalance, that is, the source domain (EUS modality) has fewer labeled samples than those in the target domain (BUS modality). Moreover, these TL methods cannot fully use the label information to explore the intrinsic relation between two modalities and then guide the promoted knowledge transfer. To this end, we propose a novel doubly supervised TL network (DDSTN) that integrates the Learning Using Privileged Information (LUPI) paradigm and the Maximum Mean Discrepancy (MMD) criterion into a unified deep TL framework. The proposed algorithm can not only make full use of the shared labels to effectively guide knowledge transfer by LUPI paradigm, but also perform additional super-vised transfer between unpaired data. We further introduce the MMD criterion to enhance the knowledge transfer. The experimental results on the breast ultra-sound dataset indicate that the proposed DDSTN outperforms all the compared state-of-the-art algorithms for the BUS-based CAD.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-29T07:32:07Z</published>\n",
      "    <arxiv:comment>Accepted by MICCAI 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Han Xiangmin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wang Jun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhou Weijun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chang Cai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ying Shihui</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shi Jun</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.01296v3</id>\n",
      "    <title>Faster Stochastic Alternating Direction Method of Multipliers for Nonconvex Optimization</title>\n",
      "    <updated>2020-08-10T03:20:17Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.01296v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.01296v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we propose a faster stochastic alternating direction method of multipliers (ADMM) for nonconvex optimization by using a new stochastic path-integrated differential estimator (SPIDER), called as SPIDER-ADMM. Moreover, we prove that the SPIDER-ADMM achieves a record-breaking incremental first-order oracle (IFO) complexity of $\\mathcal{O}(n+n^{1/2}ε^{-1})$ for finding an $ε$-approximate stationary point, which improves the deterministic ADMM by a factor $\\mathcal{O}(n^{1/2})$, where $n$ denotes the sample size. As one of major contribution of this paper, we provide a new theoretical analysis framework for nonconvex stochastic ADMM methods with providing the optimal IFO complexity. Based on this new analysis framework, we study the unsolved optimal IFO complexity of the existing non-convex SVRG-ADMM and SAGA-ADMM methods, and prove they have the optimal IFO complexity of $\\mathcal{O}(n+n^{2/3}ε^{-1})$. Thus, the SPIDER-ADMM improves the existing stochastic ADMM methods by a factor of $\\mathcal{O}(n^{1/6})$. Moreover, we extend SPIDER-ADMM to the online setting, and propose a faster online SPIDER-ADMM. Our theoretical analysis shows that the online SPIDER-ADMM has the IFO complexity of $\\mathcal{O}(ε^{-\\frac{3}{2}})$, which improves the existing best results by a factor of $\\mathcal{O}(ε^{-\\frac{1}{2}})$. Finally, the experimental results on benchmark datasets validate that the proposed algorithms have faster convergence rate than the existing ADMM algorithms for nonconvex optimization.</summary>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-04T02:59:42Z</published>\n",
      "    <arxiv:comment>Published in ICML 2019, 43 pages. arXiv admin note: text overlap with arXiv:1907.13463</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"math.OC\"/>\n",
      "    <author>\n",
      "      <name>Feihu Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Songcan Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Heng Huang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.01332v1</id>\n",
      "    <title>Real-Time Cleaning and Refinement of Facial Animation Signals</title>\n",
      "    <updated>2020-08-04T05:21:02Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.01332v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.01332v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>With the increasing demand for real-time animated 3D content in the entertainment industry and beyond, performance-based animation has garnered interest among both academic and industrial communities. While recent solutions for motion-capture animation have achieved impressive results, handmade post-processing is often needed, as the generated animations often contain artifacts. Existing real-time motion capture solutions have opted for standard signal processing methods to strengthen temporal coherence of the resulting animations and remove inaccuracies. While these methods produce smooth results, they inherently filter-out part of the dynamics of facial motion, such as high frequency transient movements. In this work, we propose a real-time animation refining system that preserves -- or even restores -- the natural dynamics of facial motions. To do so, we leverage an off-the-shelf recurrent neural network architecture that learns proper facial dynamics patterns on clean animation data. We parametrize our system using the temporal derivatives of the signal, enabling our network to process animations at any framerate. Qualitative results show that our system is able to retrieve natural motion signals from noisy or degraded input animation.</summary>\n",
      "    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-04T05:21:02Z</published>\n",
      "    <arxiv:comment>ICGSP 2020: Proceedings of the 2020 The 4th International Conference on Graphics and Signal Processing</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.GR\"/>\n",
      "    <author>\n",
      "      <name>Eloïse Berson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Catherine Soladié</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nicolas Stoiber</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3406971.3406985</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3406971.3406985\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.01133v1</id>\n",
      "    <title>AiRound and CV-BrCT: Novel Multi-View Datasets for Scene Classification</title>\n",
      "    <updated>2020-08-03T18:55:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.01133v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.01133v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>It is undeniable that aerial/satellite images can provide useful information for a large variety of tasks. But, since these images are always looking from above, some applications can benefit from complementary information provided by other perspective views of the scene, such as ground-level images. Despite a large number of public repositories for both georeferenced photographs and aerial images, there is a lack of benchmark datasets that allow the development of approaches that exploit the benefits and complementarity of aerial/ground imagery. In this paper, we present two new publicly available datasets named \\thedataset~and CV-BrCT. The first one contains triplets of images from the same geographic coordinate with different perspectives of view extracted from various places around the world. Each triplet is composed of an aerial RGB image, a ground-level perspective image, and a Sentinel-2 sample. The second dataset contains pairs of aerial and street-level images extracted from southeast Brazil. We design an extensive set of experiments concerning multi-view scene classification, using early and late fusion. Such experiments were conducted to show that image classification can be enhanced using multi-view data.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-03T18:55:46Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Gabriel Machado</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Edemir Ferreira</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Keiller Nogueira</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hugo Oliveira</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pedro Gama</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jefersson A. dos Santos</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.01179v3</id>\n",
      "    <title>PillarFlow: End-to-end Birds-eye-view Flow Estimation for Autonomous Driving</title>\n",
      "    <updated>2020-08-29T13:35:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.01179v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.01179v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In autonomous driving, accurately estimating the state of surrounding obstacles is critical for safe and robust path planning. However, this perception task is difficult, particularly for generic obstacles/objects, due to appearance and occlusion changes. To tackle this problem, we propose an end-to-end deep learning framework for LIDAR-based flow estimation in bird's eye view (BeV). Our method takes consecutive point cloud pairs as input and produces a 2-D BeV flow grid describing the dynamic state of each cell. The experimental results show that the proposed method not only estimates 2-D BeV flow accurately but also improves tracking performance of both dynamic and static objects.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-03T20:36:28Z</published>\n",
      "    <arxiv:comment>Accepted by IROS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Kuan-Hui Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matthew Kliemann</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Adrien Gaidon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jie Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chao Fang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sudeep Pillai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wolfram Burgard</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.01192v1</id>\n",
      "    <title>Presentation of a Recommender System with Ensemble Learning and Graph Embedding: A Case on MovieLens</title>\n",
      "    <updated>2020-07-15T12:52:15Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.01192v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.01192v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Information technology has spread widely, and extraordinarily large amounts of data have been made accessible to users, which has made it challenging to select data that are in accordance with user needs. For the resolution of the above issue, recommender systems have emerged, which much help users go through the process of decision-making and selecting relevant data. A recommender system predicts users behavior to be capable of detecting their interests and needs, and it often uses the classification technique for this purpose. It may not be sufficiently accurate to employ individual classification, where not all cases can be examined, which makes the method inappropriate to specific problems. In this research, group classification and the ensemble learning technique were used for increasing prediction accuracy in recommender systems. Another issue that is raised here concerns user analysis. Given the large size of the data and a large number of users, the process of user needs analysis and prediction (using a graph in most cases, representing the relations between users and their selected items) is complicated and cumbersome in recommender systems. Graph embedding was also proposed for resolution of this issue, where all or part of user behavior can be simulated through the generation of several vectors, resolving the problem of user behavior analysis to a large extent while maintaining high efficiency. In this research, individuals most similar to the target user were classified using ensemble learning, fuzzy rules, and the decision tree, and relevant recommendations were then made to each user with a heterogeneous knowledge graph and embedding vectors. This study was performed on the MovieLens datasets, and the obtained results indicated the high efficiency of the presented method.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-15T12:52:15Z</published>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Saman Forouzandeh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mehrdad Rostami</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kamal Berahmand</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.01201v1</id>\n",
      "    <title>Mixup-CAM: Weakly-supervised Semantic Segmentation via Uncertainty Regularization</title>\n",
      "    <updated>2020-08-03T21:19:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.01201v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.01201v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Obtaining object response maps is one important step to achieve weakly-supervised semantic segmentation using image-level labels. However, existing methods rely on the classification task, which could result in a response map only attending on discriminative object regions as the network does not need to see the entire object for optimizing the classification loss. To tackle this issue, we propose a principled and end-to-end train-able framework to allow the network to pay attention to other parts of the object, while producing a more complete and uniform response map. Specifically, we introduce the mixup data augmentation scheme into the classification network and design two uncertainty regularization terms to better interact with the mixup strategy. In experiments, we conduct extensive analysis to demonstrate the proposed method and show favorable performance against state-of-the-art approaches.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-03T21:19:08Z</published>\n",
      "    <arxiv:comment>Accepted at BMVC 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Yu-Ting Chang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qiaosong Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wei-Chih Hung</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Robinson Piramuthu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yi-Hsuan Tsai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ming-Hsuan Yang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.01205v1</id>\n",
      "    <title>Concurrent Training Improves the Performance of Behavioral Cloning from Observation</title>\n",
      "    <updated>2020-08-03T21:30:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.01205v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.01205v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Learning from demonstration is widely used as an efficient way for robots to acquire new skills. However, it typically requires that demonstrations provide full access to the state and action sequences. In contrast, learning from observation offers a way to utilize unlabeled demonstrations (e.g., video) to perform imitation learning. One approach to this is behavioral cloning from observation (BCO). The original implementation of BCO proceeds by first learning an inverse dynamics model and then using that model to estimate action labels, thereby reducing the problem to behavioral cloning. However, existing approaches to BCO require a large number of initial interactions in the first step. Here, we provide a novel theoretical analysis of BCO, introduce a modification BCO*, and show that in the semi-supervised setting, BCO* can concurrently improve both its estimate for the inverse dynamics model and the expert policy. This result allows us to eliminate the dependence on initial interactions and dramatically improve the sample complexity of BCO. We evaluate the effectiveness of our algorithm through experiments on various benchmark domains. The results demonstrate that concurrent training not only improves over the performance of BCO but also results in performance that is competitive with state-of-the-art imitation learning methods such as GAIL and Value-Dice.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-03T21:30:28Z</published>\n",
      "    <arxiv:comment>13 pages, 2 figures, Submitted to the 4th Conference on Robot Learning (CoRL 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Zachary W. Robertson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matthew R. Walter</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.00698v2</id>\n",
      "    <title>Anti-Bandit Neural Architecture Search for Model Defense</title>\n",
      "    <updated>2020-08-05T08:33:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.00698v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.00698v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep convolutional neural networks (DCNNs) have dominated as the best performers in machine learning, but can be challenged by adversarial attacks. In this paper, we defend against adversarial attacks using neural architecture search (NAS) which is based on a comprehensive search of denoising blocks, weight-free operations, Gabor filters and convolutions. The resulting anti-bandit NAS (ABanditNAS) incorporates a new operation evaluation measure and search process based on the lower and upper confidence bounds (LCB and UCB). Unlike the conventional bandit algorithm using UCB for evaluation only, we use UCB to abandon arms for search efficiency and LCB for a fair competition between arms. Extensive experiments demonstrate that ABanditNAS is faster than other NAS methods, while achieving an $8.73\\%$ improvement over prior arts on CIFAR-10 under PGD-$7$.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-03T07:59:39Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Hanlin Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Baochang Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Song Xue</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xuan Gong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hong Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rongrong Ji</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Doermann</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.00752v1</id>\n",
      "    <title>GmFace: A Mathematical Model for Face Image Representation Using Multi-Gaussian</title>\n",
      "    <updated>2020-08-03T10:11:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.00752v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.00752v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Establishing mathematical models is a ubiquitous and effective method to understand the objective world. Due to complex physiological structures and dynamic behaviors, mathematical representation of the human face is an especially challenging task. A mathematical model for face image representation called GmFace is proposed in the form of a multi-Gaussian function in this paper. The model utilizes the advantages of two-dimensional Gaussian function which provides a symmetric bell surface with a shape that can be controlled by parameters. The GmNet is then designed using Gaussian functions as neurons, with parameters that correspond to each of the parameters of GmFace in order to transform the problem of GmFace parameter solving into a network optimization problem of GmNet. The face modeling process can be described by the following steps: (1) GmNet initialization; (2) feeding GmNet with face image(s); (3) training GmNet until convergence; (4) drawing out the parameters of GmNet (as the same as GmFace); (5) recording the face model GmFace. Furthermore, using GmFace, several face image transformation operations can be realized mathematically through simple parameter computation.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-03T10:11:10Z</published>\n",
      "    <arxiv:comment>12 pages, 12 figures, 4 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Liping Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Weijun Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lina Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaoli Dong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Linjun Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xin Ning</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jian Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hong Qin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.00756v1</id>\n",
      "    <title>Structure and Automatic Segmentation of Dhrupad Vocal Bandish Audio</title>\n",
      "    <updated>2020-08-03T10:16:42Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.00756v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.00756v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A Dhrupad vocal concert comprises a composition section that is interspersed with improvised episodes of increased rhythmic activity involving the interaction between the vocals and the percussion. Tracking the changing rhythmic density, in relation to the underlying metric tempo of the piece, thus facilitates the detection and labeling of the improvised sections in the concert structure. This work concerns the automatic detection of the musically relevant rhythmic densities as they change in time across the bandish (composition) performance. An annotated dataset of Dhrupad bandish concert sections is presented. We investigate a CNN-based system, trained to detect local tempo relationships, and follow it with temporal smoothing. We also employ audio source separation as a pre-processing step to the detection of the individual surface densities of the vocals and the percussion. This helps us obtain the complete musical description of the concert sections in terms of capturing the changing rhythmic interaction of the two performers.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-03T10:16:42Z</published>\n",
      "    <arxiv:comment>Part of this work published in ISMIR 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Rohit M. A.</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Preeti Rao</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.00768v1</id>\n",
      "    <title>One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech</title>\n",
      "    <updated>2020-08-03T10:43:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.00768v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.00768v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We introduce an approach to multilingual speech synthesis which uses the meta-learning concept of contextual parameter generation and produces natural-sounding multilingual speech using more languages and less training data than previous approaches. Our model is based on Tacotron 2 with a fully convolutional input text encoder whose weights are predicted by a separate parameter generator network. To boost voice cloning, the model uses an adversarial speaker classifier with a gradient reversal layer that removes speaker-specific information from the encoder.\n",
      "  We arranged two experiments to compare our model with baselines using various levels of cross-lingual parameter sharing, in order to evaluate: (1) stability and performance when training on low amounts of data, (2) pronunciation accuracy and voice quality of code-switching synthesis. For training, we used the CSS10 dataset and our new small dataset based on Common Voice recordings in five languages. Our model is shown to effectively share information across languages and according to a subjective evaluation test, it produces more natural and accurate code-switching speech than the baselines.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-03T10:43:30Z</published>\n",
      "    <arxiv:comment>Accepted to INTERSPEECH 2020; for the source files, see https://github.com/Tomiinek/Multilingual_Text_to_Speech</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Tomáš Nekvinda</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ondřej Dušek</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.00614v1</id>\n",
      "    <title>Dynamics Generalization via Information Bottleneck in Deep Reinforcement Learning</title>\n",
      "    <updated>2020-08-03T02:24:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.00614v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.00614v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Despite the significant progress of deep reinforcement learning (RL) in solving sequential decision making problems, RL agents often overfit to training environments and struggle to adapt to new, unseen environments. This prevents robust applications of RL in real world situations, where system dynamics may deviate wildly from the training settings. In this work, our primary contribution is to propose an information theoretic regularization objective and an annealing-based optimization method to achieve better generalization ability in RL agents. We demonstrate the extreme generalization benefits of our approach in different domains ranging from maze navigation to robotic tasks; for the first time, we show that agents can generalize to test parameters more than 10 standard deviations away from the training parameter distribution. This work provides a principled way to improve generalization in RL by gradually removing information that is redundant for task-solving; it opens doors for the systematic study of generalization from training to extremely different testing settings, focusing on the established connections between information theory and machine learning.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-03T02:24:20Z</published>\n",
      "    <arxiv:comment>16 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xingyu Lu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kimin Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pieter Abbeel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stas Tiomkin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.00616v1</id>\n",
      "    <title>Multitask learning for instrument activation aware music source separation</title>\n",
      "    <updated>2020-08-03T02:35:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.00616v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.00616v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Music source separation is a core task in music information retrieval which has seen a dramatic improvement in the past years. Nevertheless, most of the existing systems focus exclusively on the problem of source separation itself and ignore the utilization of other~---possibly related---~MIR tasks which could lead to additional quality gains. In this work, we propose a novel multitask structure to investigate using instrument activation information to improve source separation performance. Furthermore, we investigate our system on six independent instruments, a more realistic scenario than the three instruments included in the widely-used MUSDB dataset, by leveraging a combination of the MedleyDB and Mixing Secrets datasets. The results show that our proposed multitask model outperforms the baseline Open-Unmix model on the mixture of Mixing Secrets and MedleyDB dataset while maintaining comparable performance on the MUSDB dataset.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-03T02:35:00Z</published>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Yun-Ning Hung</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexander Lerch</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.01571v2</id>\n",
      "    <title>IntelligentPooling: Practical Thompson Sampling for mHealth</title>\n",
      "    <updated>2020-12-12T21:30:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.01571v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.01571v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In mobile health (mHealth) smart devices deliver behavioral treatments repeatedly over time to a user with the goal of helping the user adopt and maintain healthy behaviors. Reinforcement learning appears ideal for learning how to optimally make these sequential treatment decisions. However, significant challenges must be overcome before reinforcement learning can be effectively deployed in a mobile healthcare setting. In this work we are concerned with the following challenges: 1) individuals who are in the same context can exhibit differential response to treatments 2) only a limited amount of data is available for learning on any one individual, and 3) non-stationary responses to treatment. To address these challenges we generalize Thompson-Sampling bandit algorithms to develop IntelligentPooling. IntelligentPooling learns personalized treatment policies thus addressing challenge one. To address the second challenge, IntelligentPooling updates each user's degree of personalization while making use of available data on other users to speed up learning. Lastly, IntelligentPooling allows responsivity to vary as a function of a user's time since beginning treatment, thus addressing challenge three. We show that IntelligentPooling achieves an average of 26% lower regret than state-of-the-art. We demonstrate the promise of this approach and its ability to learn from even a small group of users in a live clinical trial.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-31T19:03:09Z</published>\n",
      "    <arxiv:comment>arXiv admin note: text overlap with arXiv:2002.09971</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Sabina Tomkins</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peng Liao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Predrag Klasnja</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Susan Murphy</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.01641v1</id>\n",
      "    <title>Exploring Variational Deep Q Networks</title>\n",
      "    <updated>2020-08-04T15:36:31Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.01641v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.01641v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This study provides both analysis and a refined, research-ready implementation of Tang and Kucukelbir's Variational Deep Q Network, a novel approach to maximising the efficiency of exploration in complex learning environments using Variational Bayesian Inference. Alongside reference implementations of both Traditional and Double Deep Q Networks, a small novel contribution is presented - the Double Variational Deep Q Network, which incorporates improvements to increase the stability and robustness of inference-based learning. Finally, an evaluation and discussion of the effectiveness of these approaches is discussed in the wider context of Bayesian Deep Learning.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-04T15:36:31Z</published>\n",
      "    <arxiv:comment>12 pages, 5 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>A. H. Bell-Thomas</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.00966v1</id>\n",
      "    <title>Using neural networks to predict icephobic performance</title>\n",
      "    <updated>2020-07-31T05:37:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.00966v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.00966v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Icephobic surfaces inspired by superhydrophobic surfaces offer a passive solution to the problem of icing. However, modeling icephobicity is challenging because some material features that aid superhydrophobicity can adversely affect the icephobic performance. This study presents a new approach based on artificial neural networks to model icephobicity. Artificial neural network models were developed to predict the icephobic performance of concrete. The models were trained on experimental data to predict the surface ice adhesion strength and the coefficient of restitution (COR) of water droplet bouncing off the surface under freezing conditions. The material and coating compositions, and environmental condition were used as the models' input variables. A multilayer perceptron was trained to predict COR with a root mean squared error of 0.08, and a 90% confidence interval of [0.042, 0.151]. The model had a coefficient of determination of 0.92 after deployment. Since ice adhesion strength varied over a wide range of values for the samples, a mixture density network was model was developed to learn the underlying relationship in the multimodal data. Coefficient of determination for the model was 0.96. The relative importance of the input variables in icephobic performance were calculated using permutation importance. The developed models will be beneficial to optimize icephobicity of concrete.</summary>\n",
      "    <category term=\"cond-mat.soft\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cond-mat.mtrl-sci\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-31T05:37:06Z</published>\n",
      "    <arxiv:primary_category term=\"cond-mat.soft\"/>\n",
      "    <author>\n",
      "      <name>Rahul Ramachandran</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.00157v1</id>\n",
      "    <title>L-CNN: A Lattice cross-fusion strategy for multistream convolutional neural networks</title>\n",
      "    <updated>2020-08-01T03:08:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.00157v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.00157v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper proposes a fusion strategy for multistream convolutional networks, the Lattice Cross Fusion. This approach crosses signals from convolution layers performing mathematical operation-based fusions right before pooling layers. Results on a purposely worsened CIFAR-10, a popular image classification data set, with a modified AlexNet-LCNN version show that this novel method outperforms by 46% the baseline single stream network, with faster convergence, stability, and robustness.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-01T03:08:28Z</published>\n",
      "    <arxiv:comment>5 pages, 3 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <arxiv:journal_ref>Electronics Letters, vol. 55, no. 22, pp. 1180-1182, 2029</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Ana Paula G. S. de Almeida</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Flavio de Barros Vidal</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1049/el.2019.2631</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1049/el.2019.2631\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.00178v1</id>\n",
      "    <title>Contrastive Explanations in Neural Networks</title>\n",
      "    <updated>2020-08-01T05:50:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.00178v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.00178v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Visual explanations are logical arguments based on visual features that justify the predictions made by neural networks. Current modes of visual explanations answer questions of the form $`Why \\text{ } P?'$. These $Why$ questions operate under broad contexts thereby providing answers that are irrelevant in some cases. We propose to constrain these $Why$ questions based on some context $Q$ so that our explanations answer contrastive questions of the form $`Why \\text{ } P, \\text{} rather \\text{ } than \\text{ } Q?'$. In this paper, we formalize the structure of contrastive visual explanations for neural networks. We define contrast based on neural networks and propose a methodology to extract defined contrasts. We then use the extracted contrasts as a plug-in on top of existing $`Why \\text{ } P?'$ techniques, specifically Grad-CAM. We demonstrate their value in analyzing both networks and data in applications of large-scale recognition, fine-grained recognition, subsurface seismic analysis, and image quality assessment.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-01T05:50:01Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Mohit Prabhushankar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gukyeong Kwon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dogancan Temel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ghassan AlRegib</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.00331v1</id>\n",
      "    <title>Learning from Mixtures of Private and Public Populations</title>\n",
      "    <updated>2020-08-01T20:11:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.00331v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.00331v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We initiate the study of a new model of supervised learning under privacy constraints. Imagine a medical study where a dataset is sampled from a population of both healthy and unhealthy individuals. Suppose healthy individuals have no privacy concerns (in such case, we call their data \"public\") while the unhealthy individuals desire stringent privacy protection for their data. In this example, the population (data distribution) is a mixture of private (unhealthy) and public (healthy) sub-populations that could be very different.\n",
      "  Inspired by the above example, we consider a model in which the population $\\mathcal{D}$ is a mixture of two sub-populations: a private sub-population $\\mathcal{D}_{\\sf priv}$ of private and sensitive data, and a public sub-population $\\mathcal{D}_{\\sf pub}$ of data with no privacy concerns. Each example drawn from $\\mathcal{D}$ is assumed to contain a privacy-status bit that indicates whether the example is private or public. The goal is to design a learning algorithm that satisfies differential privacy only with respect to the private examples.\n",
      "  Prior works in this context assumed a homogeneous population where private and public data arise from the same distribution, and in particular designed solutions which exploit this assumption. We demonstrate how to circumvent this assumption by considering, as a case study, the problem of learning linear classifiers in $\\mathbb{R}^d$. We show that in the case where the privacy status is correlated with the target label (as in the above example), linear classifiers in $\\mathbb{R}^d$ can be learned, in the agnostic as well as the realizable setting, with sample complexity which is comparable to that of the classical (non-private) PAC-learning. It is known that this task is impossible if all the data is considered private.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-01T20:11:50Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Raef Bassily</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shay Moran</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anupama Nandi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.00363v1</id>\n",
      "    <title>Looking in the Right place for Anomalies: Explainable AI through Automatic Location Learning</title>\n",
      "    <updated>2020-08-02T00:02:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.00363v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.00363v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep learning has now become the de facto approach to the recognition of anomalies in medical imaging. Their 'black box' way of classifying medical images into anomaly labels poses problems for their acceptance, particularly with clinicians. Current explainable AI methods offer justifications through visualizations such as heat maps but cannot guarantee that the network is focusing on the relevant image region fully containing the anomaly. In this paper, we develop an approach to explainable AI in which the anomaly is assured to be overlapping the expected location when present. This is made possible by automatically extracting location-specific labels from textual reports and learning the association of expected locations to labels using a hybrid combination of Bi-Directional Long Short-Term Memory Recurrent Neural Networks (Bi-LSTM) and DenseNet-121. Use of this expected location to bias the subsequent attention-guided inference network based on ResNet101 results in the isolation of the anomaly at the expected location when present. The method is evaluated on a large chest X-ray dataset.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-02T00:02:37Z</published>\n",
      "    <arxiv:comment>5 pages, Paper presented as a poster at the International Symposium on Biomedical Imaging, 2020, Paper Number 655</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <arxiv:journal_ref>2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Satyananda Kashyap</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexandros Karargyris</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joy Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yaniv Gur</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arjun Sharma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ken C. L. Wong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mehdi Moradi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tanveer Syeda-Mahmood</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/ISBI45749.2020.9098370</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/ISBI45749.2020.9098370\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.00408v1</id>\n",
      "    <title>Blackbox Trojanising of Deep Learning Models : Using non-intrusive network structure and binary alterations</title>\n",
      "    <updated>2020-08-02T06:33:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.00408v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.00408v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent advancements in Artificial Intelligence namely in Deep Learning has heightened its adoption in many applications. Some are playing important roles to the extent that we are heavily dependent on them for our livelihood. However, as with all technologies, there are vulnerabilities that malicious actors could exploit. A form of exploitation is to turn these technologies, intended for good, to become dual-purposed instruments to support deviant acts like malicious software trojans. As part of proactive defense, researchers are proactively identifying such vulnerabilities so that protective measures could be developed subsequently. This research explores a novel blackbox trojanising approach using a simple network structure modification to any deep learning image classification model that would transform a benign model into a deviant one with a simple manipulation of the weights to induce specific types of errors. Propositions to protect the occurrence of such simple exploits are discussed in this research. This research highlights the importance of providing sufficient safeguards to these models so that the intended good of AI innovation and adoption may be protected.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-02T06:33:47Z</published>\n",
      "    <arxiv:comment>6 pages, 2 Figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Jonathan Pan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.00250v1</id>\n",
      "    <title>Deep Reinforcement Learning Based Mobile Edge Computing for Intelligent Internet of Things</title>\n",
      "    <updated>2020-08-01T11:45:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.00250v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.00250v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we investigate mobile edge computing (MEC) networks for intelligent internet of things (IoT), where multiple users have some computational tasks assisted by multiple computational access points (CAPs). By offloading some tasks to the CAPs, the system performance can be improved through reducing the latency and energy consumption, which are the two important metrics of interest in the MEC networks. We devise the system by proposing the offloading strategy intelligently through the deep reinforcement learning algorithm. In this algorithm, Deep Q-Network is used to automatically learn the offloading decision in order to optimize the system performance, and a neural network (NN) is trained to predict the offloading action, where the training data is generated from the environmental system. Moreover, we employ the bandwidth allocation in order to optimize the wireless spectrum for the links between the users and CAPs, where several bandwidth allocation schemes are proposed. In further, we use the CAP selection in order to choose one best CAP to assist the computational tasks from the users. Simulation results are finally presented to show the effectiveness of the proposed reinforcement learning offloading strategy. In particular, the system cost of latency and energy consumption can be reduced significantly by the proposed deep reinforcement learning based algorithm.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-01T11:45:54Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Rui Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xinjie Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Junjuan Xia</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liseng Fan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.16104v1</id>\n",
      "    <title>Uncovering the structure of clinical EEG signals with self-supervised learning</title>\n",
      "    <updated>2020-07-31T14:34:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.16104v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.16104v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Objective. Supervised learning paradigms are often limited by the amount of labeled data that is available. This phenomenon is particularly problematic in clinically-relevant data, such as electroencephalography (EEG), where labeling can be costly in terms of specialized expertise and human processing time. Consequently, deep learning architectures designed to learn on EEG data have yielded relatively shallow models and performances at best similar to those of traditional feature-based approaches. However, in most situations, unlabeled data is available in abundance. By extracting information from this unlabeled data, it might be possible to reach competitive performance with deep neural networks despite limited access to labels. Approach. We investigated self-supervised learning (SSL), a promising technique for discovering structure in unlabeled data, to learn representations of EEG signals. Specifically, we explored two tasks based on temporal context prediction as well as contrastive predictive coding on two clinically-relevant problems: EEG-based sleep staging and pathology detection. We conducted experiments on two large public datasets with thousands of recordings and performed baseline comparisons with purely supervised and hand-engineered approaches. Main results. Linear classifiers trained on SSL-learned features consistently outperformed purely supervised deep neural networks in low-labeled data regimes while reaching competitive performance when all labels were available. Additionally, the embeddings learned with each method revealed clear latent structures related to physiological and clinical phenomena, such as age effects. Significance. We demonstrate the benefit of self-supervised learning approaches on EEG data. Our results suggest that SSL may pave the way to a wider use of deep learning models on EEG data.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-31T14:34:47Z</published>\n",
      "    <arxiv:comment>32 pages, 9 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Hubert Banville</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Omar Chehab</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aapo Hyvärinen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Denis-Alexander Engemann</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexandre Gramfort</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.16109v1</id>\n",
      "    <title>Sequential Drift Detection in Deep Learning Classifiers</title>\n",
      "    <updated>2020-07-31T14:46:21Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.16109v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.16109v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We utilize neural network embeddings to detect data drift by formulating the drift detection within an appropriate sequential decision framework. This enables control of the false alarm rate although the statistical tests are repeatedly applied. Since change detection algorithms naturally face a tradeoff between avoiding false alarms and quick correct detection, we introduce a loss function which evaluates an algorithm's ability to balance these two concerns, and we use it in a series of experiments.</summary>\n",
      "    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-31T14:46:21Z</published>\n",
      "    <arxiv:comment>11 pages + appendix, 7 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.AP\"/>\n",
      "    <author>\n",
      "      <name>Samuel Ackerman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Parijat Dube</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eitan Farchi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.16122v2</id>\n",
      "    <title>COLD: Towards the Next Generation of Pre-Ranking System</title>\n",
      "    <updated>2020-08-17T13:13:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.16122v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.16122v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Multi-stage cascade architecture exists widely in many industrial systems such as recommender systems and online advertising, which often consists of sequential modules including matching, pre-ranking, ranking, etc. For a long time, it is believed pre-ranking is just a simplified version of the ranking module, considering the larger size of the candidate set to be ranked. Thus, efforts are made mostly on simplifying ranking model to handle the explosion of computing power for online inference. In this paper, we rethink the challenge of the pre-ranking system from an algorithm-system co-design view. Instead of saving computing power with restriction of model architecture which causes loss of model performance, here we design a new pre-ranking system by joint optimization of both the pre-ranking model and the computing power it costs. We name it COLD (Computing power cost-aware Online and Lightweight Deep pre-ranking system). COLD beats SOTA in three folds: (i) an arbitrary deep model with cross features can be applied in COLD under a constraint of controllable computing power cost. (ii) computing power cost is explicitly reduced by applying optimization tricks for inference acceleration. This further brings space for COLD to apply more complex deep models to reach better performance. (iii) COLD model works in an online learning and severing manner, bringing it excellent ability to handle the challenge of the data distribution shift. Meanwhile, the fully online pre-ranking system of COLD provides us with a flexible infrastructure that supports efficient new model developing and online A/B testing.Since 2019, COLD has been deployed in almost all products involving the pre-ranking module in the display advertising system in Alibaba, bringing significant improvements.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-31T15:06:43Z</published>\n",
      "    <arxiv:comment>accepted by DLP-KDD 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Zhe Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liqin Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Biye Jiang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Guorui Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaoqiang Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kun Gai</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.16173v1</id>\n",
      "    <title>Embedding Ranking-Oriented Recommender System Graphs</title>\n",
      "    <updated>2020-07-31T16:56:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.16173v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.16173v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Graph-based recommender systems (GRSs) analyze the structural information in the graphical representation of data to make better recommendations, especially when the direct user-item relation data is sparse. Ranking-oriented GRSs that form a major class of recommendation systems, mostly use the graphical representation of preference (or rank) data for measuring node similarities, from which they can infer a recommendation list using a neighborhood-based mechanism. In this paper, we propose PGRec, a novel graph-based ranking-oriented recommendation framework. PGRec models the preferences of the users over items, by a novel graph structure called PrefGraph. This graph is then exploited by an improved embedding approach, taking advantage of both factorization and deep learning methods, to extract vectors representing users, items, and preferences. The resulting embedding are then used for predicting users' unknown pairwise preferences from which the final recommendation lists are inferred. We have evaluated the performance of the proposed method against the state of the art model-based and neighborhood-based recommendation methods, and our experiments show that PGRec outperforms the baseline algorithms up to 3.2% in terms of NDCG@10 in different MovieLens datasets.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-31T16:56:54Z</published>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Taher Hekmatfar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Saman Haratizadeh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sama Goliaei</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.16187v1</id>\n",
      "    <title>Ultra-light deep MIR by trimming lottery tickets</title>\n",
      "    <updated>2020-07-31T17:30:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.16187v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.16187v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Current state-of-the-art results in Music Information Retrieval are largely dominated by deep learning approaches. These provide unprecedented accuracy across all tasks. However, the consistently overlooked downside of these models is their stunningly massive complexity, which seems concomitantly crucial to their success. In this paper, we address this issue by proposing a model pruning method based on the lottery ticket hypothesis. We modify the original approach to allow for explicitly removing parameters, through structured trimming of entire units, instead of simply masking individual weights. This leads to models which are effectively lighter in terms of size, memory and number of operations. We show that our proposal can remove up to 90% of the model parameters without loss of accuracy, leading to ultra-light deep MIR models. We confirm the surprising result that, at smaller compression ratios (removing up to 85% of a network), lighter models consistently outperform their heavier counterparts. We exhibit these results on a large array of MIR tasks including audio classification, pitch recognition, chord extraction, drum transcription and onset estimation. The resulting ultra-light deep learning models for MIR can run on CPU, and can even fit on embedded devices with minimal degradation of accuracy.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-31T17:30:28Z</published>\n",
      "    <arxiv:comment>8 pages, 2 figures. 21st International Society for Music Information Retrieval Conference 11-15 October 2020, Montreal, Canada</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Philippe Esling</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Theis Bazin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Adrien Bitton</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tristan Carsault</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ninon Devis</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.08952v1</id>\n",
      "    <title>Always-On 674uW @ 4GOP/s Error Resilient Binary Neural Networks with Aggressive SRAM Voltage Scaling on a 22nm IoT End-Node</title>\n",
      "    <updated>2020-07-17T12:56:58Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.08952v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.08952v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Binary Neural Networks (BNNs) have been shown to be robust to random bit-level noise, making aggressive voltage scaling attractive as a power-saving technique for both logic and SRAMs. In this work, we introduce the first fully programmable IoT end-node system-on-chip (SoC) capable of executing software-defined, hardware-accelerated BNNs at ultra-low voltage. Our SoC exploits a hybrid memory scheme where error-vulnerable SRAMs are complemented by reliable standard-cell memories to safely store critical data under aggressive voltage scaling. On a prototype in 22nm FDX technology, we demonstrate that both the logic and SRAM voltage can be dropped to 0.5Vwithout any accuracy penalty on a BNN trained for the CIFAR-10 dataset, improving energy efficiency by 2.2X w.r.t. nominal conditions. Furthermore, we show that the supply voltage can be dropped to 0.42V (50% of nominal) while keeping more than99% of the nominal accuracy (with a bit error rate ~1/1000). In this operating point, our prototype performs 4Gop/s (15.4Inference/s on the CIFAR-10 dataset) by computing up to 13binary ops per pJ, achieving 22.8 Inference/s/mW while keeping within a peak power envelope of 674uW - low enough to enable always-on operation in ultra-low power smart cameras, long-lifetime environmental sensors, and insect-sized pico-drones.</summary>\n",
      "    <category term=\"cs.AR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-17T12:56:58Z</published>\n",
      "    <arxiv:comment>Submitted to ISICAS2020 journal special issue</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.AR\"/>\n",
      "    <author>\n",
      "      <name>Alfio Di Mauro</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Francesco Conti</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pasquale Davide Schiavone</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Davide Rossi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luca Benini</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.15559v1</id>\n",
      "    <title>An early warning tool for predicting mortality risk of COVID-19 patients using machine learning</title>\n",
      "    <updated>2020-07-29T15:16:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.15559v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.15559v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>COVID-19 pandemic has created an extreme pressure on the global healthcare services. Fast, reliable and early clinical assessment of the severity of the disease can help in allocating and prioritizing resources to reduce mortality. In order to study the important blood biomarkers for predicting disease mortality, a retrospective study was conducted on 375 COVID-19 positive patients admitted to Tongji Hospital (China) from January 10 to February 18, 2020. Demographic and clinical characteristics, and patient outcomes were investigated using machine learning tools to identify key biomarkers to predict the mortality of individual patient. A nomogram was developed for predicting the mortality risk among COVID-19 patients. Lactate dehydrogenase, neutrophils (%), lymphocyte (%), high sensitive C-reactive protein, and age - acquired at hospital admission were identified as key predictors of death by multi-tree XGBoost model. The area under curve (AUC) of the nomogram for the derivation and validation cohort were 0.961 and 0.991, respectively. An integrated score (LNLCA) was calculated with the corresponding death probability. COVID-19 patients were divided into three subgroups: low-, moderate- and high-risk groups using LNLCA cut-off values of 10.4 and 12.65 with the death probability less than 5%, 5% to 50%, and above 50%, respectively. The prognostic model, nomogram and LNLCA score can help in early detection of high mortality risk of COVID-19 patients, which will help doctors to improve the management of patient stratification.</summary>\n",
      "    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-29T15:16:09Z</published>\n",
      "    <arxiv:comment>23 pages, 8 Figure, 6 Tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"q-bio.QM\"/>\n",
      "    <author>\n",
      "      <name>Muhammad E. H. Chowdhury</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tawsifur Rahman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amith Khandakar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Somaya Al-Madeed</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Susu M. Zughaier</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Suhail A. R. Doi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hanadi Hassen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mohammad T. Islam</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.15646v1</id>\n",
      "    <title>Rewriting a Deep Generative Model</title>\n",
      "    <updated>2020-07-30T17:58:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.15646v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.15646v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A deep generative model such as a GAN learns to model a rich set of semantic and physical rules about the target distribution, but up to now, it has been obscure how such rules are encoded in the network, or how a rule could be changed. In this paper, we introduce a new problem setting: manipulation of specific rules encoded by a deep generative model. To address the problem, we propose a formulation in which the desired rule is changed by manipulating a layer of a deep network as a linear associative memory. We derive an algorithm for modifying one entry of the associative memory, and we demonstrate that several interesting structural rules can be located and modified within the layers of state-of-the-art generative models. We present a user interface to enable users to interactively change the rules of a generative model to achieve desired effects, and we show several proof-of-concept applications. Finally, results on multiple datasets demonstrate the advantage of our method against standard fine-tuning methods and edit transfer algorithms.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-30T17:58:16Z</published>\n",
      "    <arxiv:comment>ECCV 2020 (oral). Code at https://github.com/davidbau/rewriting. For videos and demos see https://rewriting.csail.mit.edu/</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>David Bau</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Steven Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tongzhou Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jun-Yan Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Antonio Torralba</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.15695v1</id>\n",
      "    <title>PR-NN: RNN-based Detection for Coded Partial-Response Channels</title>\n",
      "    <updated>2020-07-30T19:15:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.15695v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.15695v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we investigate the use of recurrent neural network (RNN)-based detection of magnetic recording channels with inter-symbol interference (ISI). We refer to the proposed detection method, which is intended for recording channels with partial-response equalization, as Partial-Response Neural Network (PR-NN). We train bi-directional gated recurrent units (bi-GRUs) to recover the ISI channel inputs from noisy channel output sequences and evaluate the network performance when applied to continuous, streaming data. The computational complexity of PR-NN during the evaluation process is comparable to that of a Viterbi detector. The recording system on which the experiments were conducted uses a rate-2/3, (1,7) runlength-limited (RLL) code with an E2PR4 partial-response channel target. Experimental results with ideal PR signals show that the performance of PR-NN detection approaches that of Viterbi detection in additive white gaussian noise (AWGN). Moreover, the PR-NN detector outperforms Viterbi detection and achieves the performance of Noise-Predictive Maximum Likelihood (NPML) detection in additive colored noise (ACN) at different channel densities. A PR-NN detector trained with both AWGN and ACN maintains the performance observed under separate training. Similarly, when trained with ACN corresponding to two different channel densities, PR-NN maintains its performance at both densities. Experiments confirm that this robustness is consistent over a wide range of signal-to-noise ratios (SNRs). Finally, PR-NN displays robust performance when applied to a more realistic magnetic recording channel with MMSE-equalized Lorentzian signals.</summary>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-30T19:15:09Z</published>\n",
      "    <arxiv:primary_category term=\"cs.IT\"/>\n",
      "    <author>\n",
      "      <name>Simeng Zheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yi Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paul H. Siegel</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.15746v1</id>\n",
      "    <title>Laser2Vec: Similarity-based Retrieval for Robotic Perception Data</title>\n",
      "    <updated>2020-07-30T21:11:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.15746v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.15746v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>As mobile robot capabilities improve and deployment times increase, tools to analyze the growing volume of data are becoming necessary. Current state-of-the-art logging, playback, and exploration systems are insufficient for practitioners seeking to discover systemic points of failure in robotic systems. This paper presents a suite of algorithms for similarity-based queries of robotic perception data and implements a system for storing 2D LiDAR data from many deployments cheaply and evaluating top-k queries for complete or partial scans efficiently. We generate compressed representations of laser scans via a convolutional variational autoencoder and store them in a database, where a light-weight dense network for distance function approximation is run at query time. Our query evaluator leverages the local continuity of the embedding space to generate evaluation orders that, in expectation, dominate full linear scans of the database. The accuracy, robustness, scalability, and efficiency of our system is tested on real-world data gathered from dozens of deployments and synthetic data generated by corrupting real data. We find our system accurately and efficiently identifies similar scans across a number of episodes where the robot encountered the same location, or similar indoor structures or objects.</summary>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-30T21:11:50Z</published>\n",
      "    <arxiv:comment>6 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.RO\"/>\n",
      "    <author>\n",
      "      <name>Samer B. Nashed</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.15797v1</id>\n",
      "    <title>A Pyramid Recurrent Network for Predicting Crowdsourced Speech-Quality Ratings of Real-World Signals</title>\n",
      "    <updated>2020-07-31T01:46:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.15797v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.15797v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The real-world capabilities of objective speech quality measures are limited since current measures (1) are developed from simulated data that does not adequately model real environments; or they (2) predict objective scores that are not always strongly correlated with subjective ratings. Additionally, a large dataset of real-world signals with listener quality ratings does not currently exist, which would help facilitate real-world assessment. In this paper, we collect and predict the perceptual quality of real-world speech signals that are evaluated by human listeners. We first collect a large quality rating dataset by conducting crowdsourced listening studies on two real-world corpora. We further develop a novel approach that predicts human quality ratings using a pyramid bidirectional long short term memory (pBLSTM) network with an attention mechanism. The results show that the proposed model achieves statistically lower estimation errors than prior assessment approaches, where the predicted scores strongly correlate with human judgments.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-31T01:46:06Z</published>\n",
      "    <arxiv:comment>Proceeding of INTERSPEECH</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Xuan Dong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Donald S. Williamson</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.15801v2</id>\n",
      "    <title>Finite Versus Infinite Neural Networks: an Empirical Study</title>\n",
      "    <updated>2020-09-08T06:25:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.15801v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.15801v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We perform a careful, thorough, and large scale empirical study of the correspondence between wide neural networks and kernel methods. By doing so, we resolve a variety of open questions related to the study of infinitely wide neural networks. Our experimental results include: kernel methods outperform fully-connected finite-width networks, but underperform convolutional finite width networks; neural network Gaussian process (NNGP) kernels frequently outperform neural tangent (NT) kernels; centered and ensembled finite networks have reduced posterior variance and behave more similarly to infinite networks; weight decay and the use of a large learning rate break the correspondence between finite and infinite networks; the NTK parameterization outperforms the standard parameterization for finite width networks; diagonal regularization of kernels acts similarly to early stopping; floating point precision limits kernel performance beyond a critical dataset size; regularized ZCA whitening improves accuracy; finite network performance depends non-monotonically on width in ways not captured by double descent phenomena; equivariance of CNNs is only beneficial for narrow networks far from the kernel regime. Our experiments additionally motivate an improved layer-wise scaling for weight decay which improves generalization in finite-width networks. Finally, we develop improved best practices for using NNGP and NT kernels for prediction, including a novel ensembling technique. Using these best practices we achieve state-of-the-art results on CIFAR-10 classification for kernels corresponding to each architecture class we consider.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-31T01:57:47Z</published>\n",
      "    <arxiv:comment>17+11 pages; v2 references added, minor improvements</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jaehoon Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Samuel S. Schoenholz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jeffrey Pennington</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ben Adlam</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lechao Xiao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Roman Novak</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jascha Sohl-Dickstein</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.15802v1</id>\n",
      "    <title>Practical Detection of Trojan Neural Networks: Data-Limited and Data-Free Cases</title>\n",
      "    <updated>2020-07-31T02:00:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.15802v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.15802v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>When the training data are maliciously tampered, the predictions of the acquired deep neural network (DNN) can be manipulated by an adversary known as the Trojan attack (or poisoning backdoor attack). The lack of robustness of DNNs against Trojan attacks could significantly harm real-life machine learning (ML) systems in downstream applications, therefore posing widespread concern to their trustworthiness. In this paper, we study the problem of the Trojan network (TrojanNet) detection in the data-scarce regime, where only the weights of a trained DNN are accessed by the detector. We first propose a data-limited TrojanNet detector (TND), when only a few data samples are available for TrojanNet detection. We show that an effective data-limited TND can be established by exploring connections between Trojan attack and prediction-evasion adversarial attacks including per-sample attack as well as all-sample universal attack. In addition, we propose a data-free TND, which can detect a TrojanNet without accessing any data samples. We show that such a TND can be built by leveraging the internal response of hidden neurons, which exhibits the Trojan behavior even at random noise inputs. The effectiveness of our proposals is evaluated by extensive experiments under different model architectures and datasets including CIFAR-10, GTSRB, and ImageNet.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-31T02:00:38Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Ren Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gaoyuan Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sijia Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pin-Yu Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jinjun Xiong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Meng Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.14677v1</id>\n",
      "    <title>On the Use of Interpretable Machine Learning for the Management of Data Quality</title>\n",
      "    <updated>2020-07-29T08:49:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.14677v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.14677v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Data quality is a significant issue for any application that requests for analytics to support decision making. It becomes very important when we focus on Internet of Things (IoT) where numerous devices can interact to exchange and process data. IoT devices are connected to Edge Computing (EC) nodes to report the collected data, thus, we have to secure data quality not only at the IoT but also at the edge of the network. In this paper, we focus on the specific problem and propose the use of interpretable machine learning to deliver the features that are important to be based for any data processing activity. Our aim is to secure data quality, at least, for those features that are detected as significant in the collected datasets. We have to notice that the selected features depict the highest correlation with the remaining in every dataset, thus, they can be adopted for dimensionality reduction. We focus on multiple methodologies for having interpretability in our learning models and adopt an ensemble scheme for the final decision. Our scheme is capable of timely retrieving the final result and efficiently select the appropriate features. We evaluate our model through extensive simulations and present numerical results. Our aim is to reveal its performance under various experimental scenarios that we create varying a set of parameters adopted in our mechanism.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-29T08:49:32Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Anna Karanika</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Panagiotis Oikonomou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kostas Kolomvatsos</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christos Anagnostopoulos</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.14698v1</id>\n",
      "    <title>Kernel Mean Embeddings of Von Neumann-Algebra-Valued Measures</title>\n",
      "    <updated>2020-07-29T09:26:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.14698v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.14698v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Kernel mean embedding (KME) is a powerful tool to analyze probability measures for data, where the measures are conventionally embedded into a reproducing kernel Hilbert space (RKHS). In this paper, we generalize KME to that of von Neumann-algebra-valued measures into reproducing kernel Hilbert modules (RKHMs), which provides an inner product and distance between von Neumann-algebra-valued measures. Von Neumann-algebra-valued measures can, for example, encode relations between arbitrary pairs of variables in a multivariate distribution or positive operator-valued measures for quantum mechanics. Thus, this allows us to perform probabilistic analyses explicitly reflected with higher-order interactions among variables, and provides a way of applying machine learning frameworks to problems in quantum mechanics. We also show that the injectivity of the existing KME and the universality of RKHS are generalized to RKHM, which confirms many useful features of the existing KME remain in our generalized KME. And, we investigate the empirical performance of our methods using synthetic and real-world data.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-29T09:26:39Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Yuka Hashimoto</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Isao Ishikawa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Masahiro Ikeda</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fuyuta Komura</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yoshinobu Kawahara</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.14714v1</id>\n",
      "    <title>End-to-End Adversarial White Box Attacks on Music Instrument Classification</title>\n",
      "    <updated>2020-07-29T09:52:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.14714v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.14714v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Small adversarial perturbations of input data are able to drastically change performance of machine learning systems, thereby challenging the validity of such systems. We present the very first end-to-end adversarial attacks on a music instrument classification system allowing to add perturbations directly to audio waveforms instead of spectrograms. Our attacks are able to reduce the accuracy close to a random baseline while at the same time keeping perturbations almost imperceptible and producing misclassifications to any desired instrument.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-29T09:52:32Z</published>\n",
      "    <arxiv:comment>8 pages, 2 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Katharina Prinz</name>\n",
      "      <arxiv:affiliation>Johannes Kepler University Linz</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arthur Flexer</name>\n",
      "      <arxiv:affiliation>Johannes Kepler University Linz</arxiv:affiliation>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.14777v1</id>\n",
      "    <title>PDCOVIDNet: A Parallel-Dilated Convolutional Neural Network Architecture for Detecting COVID-19 from Chest X-Ray Images</title>\n",
      "    <updated>2020-07-29T12:28:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.14777v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.14777v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The COVID-19 pandemic continues to severely undermine the prosperity of the global health system. To combat this pandemic, effective screening techniques for infected patients are indispensable. There is no doubt that the use of chest X-ray images for radiological assessment is one of the essential screening techniques. Some of the early studies revealed that the patient's chest X-ray images showed abnormalities, which is natural for patients infected with COVID-19. In this paper, we proposed a parallel-dilated convolutional neural network (CNN) based COVID-19 detection system from chest x-ray images, named as Parallel-Dilated COVIDNet (PDCOVIDNet). First, the publicly available chest X-ray collection fully preloaded and enhanced, and then classified by the proposed method. Differing convolution dilation rate in a parallel form demonstrates the proof-of-principle for using PDCOVIDNet to extract radiological features for COVID-19 detection. Accordingly, we have assisted our method with two visualization methods, which are specifically designed to increase understanding of the key components associated with COVID-19 infection. Both visualization methods compute gradients for a given image category related to feature maps of the last convolutional layer to create a class-discriminative region. In our experiment, we used a total of 2,905 chest X-ray images, comprising three cases (such as COVID-19, normal, and viral pneumonia), and empirical evaluations revealed that the proposed method extracted more significant features expeditiously related to the suspected disease. The experimental results demonstrate that our proposed method significantly improves performance metrics: accuracy, precision, recall, and F1 scores reach 96.58%, 96.58%, 96.59%, and 96.58%, respectively, which is comparable or enhanced compared with the state-of-the-art methods.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-29T12:28:16Z</published>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <arxiv:journal_ref>Health information science and systems, 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Nihad Karim Chowdhury</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Md. Muhtadir Rahman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Muhammad Ashad Kabir</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1007/s13755-020-00119-3</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1007/s13755-020-00119-3\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.14461v1</id>\n",
      "    <title>Modeling Behaviour to Predict User State: Self-Reports as Ground Truth</title>\n",
      "    <updated>2020-07-28T20:09:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.14461v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.14461v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Methods that detect user states such as emotions are useful for interactive systems. In this position paper, we argue for model-based approaches that are trained on user behaviour and self-reported user state as ground truths. In an application context, they record behaviour, extract relevant features, and use the models to predict user states. We describe how this approach can be implemented and discuss its benefits in comparison to solely self-reports in an application and to models of behaviour without the selfreport ground truths. Finally, we discuss shortcomings of this approach by considering its drawbacks and limitations.</summary>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-28T20:09:09Z</published>\n",
      "    <arxiv:primary_category term=\"cs.HC\"/>\n",
      "    <author>\n",
      "      <name>Julian Frommel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Regan L Mandryk</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.14471v1</id>\n",
      "    <title>Learning to predict metal deformations in hot-rolling processes</title>\n",
      "    <updated>2020-07-22T13:33:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.14471v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.14471v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Hot-rolling is a metal forming process that produces a workpiece with a desired target cross-section from an input workpiece through a sequence of plastic deformations; each deformation is generated by a stand composed of opposing rolls with a specific geometry. In current practice, the rolling sequence (i.e., the sequence of stands and the geometry of their rolls) needed to achieve a given final cross-section is designed by experts based on previous experience, and iteratively refined in a costly trial-and-error process. Finite Element Method simulations are increasingly adopted to make this process more efficient and to test potential rolling sequences, achieving good accuracy at the cost of long simulation times, limiting the practical use of the approach. We propose a supervised learning approach to predict the deformation of a given workpiece by a set of rolls with a given geometry; the model is trained on a large dataset of procedurally-generated FEM simulations, which we publish as supplementary material. The resulting predictor is four orders of magnitude faster than simulations, and yields an average Jaccard Similarity Index of 0.972 (against ground truth from simulations) and 0.925 (against real-world measured deformations); we additionally report preliminary results on using the predictor for automatic planning of rolling sequences.</summary>\n",
      "    <category term=\"cs.CE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-22T13:33:44Z</published>\n",
      "    <arxiv:comment>Accepted for publication in the IEEE Robotics &amp; Automation Letters (2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CE\"/>\n",
      "    <author>\n",
      "      <name>R. Omar Chavez-Garcia</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Emian Furger</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Samuele Kronauer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christian Brianza</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marco Scarfò</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luca Diviani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alessandro Giusti</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.14487v1</id>\n",
      "    <title>Unsupervised Learning of Particle Image Velocimetry</title>\n",
      "    <updated>2020-07-28T21:08:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.14487v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.14487v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Particle Image Velocimetry (PIV) is a classical flow estimation problem which is widely considered and utilised, especially as a diagnostic tool in experimental fluid dynamics and the remote sensing of environmental flows. Recently, the development of deep learning based methods has inspired new approaches to tackle the PIV problem. These supervised learning based methods are driven by large volumes of data with ground truth training information. However, it is difficult to collect reliable ground truth data in large-scale, real-world scenarios. Although synthetic datasets can be used as alternatives, the gap between the training set-ups and real-world scenarios limits applicability. We present here what we believe to be the first work which takes an unsupervised learning based approach to tackle PIV problems. The proposed approach is inspired by classic optical flow methods. Instead of using ground truth data, we make use of photometric loss between two consecutive image frames, consistency loss in bidirectional flow estimates and spatial smoothness loss to construct the total unsupervised loss function. The approach shows significant potential and advantages for fluid flow estimation. Results presented here demonstrate that our method outputs competitive results compared with classical PIV methods as well as supervised learning based methods for a broad PIV dataset, and even outperforms these existing approaches in some difficult flow cases. Codes and trained models are available at https://github.com/erizmr/UnLiteFlowNet-PIV.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-28T21:08:37Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Mingrui Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matthew D. Piggott</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.14539v1</id>\n",
      "    <title>Truncated Linear Regression in High Dimensions</title>\n",
      "    <updated>2020-07-29T00:31:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.14539v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.14539v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>As in standard linear regression, in truncated linear regression, we are given access to observations $(A_i, y_i)_i$ whose dependent variable equals $y_i= A_i^{\\rm T} \\cdot x^* + η_i$, where $x^*$ is some fixed unknown vector of interest and $η_i$ is independent noise; except we are only given an observation if its dependent variable $y_i$ lies in some \"truncation set\" $S \\subset \\mathbb{R}$. The goal is to recover $x^*$ under some favorable conditions on the $A_i$'s and the noise distribution. We prove that there exists a computationally and statistically efficient method for recovering $k$-sparse $n$-dimensional vectors $x^*$ from $m$ truncated samples, which attains an optimal $\\ell_2$ reconstruction error of $O(\\sqrt{(k \\log n)/m})$. As a corollary, our guarantees imply a computationally efficient and information-theoretically optimal algorithm for compressed sensing with truncation, which may arise from measurement saturation effects. Our result follows from a statistical and computational analysis of the Stochastic Gradient Descent (SGD) algorithm for solving a natural adaptation of the LASSO optimization problem that accommodates truncation. This generalizes the works of both: (1) [Daskalakis et al. 2018], where no regularization is needed due to the low-dimensionality of the data, and (2) [Wainright 2009], where the objective function is simple due to the absence of truncation. In order to deal with both truncation and high-dimensionality at the same time, we develop new techniques that not only generalize the existing ones but we believe are of independent interest.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-29T00:31:34Z</published>\n",
      "    <arxiv:comment>30 pages, 1 figure</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Constantinos Daskalakis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dhruv Rohatgi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Manolis Zampetakis</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.15067v1</id>\n",
      "    <title>dMelodies: A Music Dataset for Disentanglement Learning</title>\n",
      "    <updated>2020-07-29T19:20:07Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.15067v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.15067v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Representation learning focused on disentangling the underlying factors of variation in given data has become an important area of research in machine learning. However, most of the studies in this area have relied on datasets from the computer vision domain and thus, have not been readily extended to music. In this paper, we present a new symbolic music dataset that will help researchers working on disentanglement problems demonstrate the efficacy of their algorithms on diverse domains. This will also provide a means for evaluating algorithms specifically designed for music. To this end, we create a dataset comprising of 2-bar monophonic melodies where each melody is the result of a unique combination of nine latent factors that span ordinal, categorical, and binary types. The dataset is large enough (approx. 1.3 million data points) to train and test deep networks for disentanglement learning. In addition, we present benchmarking experiments using popular unsupervised disentanglement algorithms on this dataset and compare the results with those obtained on an image-based dataset.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-29T19:20:07Z</published>\n",
      "    <arxiv:comment>To be published in: Proceedings of 21st International Society for Music Information Retrieval Conference (ISMIR), Montréal, Canada, 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Ashis Pati</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Siddharth Gururani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexander Lerch</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.14906v1</id>\n",
      "    <title>Fantastic Embeddings and How to Align Them: Zero-Shot Inference in a Multi-Shop Scenario</title>\n",
      "    <updated>2020-07-20T13:46:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.14906v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.14906v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper addresses the challenge of leveraging multiple embedding spaces for multi-shop personalization, proving that zero-shot inference is possible by transferring shopping intent from one website to another without manual intervention. We detail a machine learning pipeline to train and optimize embeddings within shops first, and support the quantitative findings with additional qualitative insights. We then turn to the harder task of using learned embeddings across shops: if products from different shops live in the same vector space, user intent - as represented by regions in this space - can then be transferred in a zero-shot fashion across websites. We propose and benchmark unsupervised and supervised methods to \"travel\" between embedding spaces, each with its own assumptions on data quantity and quality. We show that zero-shot personalization is indeed possible at scale by testing the shared embedding space with two downstream tasks, event prediction and type-ahead suggestions. Finally, we curate a cross-shop anonymized embeddings dataset to foster an inclusive discussion of this important business scenario.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-20T13:46:23Z</published>\n",
      "    <arxiv:comment>accepted at 2020 SIGIR Workshop On eCommerce</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Federico Bianchi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jacopo Tagliabue</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bingqing Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luca Bigon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ciro Greco</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.14996v1</id>\n",
      "    <title>Supervised Neural Networks for RFI Flagging</title>\n",
      "    <updated>2020-07-29T06:57:36Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.14996v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.14996v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Neural network (NN) based methods are applied to the detection of radio frequency interference (RFI) in post-correlation,post-calibration time/frequency data. While calibration doesaffect RFI for the sake of this work a reduced dataset inpost-calibration is used. Two machine learning approachesfor flagging real measurement data are demonstrated usingthe existing RFI flagging technique AOFlagger as a groundtruth. It is shown that a single layer fully connects networkcan be trained using each time/frequency sample individuallywith the magnitude and phase of each polarization and Stokesvisibilities as features. This method was able to predict aBoolean flag map for each baseline to a high degree of accuracy achieving a Recall of 0.69 and Precision of 0.83 and anF1-Score of 0.75.</summary>\n",
      "    <category term=\"astro-ph.IM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-29T06:57:36Z</published>\n",
      "    <arxiv:comment>This paper has been published in the Proceedings of RFI 2019 Workshop by IEEE Xplorer at: https://ieeexplore.ieee.org/xpl/conhome/9108774/proceeding</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"astro-ph.IM\"/>\n",
      "    <author>\n",
      "      <name>Kyle Harrison</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amit Kumar Mishra</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.23919/RFI48793.2019.9111748</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.23919/RFI48793.2019.9111748\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.15859v1</id>\n",
      "    <title>Learning Forward Reuse Distance</title>\n",
      "    <updated>2020-07-31T05:57:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.15859v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.15859v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Caching techniques are widely used in the era of cloud computing from applications, such as Web caches to infrastructures, Memcached and memory caches in computer architectures. Prediction of cached data can greatly help improve cache management and performance. The recent advancement of deep learning techniques enables the design of novel intelligent cache replacement policies. In this work, we propose a learning-aided approach to predict future data accesses. We find that a powerful LSTM-based recurrent neural network model can provide high prediction accuracy based on only a cache trace as input. The high accuracy results from a carefully crafted locality-driven feature design. Inspired by the high prediction accuracy, we propose a pseudo OPT policy and evaluate it upon 13 real-world storage workloads from Microsoft Research. Results demonstrate that the new cache policy improves state-of-art practical policies by up to 19.2% and incurs only 2.3% higher miss ratio than OPT on average.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-31T05:57:50Z</published>\n",
      "    <arxiv:comment>13 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Pengcheng Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yongbin Gu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.15920v1</id>\n",
      "    <title>Neural Style Transfer for Remote Sensing</title>\n",
      "    <updated>2020-07-31T09:30:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.15920v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.15920v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The well-known technique outlined in the paper of Leon A. Gatys et al., A Neural Algorithm of Artistic Style, has become a trending topic both in academic literature and industrial applications. Neural Style Transfer (NST) constitutes an essential tool for a wide range of applications, such as artistic stylization of 2D images, user-assisted creation tools and production tools for entertainment applications. The purpose of this study is to present a method for creating artistic maps from satellite images, based on the NST algorithm. This method includes three basic steps (i) application of semantic image segmentation on the original satellite image, dividing its content into classes (i.e. land, water), (ii) application of neural style transfer for each class and (iii) creation of a collage, i.e. an artistic image consisting of a combination of the two stylized image generated on the previous step.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-31T09:30:48Z</published>\n",
      "    <arxiv:comment>10 pages, 5 figures, presented in 2nd Greek Remote Sensing Workshop RSSAC2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Maria Karatzoglidi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Georgios Felekis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eleni Charou</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.15958v1</id>\n",
      "    <title>Feature Learning for Accelerometer based Gait Recognition</title>\n",
      "    <updated>2020-07-31T10:58:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.15958v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.15958v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent advances in pattern matching, such as speech or object recognition support the viability of feature learning with deep learning solutions for gait recognition. Past papers have evaluated deep neural networks trained in a supervised manner for this task. In this work, we investigated both supervised and unsupervised approaches. Feature extractors using similar architectures incorporated into end-to-end models and autoencoders were compared based on their ability of learning good representations for a gait verification system. Both feature extractors were trained on the IDNet dataset then used for feature extraction on the ZJU-GaitAccel dataset. Results show that autoencoders are very close to discriminative end-to-end models with regards to their feature learning ability and that fully convolutional models are able to learn good feature representations, regardless of the training strategy.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-31T10:58:01Z</published>\n",
      "    <arxiv:comment>23 pages, 10 figures, 3 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Szilárd Nemes</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Margit Antal</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.16008v1</id>\n",
      "    <title>Multi-task learning for natural language processing in the 2020s: where are we going?</title>\n",
      "    <updated>2020-07-22T13:44:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.16008v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.16008v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Multi-task learning (MTL) significantly pre-dates the deep learning era, and it has seen a resurgence in the past few years as researchers have been applying MTL to deep learning solutions for natural language tasks. While steady MTL research has always been present, there is a growing interest driven by the impressive successes published in the related fields of transfer learning and pre-training, such as BERT, and the release of new challenge problems, such as GLUE and the NLP Decathlon (decaNLP). These efforts place more focus on how weights are shared across networks, evaluate the re-usability of network components and identify use cases where MTL can significantly outperform single-task solutions. This paper strives to provide a comprehensive survey of the numerous recent MTL contributions to the field of natural language processing and provide a forum to focus efforts on the hardest unsolved problems in the next decade. While novel models that improve performance on NLP benchmarks are continually produced, lasting MTL challenges remain unsolved which could hold the key to better language understanding, knowledge discovery and natural language interfaces.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-22T13:44:57Z</published>\n",
      "    <arxiv:comment>12 pages, 2 figures. Published in Elsevier Pattern Recognition Letters Volume 136. Accepted manuscript published here</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <arxiv:journal_ref>Pattern Recognition Letters 136 (2020) 120-126</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Joseph Worsham</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jugal Kalita</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1016/j.patrec.2020.05.031</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1016/j.patrec.2020.05.031\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.16013v2</id>\n",
      "    <title>Neural Composition: Learning to Generate from Multiple Models</title>\n",
      "    <updated>2020-11-09T23:41:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.16013v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.16013v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Decomposing models into multiple components is critically important in many applications such as language modeling (LM) as it enables adapting individual components separately and biasing of some components to the user's personal preferences. Conventionally, contextual and personalized adaptation for language models, are achieved through class-based factorization, which requires class-annotated data, or through biasing to individual phrases which is limited in scale. In this paper, we propose a system that combines model-defined components, by learning when to activate the generation process from each individual component, and how to combine probability distributions from each component, directly from unlabeled text data.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-10T22:58:53Z</published>\n",
      "    <arxiv:comment>Self-Supervised Learning for Speech and Audio Processing Workshop @ NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Denis Filimonov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ravi Teja Gadde</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ariya Rastrow</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.15338v1</id>\n",
      "    <title>New approach to MPI program execution time prediction</title>\n",
      "    <updated>2020-07-30T09:35:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.15338v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.15338v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The problem of MPI programs execution time prediction on a certain set of computer installations is considered. This problem emerges with orchestration and provisioning a virtual infrastructure in a cloud computing environment over a heterogeneous network of computer installations: supercomputers or clusters of servers (e.g. mini data centers). One of the key criteria for the effectiveness of the cloud computing environment is the time staying by the program inside the environment. This time consists of the waiting time in the queue and the execution time on the selected physical computer installation, to which the computational resource of the virtual infrastructure is dynamically mapped. One of the components of this problem is the estimation of the MPI programs execution time on a certain set of computer installations. This is necessary to determine a proper choice of order and place for program execution. The article proposes two new approaches to the program execution time prediction problem. The first one is based on computer installations grouping based on the Pearson correlation coefficient. The second one is based on vector representations of computer installations and MPI programs, so-called embeddings. The embedding technique is actively used in recommendation systems, such as for goods (Amazon), for articles (Arxiv.org), for videos (YouTube, Netflix). The article shows how the embeddings technique helps to predict the execution time of a MPI program on a certain set of computer installations.</summary>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-30T09:35:08Z</published>\n",
      "    <arxiv:primary_category term=\"cs.DC\"/>\n",
      "    <author>\n",
      "      <name>A. Chupakhin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>A. Kolosov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>R. Smeliansky</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>V. Antonenko</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>G. Ishelev</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.15359v1</id>\n",
      "    <title>Trade-offs in Top-k Classification Accuracies on Losses for Deep Learning</title>\n",
      "    <updated>2020-07-30T10:18:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.15359v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.15359v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper presents an experimental analysis about trade-offs in top-k classification accuracies on losses for deep leaning and proposal of a novel top-k loss. Commonly-used cross entropy (CE) is not guaranteed to optimize top-k prediction without infinite training data and model complexities. The objective is to clarify when CE sacrifices top-k accuracies to optimize top-1 prediction, and to design loss that improve top-k accuracy under such conditions. Our novel loss is basically CE modified by grouping temporal top-k classes as a single class. To obtain a robust decision boundary, we introduce an adaptive transition from normal CE to our loss, and thus call it top-k transition loss. It is demonstrated that CE is not always the best choice to learn top-k prediction in our experiments. First, we explore trade-offs between top-1 and top-k (=2) accuracies on synthetic datasets, and find a failure of CE in optimizing top-k prediction when we have complex data distribution for a given model to represent optimal top-1 prediction. Second, we compare top-k accuracies on CIFAR-100 dataset targeting top-5 prediction in deep learning. While CE performs the best in top-1 accuracy, in top-5 accuracy our loss performs better than CE except using one experimental setup. Moreover, our loss has been found to provide better top-k accuracies compared to CE at k larger than 10. As a result, a ResNet18 model trained with our loss reaches 99 % accuracy with k=25 candidates, which is a smaller candidate number than that of CE by 8.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-30T10:18:57Z</published>\n",
      "    <arxiv:comment>Submitted to ICPR 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Azusa Sawada</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eiji Kaneko</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kazutoshi Sagi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.15378v1</id>\n",
      "    <title>Generalization Comparison of Deep Neural Networks via Output Sensitivity</title>\n",
      "    <updated>2020-07-30T11:08:42Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.15378v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.15378v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Although recent works have brought some insights into the performance improvement of techniques used in state-of-the-art deep-learning models, more work is needed to understand their generalization properties. We shed light on this matter by linking the loss function to the output's sensitivity to its input. We find a rather strong empirical relation between the output sensitivity and the variance in the bias-variance decomposition of the loss function, which hints on using sensitivity as a metric for comparing the generalization performance of networks, without requiring labeled data. We find that sensitivity is decreased by applying popular methods which improve the generalization performance of the model, such as (1) using a deep network rather than a wide one, (2) adding convolutional layers to baseline classifiers instead of adding fully-connected layers, (3) using batch normalization, dropout and max-pooling, and (4) applying parameter initialization techniques.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-30T11:08:42Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Mahsa Forouzesh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Farnood Salehi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Patrick Thiran</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.15422v1</id>\n",
      "    <title>Few shot domain adaptation for in situ macromolecule structural classification in cryo-electron tomograms</title>\n",
      "    <updated>2020-07-30T12:39:21Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.15422v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.15422v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Motivation: Cryo-Electron Tomography (cryo-ET) visualizes structure and spatial organization of macromolecules and their interactions with other subcellular components inside single cells in the close-to-native state at sub-molecular resolution. Such information is critical for the accurate understanding of cellular processes. However, subtomogram classification remains one of the major challenges for the systematic recognition and recovery of the macromolecule structures in cryo-ET because of imaging limits and data quantity. Recently, deep learning has significantly improved the throughput and accuracy of large-scale subtomogram classification. However often it is difficult to get enough high-quality annotated subtomogram data for supervised training due to the enormous expense of labeling. To tackle this problem, it is beneficial to utilize another already annotated dataset to assist the training process. However, due to the discrepancy of image intensity distribution between source domain and target domain, the model trained on subtomograms in source domainmay perform poorly in predicting subtomogram classes in the target domain.\n",
      "  Results: In this paper, we adapt a few shot domain adaptation method for deep learning based cross-domain subtomogram classification. The essential idea of our method consists of two parts: 1) take full advantage of the distribution of plentiful unlabeled target domain data, and 2) exploit the correlation between the whole source domain dataset and few labeled target domain data. Experiments conducted on simulated and real datasets show that our method achieves significant improvement on cross domain subtomogram classification compared with baseline methods.</summary>\n",
      "    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-30T12:39:21Z</published>\n",
      "    <arxiv:comment>This article has been accepted for publication in Bioinformatics Published by Oxford University Press</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"q-bio.QM\"/>\n",
      "    <arxiv:journal_ref>Bioinformatics 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Liangyong Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ran Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiangrui Zeng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hongyi Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jie Jin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ge Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rui Jiang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Min Xu</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1093/bioinformatics/btaa671</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1093/bioinformatics/btaa671\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.16942v1</id>\n",
      "    <title>Do not forget interaction: Predicting fatality of COVID-19 patients using logistic regression</title>\n",
      "    <updated>2020-06-30T16:28:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.16942v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.16942v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Amid the ongoing COVID-19 pandemic, whether COVID-19 patients with high risks can be recovered or not depends, to a large extent, on how early they will be treated appropriately before irreversible consequences are caused to the patients by the virus. In this research, we reported an explainable, intuitive, and accurate machine learning model based on logistic regression to predict the fatality rate of COVID-19 patients using only three important blood biomarkers, including lactic dehydrogenase, lymphocyte (%) and high-sensitivity C-reactive protein, and their interactions. We found that when the fatality probability produced by the logistic regression model was over 0.8, the model had the optimal performance in that it was able to predict patient fatalities more than 11.30 days on average with maximally 34.91 days in advance, an accumulative f1-score of 93.76% and and an accumulative accuracy score of 93.92%. Such a model can be used to identify COVID-19 patients with high risks with three blood biomarkers and help the medical systems around the world plan critical medical resources amid this pandemic.</summary>\n",
      "    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-30T16:28:41Z</published>\n",
      "    <arxiv:primary_category term=\"stat.AP\"/>\n",
      "    <author>\n",
      "      <name>Feng Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tao Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Baiying Lei</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.16950v1</id>\n",
      "    <title>Bounded Rationality in Las Vegas: Probabilistic Finite Automata PlayMulti-Armed Bandits</title>\n",
      "    <updated>2020-06-30T16:42:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.16950v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.16950v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>While traditional economics assumes that humans are fully rational agents who always maximize their expected utility, in practice, we constantly observe apparently irrational behavior. One explanation is that people have limited computational power, so that they are, quite rationally, making the best decisions they can, given their computational limitations. To test this hypothesis, we consider the multi-armed bandit (MAB) problem. We examine a simple strategy for playing an MAB that can be implemented easily by a probabilistic finite automaton (PFA). Roughly speaking, the PFA sets certain expectations, and plays an arm as long as it meets them. If the PFA has sufficiently many states, it performs near-optimally. Its performance degrades gracefully as the number of states decreases. Moreover, the PFA acts in a \"human-like\" way, exhibiting a number of standard human biases, like an optimism bias and a negativity bias.</summary>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-30T16:42:08Z</published>\n",
      "    <arxiv:comment>10 pages, 4 pages, Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI), PMLR volume 124, 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.AI\"/>\n",
      "    <author>\n",
      "      <name>Xinming Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joseph Y. Halpern</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.16971v2</id>\n",
      "    <title>Improving robustness against common corruptions by covariate shift adaptation</title>\n",
      "    <updated>2020-10-23T04:37:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.16971v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.16971v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Today's state-of-the-art machine vision models are vulnerable to image corruptions like blurring or compression artefacts, limiting their performance in many real-world applications. We here argue that popular benchmarks to measure model robustness against common corruptions (like ImageNet-C) underestimate model robustness in many (but not all) application scenarios. The key insight is that in many scenarios, multiple unlabeled examples of the corruptions are available and can be used for unsupervised online adaptation. Replacing the activation statistics estimated by batch normalization on the training set with the statistics of the corrupted images consistently improves the robustness across 25 different popular computer vision models. Using the corrected statistics, ResNet-50 reaches 62.2% mCE on ImageNet-C compared to 76.7% without adaptation. With the more robust DeepAugment+AugMix model, we improve the state of the art achieved by a ResNet50 model up to date from 53.6% mCE to 45.4% mCE. Even adapting to a single sample improves robustness for the ResNet-50 and AugMix models, and 32 samples are sufficient to improve the current state of the art for a ResNet-50 architecture. We argue that results with adapted statistics should be included whenever reporting scores in corruption benchmarks and other out-of-distribution generalization settings.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-30T17:01:37Z</published>\n",
      "    <arxiv:comment>Accepted at the Thirty-fourth Conference on Neural Information Processing Systems. Web: https://domainadaptation.org/batchnorm/</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Steffen Schneider</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Evgenia Rusak</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luisa Eck</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Oliver Bringmann</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wieland Brendel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matthias Bethge</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.16984v2</id>\n",
      "    <title>Mining Documentation to Extract Hyperparameter Schemas</title>\n",
      "    <updated>2020-07-02T21:04:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.16984v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.16984v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>AI automation tools need machine-readable hyperparameter schemas to define their search spaces. At the same time, AI libraries often come with good human-readable documentation. While such documentation contains most of the necessary information, it is unfortunately not ready to consume by tools. This paper describes how to automatically mine Python docstrings in AI libraries to extract JSON Schemas for their hyperparameters. We evaluate our approach on 119 transformers and estimators from three different libraries and find that it is effective at extracting machine-readable schemas. Our vision is to reduce the burden to manually create and maintain such schemas for AI automation tools and broaden the reach of automation to larger libraries and richer schemas.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-30T17:32:47Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Guillaume Baudart</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peter D. Kirchner</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Martin Hirzel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kiran Kate</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.00045v1</id>\n",
      "    <title>K-Nearest Neighbour and Support Vector Machine Hybrid Classification</title>\n",
      "    <updated>2020-06-28T15:26:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.00045v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.00045v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, a novel K-Nearest Neighbour and Support Vector Machine hybrid classification technique has been proposed that is simple and robust. It is based on the concept of discriminative nearest neighbourhood classification. The technique consists of using K-Nearest Neighbour Classification for test samples satisfying a proximity condition. The patterns which do not pass the proximity condition are separated. This is followed by sifting the training set for a fixed number of patterns for every class which are closest to each separated test pattern respectively, based on the Euclidean distance metric. Subsequently, for every separated test sample, a Support Vector Machine is trained on the sifted training set patterns associated with it, and classification for the test sample is done. The proposed technique has been compared to the state of art in this research area. Three datasets viz. the United States Postal Service (USPS) Handwritten Digit Dataset, MNIST Dataset, and an Arabic numeral dataset, the Modified Arabic Digits Database, MADB, have been used to evaluate the performance of the algorithm. The algorithm generally outperforms the other algorithms with which it has been compared.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-28T15:26:56Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <arxiv:journal_ref>International Journal of Imaging and Robotics, Vol.19, No.4, pp.33-41, CESER Publications (2019)</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>A. M. Hafiz</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.00046v2</id>\n",
      "    <title>Fast Training of Deep Networks with One-Class CNNs</title>\n",
      "    <updated>2020-07-22T11:51:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.00046v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.00046v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>One-class CNNs have shown promise in novelty detection. However, very less work has been done on extending them to multiclass classification. The proposed approach is a viable effort in this direction. It uses one-class CNNs i.e., it trains one CNN per class, for multiclass classification. An ensemble of such one-class CNNs is used for multiclass classification. The benefits of the approach are generally better recognition accuracy while taking almost even half or two-thirds of the training time of a conventional multi-class deep network. The proposed approach has been applied successfully to face recognition and object recognition tasks. For face recognition, a 1000 frame RGB video, featuring many faces together, has been used for benchmarking of the proposed approach. Its database is available on request via e-mail. For object recognition, the Caltech-101 Image Database and 17Flowers Dataset have also been used. The experimental results support the claims made.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-28T14:53:45Z</published>\n",
      "    <arxiv:comment>Camera Ready: 2nd International Conference on Cybernetics, Cognition and Machine Learning Applications(ICCCMLA), 2020, India</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Abdul Mueed Hafiz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ghulam Mohiuddin Bhat</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.00095v2</id>\n",
      "    <title>Deep Learning for Vision-based Prediction: A Survey</title>\n",
      "    <updated>2020-07-22T15:33:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.00095v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.00095v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Vision-based prediction algorithms have a wide range of applications including autonomous driving, surveillance, human-robot interaction, weather prediction. The objective of this paper is to provide an overview of the field in the past five years with a particular focus on deep learning approaches. For this purpose, we categorize these algorithms into video prediction, action prediction, trajectory prediction, body motion prediction, and other prediction applications. For each category, we highlight the common architectures, training methods and types of data used. In addition, we discuss the common evaluation metrics and datasets used for vision-based prediction tasks. A database of all the information presented in this survey including, cross-referenced according to papers, datasets and metrics, can be found online at https://github.com/aras62/vision-based-prediction.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-30T20:26:46Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Amir Rasouli</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03074v1</id>\n",
      "    <title>Kernel Stein Generative Modeling</title>\n",
      "    <updated>2020-07-06T21:26:04Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03074v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03074v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We are interested in gradient-based Explicit Generative Modeling where samples can be derived from iterative gradient updates based on an estimate of the score function of the data distribution. Recent advances in Stochastic Gradient Langevin Dynamics (SGLD) demonstrates impressive results with energy-based models on high-dimensional and complex data distributions. Stein Variational Gradient Descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate a given distribution, based on functional gradient descent that decreases the KL divergence. SVGD has promising results on several Bayesian inference applications. However, applying SVGD on high dimensional problems is still under-explored. The goal of this work is to study high dimensional inference with SVGD. We first identify key challenges in practical kernel SVGD inference in high-dimension. We propose noise conditional kernel SVGD (NCK-SVGD), that works in tandem with the recently introduced Noise Conditional Score Network estimator. NCK is crucial for successful inference with SVGD in high dimension, as it adapts the kernel to the noise level of the score estimate. As we anneal the noise, NCK-SVGD targets the real data distribution. We then extend the annealed SVGD with an entropic regularization. We show that this offers a flexible control between sample quality and diversity, and verify it empirically by precision and recall evaluations. The NCK-SVGD produces samples comparable to GANs and annealed SGLD on computer vision benchmarks, including MNIST and CIFAR-10.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-06T21:26:04Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Wei-Cheng Chang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chun-Liang Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Youssef Mroueh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yiming Yang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03121v1</id>\n",
      "    <title>Multi-Armed Bandits with Local Differential Privacy</title>\n",
      "    <updated>2020-07-06T23:36:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03121v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03121v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper investigates the problem of regret minimization for multi-armed bandit (MAB) problems with local differential privacy (LDP) guarantee. In stochastic bandit systems, the rewards may refer to the users' activities, which may involve private information and the users may not want the agent to know. However, in many cases, the agent needs to know these activities to provide better services such as recommendations and news feeds. To handle this dilemma, we adopt differential privacy and study the regret upper and lower bounds for MAB algorithms with a given LDP guarantee. In this paper, we prove a lower bound and propose algorithms whose regret upper bounds match the lower bound up to constant factors. Numerical experiments also confirm our conclusions.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-06T23:36:20Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Wenbo Ren</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xingyu Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jia Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ness B. Shroff</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03137v2</id>\n",
      "    <title>Predicting Afrobeats Hit Songs Using Spotify Data</title>\n",
      "    <updated>2020-08-10T13:13:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03137v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03137v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This study approached the Hit Song Science problem with the aim of predicting which songs in the Afrobeats genre will become popular among Spotify listeners. A dataset of 2063 songs was generated through the Spotify Web API, with the provided audio features. Random Forest and Gradient Boosting algorithms proved to be successful with approximately F1 scores of 86%.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-07T00:14:30Z</published>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Adewale Adeagbo</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03181v1</id>\n",
      "    <title>Bidirectional Loss Function for Label Enhancement and Distribution Learning</title>\n",
      "    <updated>2020-07-07T03:02:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03181v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03181v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Label distribution learning (LDL) is an interpretable and general learning paradigm that has been applied in many real-world applications. In contrast to the simple logical vector in single-label learning (SLL) and multi-label learning (MLL), LDL assigns labels with a description degree to each instance. In practice, two challenges exist in LDL, namely, how to address the dimensional gap problem during the learning process of LDL and how to exactly recover label distributions from existing logical labels, i.e., Label Enhancement (LE). For most existing LDL and LE algorithms, the fact that the dimension of the input matrix is much higher than that of the output one is alway ignored and it typically leads to the dimensional reduction owing to the unidirectional projection. The valuable information hidden in the feature space is lost during the mapping process. To this end, this study considers bidirectional projections function which can be applied in LE and LDL problems simultaneously. More specifically, this novel loss function not only considers the mapping errors generated from the projection of the input space into the output one but also accounts for the reconstruction errors generated from the projection of the output space back to the input one. This loss function aims to potentially reconstruct the input data from the output data. Therefore, it is expected to obtain more accurate results. Finally, experiments on several real-world datasets are carried out to demonstrate the superiority of the proposed method for both LE and LDL.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-07T03:02:54Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xinyuan Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jihua Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qinghai Zheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhongyu Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruixin Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jun Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03213v1</id>\n",
      "    <title>Enabling On-Device CNN Training by Self-Supervised Instance Filtering and Error Map Pruning</title>\n",
      "    <updated>2020-07-07T05:52:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03213v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03213v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This work aims to enable on-device training of convolutional neural networks (CNNs) by reducing the computation cost at training time. CNN models are usually trained on high-performance computers and only the trained models are deployed to edge devices. But the statically trained model cannot adapt dynamically in a real environment and may result in low accuracy for new inputs. On-device training by learning from the real-world data after deployment can greatly improve accuracy. However, the high computation cost makes training prohibitive for resource-constrained devices. To tackle this problem, we explore the computational redundancies in training and reduce the computation cost by two complementary approaches: self-supervised early instance filtering on data level and error map pruning on the algorithm level. The early instance filter selects important instances from the input stream to train the network and drops trivial ones. The error map pruning further prunes out insignificant computations when training with the selected instances. Extensive experiments show that the computation cost is substantially reduced without any or with marginal accuracy loss. For example, when training ResNet-110 on CIFAR-10, we achieve 68% computation saving while preserving full accuracy and 75% computation saving with a marginal accuracy loss of 1.3%. Aggressive computation saving of 96% is achieved with less than 0.1% accuracy loss when quantization is integrated into the proposed approaches. Besides, when training LeNet on MNIST, we save 79% computation while boosting accuracy by 0.2%.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-07T05:52:37Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yawen Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhepeng Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yiyu Shi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jingtong Hu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03230v2</id>\n",
      "    <title>Robust Processing-In-Memory Neural Networks via Noise-Aware Normalization</title>\n",
      "    <updated>2020-11-24T05:35:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03230v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03230v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Analog computing hardwares, such as Processing-in-memory (PIM) accelerators, have gradually received more attention for accelerating the neural network computations. However, PIM accelerators often suffer from intrinsic noise in the physical components, making it challenging for neural network models to achieve the same performance as on the digital hardware. Previous works in mitigating intrinsic noise assumed the knowledge of the noise model, and retraining the neural networks accordingly was required. In this paper, we propose a noise-agnostic method to achieve robust neural network performance against any noise setting. Our key observation is that the degradation of performance is due to the distribution shifts in network activations, which are caused by the noise. To properly track the shifts and calibrate the biased distributions, we propose a \"noise-aware\" batch normalization layer, which is able to align the distributions of the activations under variational noise inherent in the analog environments. Our method is simple, easy to implement, general to various noise settings, and does not need to retrain the models. We conduct experiments on several tasks in computer vision, including classification, object detection and semantic segmentation. The results demonstrate the effectiveness of our method, achieving robust performance under a wide range of noise settings, more reliable than existing methods. We believe that our simple yet general method can facilitate the adoption of analog computing devices for neural networks.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-07T06:51:28Z</published>\n",
      "    <arxiv:comment>7 pages, 3 figure</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Li-Huang Tsai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shih-Chieh Chang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu-Ting Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jia-Yu Pan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wei Wei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Da-Cheng Juan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.02923v1</id>\n",
      "    <title>Descent-to-Delete: Gradient-Based Methods for Machine Unlearning</title>\n",
      "    <updated>2020-07-06T17:54:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.02923v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.02923v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study the data deletion problem for convex models. By leveraging techniques from convex optimization and reservoir sampling, we give the first data deletion algorithms that are able to handle an arbitrarily long sequence of adversarial updates while promising both per-deletion run-time and steady-state error that do not grow with the length of the update sequence. We also introduce several new conceptual distinctions: for example, we can ask that after a deletion, the entire state maintained by the optimization algorithm is statistically indistinguishable from the state that would have resulted had we retrained, or we can ask for the weaker condition that only the observable output is statistically indistinguishable from the observable output that would have resulted from retraining. We are able to give more efficient deletion algorithms under this weaker deletion criterion.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-06T17:54:35Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Seth Neel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aaron Roth</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Saeed Sharifi-Malvajerdi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03020v1</id>\n",
      "    <title>Deep Contextual Embeddings for Address Classification in E-commerce</title>\n",
      "    <updated>2020-07-06T19:06:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03020v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03020v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>E-commerce customers in developing nations like India tend to follow no fixed format while entering shipping addresses. Parsing such addresses is challenging because of a lack of inherent structure or hierarchy. It is imperative to understand the language of addresses, so that shipments can be routed without delays. In this paper, we propose a novel approach towards understanding customer addresses by deriving motivation from recent advances in Natural Language Processing (NLP). We also formulate different pre-processing steps for addresses using a combination of edit distance and phonetic algorithms. Then we approach the task of creating vector representations for addresses using Word2Vec with TF-IDF, Bi-LSTM and BERT based approaches. We compare these approaches with respect to sub-region classification task for North and South Indian cities. Through experiments, we demonstrate the effectiveness of generalized RoBERTa model, pre-trained over a large address corpus for language modelling task. Our proposed RoBERTa model achieves a classification accuracy of around 90% with minimal text preprocessing for sub-region classification task outperforming all other approaches. Once pre-trained, the RoBERTa model can be fine-tuned for various downstream tasks in supply chain like pincode suggestion and geo-coding. The model generalizes well for such tasks even with limited labelled data. To the best of our knowledge, this is the first of its kind research proposing a novel approach of understanding customer addresses in e-commerce domain by pre-training language models and fine-tuning them for different purposes.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-06T19:06:34Z</published>\n",
      "    <arxiv:comment>9 Pages, 8 Figures, AI for fashion supply chain, KDD2020 Workshop</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Shreyas Mangalgi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lakshya Kumar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ravindra Babu Tallamraju</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.01919v3</id>\n",
      "    <title>Efficient Marginalization of Discrete and Structured Latent Variables via Sparsity</title>\n",
      "    <updated>2020-12-28T10:33:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.01919v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.01919v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Training neural network models with discrete (categorical or structured) latent variables can be computationally challenging, due to the need for marginalization over large or combinatorial sets. To circumvent this issue, one typically resorts to sampling-based approximations of the true marginal, requiring noisy gradient estimators (e.g., score function estimator) or continuous relaxations with lower-variance reparameterized gradients (e.g., Gumbel-Softmax). In this paper, we propose a new training strategy which replaces these estimators by an exact yet efficient marginalization. To achieve this, we parameterize discrete distributions over latent assignments using differentiable sparse mappings: sparsemax and its structured counterparts. In effect, the support of these distributions is greatly reduced, which enables efficient marginalization. We report successful results in three tasks covering a range of latent variable modeling applications: a semisupervised deep generative model, a latent communication game, and a generative model with a bit-vector latent representation. In all cases, we obtain good performance while still achieving the practicality of sampling-based approximations.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-03T19:36:35Z</published>\n",
      "    <arxiv:comment>Accepted for spotlight presentation at NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Gonçalo M. Correia</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vlad Niculae</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wilker Aziz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>André F. T. Martins</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.01996v4</id>\n",
      "    <title>On the Asymptotic Linear Convergence Speed of Anderson Acceleration, Nesterov Acceleration, and Nonlinear GMRES</title>\n",
      "    <updated>2020-11-07T16:24:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.01996v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.01996v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We consider nonlinear convergence acceleration methods for fixed-point iteration $x_{k+1}=q(x_k)$, including Anderson acceleration (AA), nonlinear GMRES (NGMRES), and Nesterov-type acceleration (corresponding to AA with window size one). We focus on fixed-point methods that converge asymptotically linearly with convergence factor $ρ&lt;1$ and that solve an underlying fully smooth and non-convex optimization problem. It is often observed that AA and NGMRES substantially improve the asymptotic convergence behavior of the fixed-point iteration, but this improvement has not been quantified theoretically. We investigate this problem under simplified conditions. First, we consider stationary versions of AA and NGMRES, and determine coefficients that result in optimal asymptotic convergence factors, given knowledge of the spectrum of $q'(x)$ at the fixed point $x^*$. This allows us to understand and quantify the asymptotic convergence improvement that can be provided by nonlinear convergence acceleration, viewing $x_{k+1}=q(x_k)$ as a nonlinear preconditioner for AA and NGMRES. Second, for the case of infinite window size, we consider linear asymptotic convergence bounds for GMRES applied to the fixed-point iteration linearized about $x^*$. Since AA and NGMRES are equivalent to GMRES in the linear case, one may expect the GMRES convergence factors to be relevant for AA and NGMRES as $x_k \\rightarrow x^*$. Our results are illustrated numerically for a class of test problems from canonical tensor decomposition, comparing steepest descent and alternating least squares (ALS) as the fixed-point iterations that are accelerated by AA and NGMRES. Our numerical tests show that both approaches allow us to estimate asymptotic convergence speed for nonstationary AA and NGMRES with finite window size.</summary>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-04T03:35:35Z</published>\n",
      "    <arxiv:comment>Minor update. This version accepted for publication in SIAM Journal on Scientific Computing</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"math.OC\"/>\n",
      "    <author>\n",
      "      <name>Hans De Sterck</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yunhui He</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.02052v1</id>\n",
      "    <title>Choosing a sampling frequency for ECG QRS detection using convolutional networks</title>\n",
      "    <updated>2020-07-04T09:30:49Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.02052v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.02052v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Automated QRS detection methods depend on the ECG data which is sampled at a certain frequency, irrespective of filter-based traditional methods or convolutional network (CNN) based deep learning methods. These methods require a selection of the sampling frequency at which they operate in the very first place. While working with data from two different datasets, which are sampled at different frequencies, often, data from both the datasets may need to resample at a common target frequency, which may be the frequency of either of the datasets or could be a different one. However, choosing data sampled at a certain frequency may have an impact on the model's generalisation capacity, and complexity. There exist some studies that investigate the effects of ECG sample frequencies on traditional filter-based methods, however, an extensive study of the effect of ECG sample frequency on deep learning-based models (convolutional networks), exploring their generalisability and complexity is yet to be explored. This experimental research investigates the impact of six different sample frequencies (50, 100, 250, 500, 1000, and 2000Hz) on four different convolutional network-based models' generalisability and complexity in order to form a basis to decide on an appropriate sample frequency for the QRS detection task for a particular performance requirement. Intra-database tests report an accuracy improvement no more than approximately 0.6\\% from 100Hz to 250Hz and the shorter interquartile range for those two frequencies for all CNN-based models. The findings reveal that convolutional network-based deep learning models are capable of scoring higher levels of detection accuracies on ECG signals sampled at frequencies as low as 100Hz or 250Hz while maintaining lower model complexity (number of trainable parameters and training time).</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-04T09:30:49Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Ahsan Habib</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chandan Karmakar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>John Yearwood</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.01754v2</id>\n",
      "    <title>Differentiable Causal Discovery from Interventional Data</title>\n",
      "    <updated>2020-11-03T20:43:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.01754v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.01754v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Learning a causal directed acyclic graph from data is a challenging task that involves solving a combinatorial problem for which the solution is not always identifiable. A new line of work reformulates this problem as a continuous constrained optimization one, which is solved via the augmented Lagrangian method. However, most methods based on this idea do not make use of interventional data, which can significantly alleviate identifiability issues. This work constitutes a new step in this direction by proposing a theoretically-grounded method based on neural networks that can leverage interventional data. We illustrate the flexibility of the continuous-constrained framework by taking advantage of expressive neural architectures such as normalizing flows. We show that our approach compares favorably to the state of the art in a variety of settings, including perfect and imperfect interventions for which the targeted nodes may even be unknown.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-03T15:19:17Z</published>\n",
      "    <arxiv:comment>Appears in: Advances in Neural Information Processing Systems 34 (NeurIPS 2020). 46 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Philippe Brouillard</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sébastien Lachapelle</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexandre Lacoste</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Simon Lacoste-Julien</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexandre Drouin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.01814v1</id>\n",
      "    <title>DynNet: Physics-based neural architecture design for linear and nonlinear structural response modeling and prediction</title>\n",
      "    <updated>2020-07-03T17:05:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.01814v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.01814v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Data-driven models for predicting dynamic responses of linear and nonlinear systems are of great importance due to their wide application from probabilistic analysis to inverse problems such as system identification and damage diagnosis. In this study, a physics-based recurrent neural network model is designed that is able to learn the dynamics of linear and nonlinear multiple degrees of freedom systems given a ground motion. The model is able to estimate a complete set of responses, including displacement, velocity, acceleration, and internal forces. Compared to the most advanced counterparts, this model requires a smaller number of trainable variables while the accuracy of predictions is higher for long trajectories. In addition, the architecture of the recurrent block is inspired by differential equation solver algorithms and it is expected that this approach yields more generalized solutions. In the training phase, we propose multiple novel techniques to dramatically accelerate the learning process using smaller datasets, such as hardsampling, utilization of trajectory loss function, and implementation of a trust-region approach. Numerical case studies are conducted to examine the strength of the network to learn different nonlinear behaviors. It is shown that the network is able to capture different nonlinear behaviors of dynamic systems with very high accuracy and with no need for prior information or very large datasets.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-03T17:05:35Z</published>\n",
      "    <arxiv:comment>Submitted to Elsevier</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>Journal of Engineering Structures, Volume 229, 15 February 2021</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Soheil Sadeghi Eshkevari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Martin Takáč</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shamim N. Pakzad</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Majid Jahani</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1016/j.engstruct.2020.111582</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1016/j.engstruct.2020.111582\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.01833v1</id>\n",
      "    <title>PsychFM: Predicting your next gamble</title>\n",
      "    <updated>2020-07-03T17:41:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.01833v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.01833v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>There is a sudden surge to model human behavior due to its vast and diverse applications which includes modeling public policies, economic behavior and consumer behavior. Most of the human behavior itself can be modeled into a choice prediction problem. Prospect theory is a theoretical model that tries to explain the anomalies in choice prediction. These theories perform well in terms of explaining the anomalies but they lack precision. Since the behavior is person dependent, there is a need to build a model that predicts choices on a per-person basis. Looking on at the average persons choice may not necessarily throw light on a particular person's choice. Modeling the gambling problem on a per person basis will help in recommendation systems and related areas. A novel hybrid model namely psychological factorisation machine ( PsychFM ) has been proposed that involves concepts from machine learning as well as psychological theories. It outperforms the popular existing models namely random forest and factorisation machines for the benchmark dataset CPC-18. Finally,the efficacy of the proposed hybrid model has been verified by comparing with the existing models.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-03T17:41:14Z</published>\n",
      "    <arxiv:comment>To be published in International Joint Conference on Neural Networks (IJCNN) 2020 conference</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Prakash Rajan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Krishna P. Miyapuram</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.02559v1</id>\n",
      "    <title>Enhancing SAT solvers with glue variable predictions</title>\n",
      "    <updated>2020-07-06T07:16:49Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.02559v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.02559v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Modern SAT solvers routinely operate at scales that make it impractical to query a neural network for every branching decision. NeuroCore, proposed by Selsam and Bjorner, offered a proof-of-concept that neural networks can still accelerate SAT solvers by only periodically refocusing a score-based branching heuristic. However, that work suffered from several limitations: their modified solvers require GPU acceleration, further ablations showed that they were no better than a random baseline on the SATCOMP 2018 benchmark, and their training target of unsat cores required an expensive data pipeline which only labels relatively easy unsatisfiable problems. We address all these limitations, using a simpler network architecture allowing CPU inference for even large industrial problems with millions of clauses, and training instead to predict {\\em glue variables}---a target for which it is easier to generate labelled data, and which can also be formulated as a reinforcement learning task. We demonstrate the effectiveness of our approach by modifying the state-of-the-art SAT solver CaDiCaL, improving its performance on SATCOMP 2018 and SATRACE 2019 with supervised learning and its performance on a dataset of SHA-1 preimage attacks with reinforcement learning.</summary>\n",
      "    <category term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-06T07:16:49Z</published>\n",
      "    <arxiv:comment>8 pages, 5 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LO\"/>\n",
      "    <author>\n",
      "      <name>Jesse Michael Han</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.02643v1</id>\n",
      "    <title>GCN for HIN via Implicit Utilization of Attention and Meta-paths</title>\n",
      "    <updated>2020-07-06T11:09:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.02643v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.02643v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Heterogeneous information network (HIN) embedding, aiming to map the structure and semantic information in a HIN to distributed representations, has drawn considerable research attention. Graph neural networks for HIN embeddings typically adopt a hierarchical attention (including node-level and meta-path-level attentions) to capture the information from meta-path-based neighbors. However, this complicated attention structure often cannot achieve the function of selecting meta-paths due to severe overfitting. Moreover, when propagating information, these methods do not distinguish direct (one-hop) meta-paths from indirect (multi-hop) ones. But from the perspective of network science, direct relationships are often believed to be more essential, which can only be used to model direct information propagation. To address these limitations, we propose a novel neural network method via implicitly utilizing attention and meta-paths, which can relieve the severe overfitting brought by the current over-parameterized attention mechanisms on HIN. We first use the multi-layer graph convolutional network (GCN) framework, which performs a discriminative aggregation at each layer, along with stacking the information propagation of direct linked meta-paths layer-by-layer, realizing the function of attentions for selecting meta-paths in an indirect way. We then give an effective relaxation and improvement via introducing a new propagation operation which can be separated from aggregation. That is, we first model the whole propagation process with well-defined probabilistic diffusion dynamics, and then introduce a random graph-based constraint which allows it to reduce noise with the increase of layers. Extensive experiments demonstrate the superiority of the new approach over state-of-the-art methods.</summary>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-06T11:09:40Z</published>\n",
      "    <arxiv:primary_category term=\"cs.SI\"/>\n",
      "    <author>\n",
      "      <name>Di Jin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhizhi Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dongxiao He</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Carl Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Philip S. Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiawei Han</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.02265v2</id>\n",
      "    <title>AM-GCN: Adaptive Multi-channel Graph Convolutional Networks</title>\n",
      "    <updated>2020-07-11T03:23:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.02265v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.02265v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Graph Convolutional Networks (GCNs) have gained great popularity in tackling various analytics tasks on graph and network data. However, some recent studies raise concerns about whether GCNs can optimally integrate node features and topological structures in a complex graph with rich information. In this paper, we first present an experimental investigation. Surprisingly, our experimental results clearly show that the capability of the state-of-the-art GCNs in fusing node features and topological structures is distant from optimal or even satisfactory. The weakness may severely hinder the capability of GCNs in some classification tasks, since GCNs may not be able to adaptively learn some deep correlation information between topological structures and node features. Can we remedy the weakness and design a new type of GCNs that can retain the advantages of the state-of-the-art GCNs and, at the same time, enhance the capability of fusing topological structures and node features substantially? We tackle the challenge and propose an adaptive multi-channel graph convolutional networks for semi-supervised classification (AM-GCN). The central idea is that we extract the specific and common embeddings from node features, topological structures, and their combinations simultaneously, and use the attention mechanism to learn adaptive importance weights of the embeddings. Our extensive experiments on benchmark data sets clearly show that AM-GCN extracts the most correlated information from both node features and topological structures substantially, and improves the classification accuracy with a clear margin.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-05T08:16:03Z</published>\n",
      "    <arxiv:comment>11 pages, KDD2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xiao Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Meiqi Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Deyu Bo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peng Cui</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chuan Shi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jian Pei</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3394486.3403177</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3394486.3403177\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.02423v3</id>\n",
      "    <title>Participation is not a Design Fix for Machine Learning</title>\n",
      "    <updated>2020-08-11T15:39:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.02423v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.02423v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper critically examines existing modes of participation in design practice and machine learning. Cautioning against 'participation-washing', it suggests that the ML community must become attuned to possibly exploitative and extractive forms of community involvement and shift away from the prerogatives of context-independent scalability.</summary>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-05T18:59:37Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CY\"/>\n",
      "    <author>\n",
      "      <name>Mona Sloane</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Emanuel Moss</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Olaitan Awomolo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Laura Forlano</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.02114v1</id>\n",
      "    <title>A Novel Multi-Step Finite-State Automaton for Arbitrarily Deterministic Tsetlin Machine Learning</title>\n",
      "    <updated>2020-07-04T14:56:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.02114v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.02114v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Due to the high energy consumption and scalability challenges of deep learning, there is a critical need to shift research focus towards dealing with energy consumption constraints. Tsetlin Machines (TMs) are a recent approach to machine learning that has demonstrated significantly reduced energy usage compared to neural networks alike, while performing competitively accuracy-wise on several benchmarks. However, TMs rely heavily on energy-costly random number generation to stochastically guide a team of Tsetlin Automata to a Nash Equilibrium of the TM game. In this paper, we propose a novel finite-state learning automaton that can replace the Tsetlin Automata in TM learning, for increased determinism. The new automaton uses multi-step deterministic state jumps to reinforce sub-patterns. Simultaneously, flipping a coin to skip every $d$'th state update ensures diversification by randomization. The $d$-parameter thus allows the degree of randomization to be finely controlled. E.g., $d=1$ makes every update random and $d=\\infty$ makes the automaton completely deterministic. Our empirical results show that, overall, only substantial degrees of determinism reduces accuracy. Energy-wise, random number generation constitutes switching energy consumption of the TM, saving up to 11 mW power for larger datasets with high $d$ values. We can thus use the new $d$-parameter to trade off accuracy against energy consumption, to facilitate low-energy machine learning.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-04T14:56:05Z</published>\n",
      "    <arxiv:comment>10 pages, 8 figures, 7 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>K. Darshana Abeyrathna</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ole-Christoffer Granmo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rishad Shafik</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alex Yakovlev</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Adrian Wheeldon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jie Lei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Morten Goodwin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.02126v2</id>\n",
      "    <title>Deep Graph Random Process for Relational-Thinking-Based Speech Recognition</title>\n",
      "    <updated>2020-07-08T09:03:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.02126v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.02126v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Lying at the core of human intelligence, relational thinking is characterized by initially relying on innumerable unconscious percepts pertaining to relations between new sensory signals and prior knowledge, consequently becoming a recognizable concept or object through coupling and transformation of these percepts. Such mental processes are difficult to model in real-world problems such as in conversational automatic speech recognition (ASR), as the percepts (if they are modelled as graphs indicating relationships among utterances) are supposed to be innumerable and not directly observable. In this paper, we present a Bayesian nonparametric deep learning method called deep graph random process (DGP) that can generate an infinite number of probabilistic graphs representing percepts. We further provide a closed-form solution for coupling and transformation of these percept graphs for acoustic modeling. Our approach is able to successfully infer relations among utterances without using any relational data during training. Experimental evaluations on ASR tasks including CHiME-2 and CHiME-5 demonstrate the effectiveness and benefits of our method.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-04T15:27:57Z</published>\n",
      "    <arxiv:comment>Accepted at ICML 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Hengguan Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fuzhao Xue</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hao Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ye Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.02141v2</id>\n",
      "    <title>Off-Policy Exploitability-Evaluation in Two-Player Zero-Sum Markov Games</title>\n",
      "    <updated>2020-12-24T08:24:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.02141v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.02141v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Off-policy evaluation (OPE) is the problem of evaluating new policies using historical data obtained from a different policy. In the recent OPE context, most studies have focused on single-player cases, and not on multi-player cases. In this study, we propose OPE estimators constructed by the doubly robust and double reinforcement learning estimators in two-player zero-sum Markov games. The proposed estimators project exploitability that is often used as a metric for determining how close a policy profile (i.e., a tuple of policies) is to a Nash equilibrium in two-player zero-sum games. We prove the exploitability estimation error bounds for the proposed estimators. We then propose the methods to find the best candidate policy profile by selecting the policy profile that minimizes the estimated exploitability from a given policy profile class. We prove the regret bounds of the policy profiles selected by our methods. Finally, we demonstrate the effectiveness and performance of the proposed estimators through experiments.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"econ.EM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-04T16:51:56Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Kenshi Abe</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yusuke Kaneko</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.02168v1</id>\n",
      "    <title>Scalable Differentiable Physics for Learning and Control</title>\n",
      "    <updated>2020-07-04T19:07:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.02168v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.02168v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Differentiable physics is a powerful approach to learning and control problems that involve physical objects and environments. While notable progress has been made, the capabilities of differentiable physics solvers remain limited. We develop a scalable framework for differentiable physics that can support a large number of objects and their interactions. To accommodate objects with arbitrary geometry and topology, we adopt meshes as our representation and leverage the sparsity of contacts for scalable differentiable collision handling. Collisions are resolved in localized regions to minimize the number of optimization variables even when the number of simulated objects is high. We further accelerate implicit differentiation of optimization with nonlinear constraints. Experiments demonstrate that the presented framework requires up to two orders of magnitude less memory and computation in comparison to recent particle-based methods. We further validate the approach on inverse problems and control scenarios, where it outperforms derivative-free and model-free baselines by at least an order of magnitude.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-04T19:07:51Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>Proceedings of the 37th International Conference on Machine Learning, ICML 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Yi-Ling Qiao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Junbang Liang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vladlen Koltun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ming C. Lin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.02683v1</id>\n",
      "    <title>Depthwise Separable Convolutions Versus Recurrent Neural Networks for Monaural Singing Voice Separation</title>\n",
      "    <updated>2020-07-06T12:32:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.02683v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.02683v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent approaches for music source separation are almost exclusively based on deep neural networks, mostly employing recurrent neural networks (RNNs). Although RNNs are in many cases superior than other types of deep neural networks for sequence processing, they are known to have specific difficulties in training and parallelization, especially for the typically long sequences encountered in music source separation. In this paper we present a use-case of replacing RNNs with depth-wise separable (DWS) convolutions, which are a lightweight and faster variant of the typical convolutions. We focus on singing voice separation, employing an RNN architecture, and we replace the RNNs with DWS convolutions (DWS-CNNs). We conduct an ablation study and examine the effect of the number of channels and layers of DWS-CNNs on the source separation performance, by utilizing the standard metrics of signal-to-artifacts, signal-to-interference, and signal-to-distortion ratio. Our results show that by replacing RNNs with DWS-CNNs yields an improvement of 1.20, 0.06, 0.37 dB, respectively, while using only 20.57% of the amount of parameters of the RNN architecture.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-06T12:32:34Z</published>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Pyry Pyykkönen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Styliannos I. Mimilakis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Konstantinos Drossos</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tuomas Virtanen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.02801v1</id>\n",
      "    <title>Online Learning of Facility Locations</title>\n",
      "    <updated>2020-07-06T15:04:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.02801v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.02801v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we provide a rigorous theoretical investigation of an online learning version of the Facility Location problem which is motivated by emerging problems in real-world applications. In our formulation, we are given a set of sites and an online sequence of user requests. At each trial, the learner selects a subset of sites and then incurs a cost for each selected site and an additional cost which is the price of the user's connection to the nearest site in the selected subset. The problem may be solved by an application of the well-known Hedge algorithm. This would, however, require time and space exponential in the number of the given sites, which motivates our design of a novel quasi-linear time algorithm for this problem, with good theoretical guarantees on its performance.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-06T15:04:53Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Stephen Pasteris</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ting He</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fabio Vitale</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shiqiang Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mark Herbster</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.02510v1</id>\n",
      "    <title>An Application of Newsboy Problem in Supply Chain Optimisation of Online Fashion E-Commerce</title>\n",
      "    <updated>2020-07-06T03:27:42Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.02510v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.02510v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We describe a supply chain optimization model deployed in an online fashion e-commerce company in India called Myntra. Our model is simple, elegant and easy to put into service. The model utilizes historic data and predicts the quantity of Stock Keeping Units (SKUs) to hold so that the metrics \"Fulfilment Index\" and \"Utilization Index\" are optimized. We present the mathematics central to our model as well as compare the performance of our model with baseline regression based solutions.</summary>\n",
      "    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-06T03:27:42Z</published>\n",
      "    <arxiv:primary_category term=\"stat.AP\"/>\n",
      "    <author>\n",
      "      <name>Chandramouli Kamanchi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gopinath Ashok Kumar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nachiappan Sundaram</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ravindra Babu T</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chaithanya Bandi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.02515v1</id>\n",
      "    <title>Traffic Agent Trajectory Prediction Using Social Convolution and Attention Mechanism</title>\n",
      "    <updated>2020-07-06T03:48:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.02515v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.02515v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The trajectory prediction is significant for the decision-making of autonomous driving vehicles. In this paper, we propose a model to predict the trajectories of target agents around an autonomous vehicle. The main idea of our method is considering the history trajectories of the target agent and the influence of surrounding agents on the target agent. To this end, we encode the target agent history trajectories as an attention mask and construct a social map to encode the interactive relationship between the target agent and its surrounding agents. Given a trajectory sequence, the LSTM networks are firstly utilized to extract the features for all agents, based on which the attention mask and social map are formed. Then, the attention mask and social map are fused to get the fusion feature map, which is processed by the social convolution to obtain a fusion feature representation. Finally, this fusion feature is taken as the input of a variable-length LSTM to predict the trajectory of the target agent. We note that the variable-length LSTM enables our model to handle the case that the number of agents in the sensing scope is highly dynamic in traffic scenes. To verify the effectiveness of our method, we widely compare with several methods on a public dataset, achieving a 20% error decrease. In addition, the model satisfies the real-time requirement with the 32 fps.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-06T03:48:08Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Tao Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhixiong Nan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>He Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shitao Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nanning Zheng</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.11273v1</id>\n",
      "    <title>An Intelligent QoS Algorithm for Home Networks</title>\n",
      "    <updated>2020-07-22T08:46:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.11273v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.11273v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A novel quality of service (QoS) management algorithm for home networks is presented in this letter. The algorithm is based on service prediction for intelligent QoS management. The service prediction is carried out by a general regression neural network with a profile containing the past records of the service. A novel profile updating technique is proposed so that the profile size can remain small for fast bandwidth allocation. The analytical study and experiments over real LAN reveal that the proposed algorithm provides reliable QoS management for home networks with low computational overhead.</summary>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-22T08:46:56Z</published>\n",
      "    <arxiv:primary_category term=\"cs.NI\"/>\n",
      "    <arxiv:journal_ref>IEEE Communications Letters 23.4 (2019): 588-591</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Wen-Jyi Hwang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tsung-Ming Tai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bo-Ting Pan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tun-Yao Lou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yun-Jie Jhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.11344v2</id>\n",
      "    <title>DEAL: Deep Evidential Active Learning for Image Classification</title>\n",
      "    <updated>2020-10-27T07:35:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.11344v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.11344v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Convolutional Neural Networks (CNNs) have proven to be state-of-the-art models for supervised computer vision tasks, such as image classification. However, large labeled data sets are generally needed for the training and validation of such models. In many domains, unlabeled data is available but labeling is expensive, for instance when specific expert knowledge is required. Active Learning (AL) is one approach to mitigate the problem of limited labeled data. Through selecting the most informative and representative data instances for labeling, AL can contribute to more efficient learning of the model. Recent AL methods for CNNs propose different solutions for the selection of instances to be labeled. However, they do not perform consistently well and are often computationally expensive. In this paper, we propose a novel AL algorithm that efficiently learns from unlabeled data by capturing high prediction uncertainty. By replacing the softmax standard output of a CNN with the parameters of a Dirichlet density, the model learns to identify data instances that contribute efficiently to improving model performance during training. We demonstrate in several experiments with publicly available data that our method consistently outperforms other state-of-the-art AL approaches. It can be easily implemented and does not require extensive computational resources for training. Additionally, we are able to show the benefits of the approach on a real-world medical use case in the field of automated detection of visual signals for pneumonia on chest radiographs.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-22T11:14:23Z</published>\n",
      "    <arxiv:comment>Extended version of the paper \"DEAL: Deep Evidential Active Learning for Image Classification\" accepted for publication at ICMLA 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Patrick Hemmer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Niklas Kühl</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jakob Schöffer</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.11346v1</id>\n",
      "    <title>Towards Social &amp; Engaging Peer Learning: Predicting Backchanneling and Disengagement in Children</title>\n",
      "    <updated>2020-07-22T11:16:42Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.11346v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.11346v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Social robots and interactive computer applications have the potential to foster early language development in young children by acting as peer learning companions. However, studies have found that children only trust robots which behave in a natural and interpersonal manner. To help robots come across as engaging and attentive peer learning companions, we develop models to predict whether the listener will lose attention (Listener Disengagement Prediction, LDP) and the extent to which a robot should generate backchanneling responses (Backchanneling Extent Prediction, BEP) in the next few seconds. We pose LDP and BEP as time series classification problems and conduct several experiments to assess the impact of different time series characteristics and feature sets on the predictive performance of our model. Using statistics &amp; machine learning, we also examine which socio-demographic factors influence the amount of time children spend backchanneling and listening to their peers. To lend interpretability to our models, we also analyzed critical features responsible for their predictive performance. Our experiments revealed the utility of multimodal features such as pupil dilation, blink rate, head movements, facial action units which have never been used before. We also found that the dynamics of time series features are rich predictors of listener disengagement and backchanneling.</summary>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-22T11:16:42Z</published>\n",
      "    <arxiv:comment>14 pages, 6 figures, 5 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.HC\"/>\n",
      "    <author>\n",
      "      <name>Mononito Goswami</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Minkush Manuja</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maitree Leekha</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.13870v1</id>\n",
      "    <title>A Unified Framework of Surrogate Loss by Refactoring and Interpolation</title>\n",
      "    <updated>2020-07-27T21:16:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.13870v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.13870v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We introduce UniLoss, a unified framework to generate surrogate losses for training deep networks with gradient descent, reducing the amount of manual design of task-specific surrogate losses. Our key observation is that in many cases, evaluating a model with a performance metric on a batch of examples can be refactored into four steps: from input to real-valued scores, from scores to comparisons of pairs of scores, from comparisons to binary variables, and from binary variables to the final performance metric. Using this refactoring we generate differentiable approximations for each non-differentiable step through interpolation. Using UniLoss, we can optimize for different tasks and metrics using one unified framework, achieving comparable performance compared with task-specific losses. We validate the effectiveness of UniLoss on three tasks and four datasets. Code is available at https://github.com/princeton-vl/uniloss.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-27T21:16:51Z</published>\n",
      "    <arxiv:comment>Accepted to ECCV 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Lanlan Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mingzhe Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jia Deng</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.13911v2</id>\n",
      "    <title>Online neural connectivity estimation with ensemble stimulation</title>\n",
      "    <updated>2020-12-07T16:26:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.13911v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.13911v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>One of the primary goals of systems neuroscience is to relate the structure of neural circuits to their function, yet patterns of connectivity are difficult to establish when recording from large populations in behaving organisms. Many previous approaches have attempted to estimate functional connectivity between neurons using statistical modeling of observational data, but these approaches rely heavily on parametric assumptions and are purely correlational. Recently, however, holographic photostimulation techniques have made it possible to precisely target selected ensembles of neurons, offering the possibility of establishing direct causal links. Here, we propose a method based on noisy group testing that drastically increases the efficiency of this process in sparse networks. By stimulating small ensembles of neurons, we show that it is possible to recover binarized network connectivity with a number of tests that grows only logarithmically with population size under minimal statistical assumptions. Moreover, we prove that our approach, which reduces to an efficiently solvable convex optimization problem, can be related to Variational Bayesian inference on the binary connection weights, and we derive rigorous bounds on the posterior marginals. This allows us to extend our method to the streaming setting, where continuously updated posteriors allow for optional stopping, and we demonstrate the feasibility of inferring connectivity for networks of up to tens of thousands of neurons online. Finally, we show how our work can be theoretically linked to compressed sensing approaches, and compare results for connectivity inference in different settings.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-27T23:47:03Z</published>\n",
      "    <arxiv:comment>Revised and expanded version of the work that appeared in NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Anne Draelos</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eva A. Naumann</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>John M. Pearson</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.13977v1</id>\n",
      "    <title>Depth separation for reduced deep networks in nonlinear model reduction: Distilling shock waves in nonlinear hyperbolic problems</title>\n",
      "    <updated>2020-07-28T03:53:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.13977v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.13977v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Classical reduced models are low-rank approximations using a fixed basis designed to achieve dimensionality reduction of large-scale systems. In this work, we introduce reduced deep networks, a generalization of classical reduced models formulated as deep neural networks. We prove depth separation results showing that reduced deep networks approximate solutions of parametrized hyperbolic partial differential equations with approximation error $ε$ with $\\mathcal{O}(|\\log(ε)|)$ degrees of freedom, even in the nonlinear setting where solutions exhibit shock waves. We also show that classical reduced models achieve exponentially worse approximation rates by establishing lower bounds on the relevant Kolmogorov $N$-widths.</summary>\n",
      "    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-28T03:53:12Z</published>\n",
      "    <arxiv:primary_category term=\"math.NA\"/>\n",
      "    <author>\n",
      "      <name>Donsub Rim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luca Venturi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joan Bruna</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Benjamin Peherstorfer</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.14272v2</id>\n",
      "    <title>Symmetric Positive Semi-definite Riemannian Geometry with Application to Domain Adaptation</title>\n",
      "    <updated>2020-08-04T16:00:03Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.14272v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.14272v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we present new results on the Riemannian geometry of symmetric positive semi-definite (SPSD) matrices. First, based on an existing approximation of the geodesic path, we introduce approximations of the logarithmic and exponential maps. Second, we present a closed-form expression for Parallel Transport (PT). Third, we derive a canonical representation for a set of SPSD matrices. Based on these results, we propose an algorithm for Domain Adaptation (DA) and demonstrate its performance in two applications: fusion of hyper-spectral images and motion identification.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-28T14:39:36Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Or Yair</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Almog Lahav</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ronen Talmon</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.14430v3</id>\n",
      "    <title>Munchausen Reinforcement Learning</title>\n",
      "    <updated>2020-11-04T16:46:15Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.14430v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.14430v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Bootstrapping is a core mechanism in Reinforcement Learning (RL). Most algorithms, based on temporal differences, replace the true value of a transiting state by their current estimate of this value. Yet, another estimate could be leveraged to bootstrap RL: the current policy. Our core contribution stands in a very simple idea: adding the scaled log-policy to the immediate reward. We show that slightly modifying Deep Q-Network (DQN) in that way provides an agent that is competitive with distributional methods on Atari games, without making use of distributional RL, n-step returns or prioritized replay. To demonstrate the versatility of this idea, we also use it together with an Implicit Quantile Network (IQN). The resulting agent outperforms Rainbow on Atari, installing a new State of the Art with very little modifications to the original algorithm. To add to this empirical study, we provide strong theoretical insights on what happens under the hood -- implicit Kullback-Leibler regularization and increase of the action-gap.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-28T18:30:23Z</published>\n",
      "    <arxiv:comment>NeurIPS 2020. Code: https://github.com/google-research/google-research/tree/master/munchausen_rl</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Nino Vieillard</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Olivier Pietquin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matthieu Geist</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.13483v2</id>\n",
      "    <title>Post-Workshop Report on Science meets Engineering in Deep Learning, NeurIPS 2019, Vancouver</title>\n",
      "    <updated>2020-07-29T13:22:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.13483v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.13483v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Science meets Engineering in Deep Learning took place in Vancouver as part of the Workshop section of NeurIPS 2019. As organizers of the workshop, we created the following report in an attempt to isolate emerging topics and recurring themes that have been presented throughout the event. Deep learning can still be a complex mix of art and engineering despite its tremendous success in recent years. The workshop aimed at gathering people across the board to address seemingly contrasting challenges in the problems they are working on. As part of the call for the workshop, particular attention has been given to the interdependence of architecture, data, and optimization that gives rise to an enormous landscape of design and performance intricacies that are not well-understood. This year, our goal was to emphasize the following directions in our community: (i) identify obstacles in the way to better models and algorithms; (ii) identify the general trends from which we would like to build scientific and potentially theoretical understanding; and (iii) the rigorous design of scientific experiments and experimental protocols whose purpose is to resolve and pinpoint the origin of mysteries while ensuring reproducibility and robustness of conclusions. In the event, these topics emerged and were broadly discussed, matching our expectations and paving the way for new studies in these directions. While we acknowledge that the text is naturally biased as it comes through our lens, here we present an attempt to do a fair job of highlighting the outcome of the workshop.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-25T12:19:09Z</published>\n",
      "    <arxiv:comment>Report of NeurIPS 2019 workshop SEDL</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Levent Sagun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Caglar Gulcehre</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Adriana Romero</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Negar Rostamzadeh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stefano Sarao Mannelli</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.13520v1</id>\n",
      "    <title>Identification, Tracking and Impact: Understanding the trade secret of catchphrases</title>\n",
      "    <updated>2020-07-20T06:11:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.13520v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.13520v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Understanding the topical evolution in industrial innovation is a challenging problem. With the advancement in the digital repositories in the form of patent documents, it is becoming increasingly more feasible to understand the innovation secrets -- \"catchphrases\" of organizations. However, searching and understanding this enormous textual information is a natural bottleneck. In this paper, we propose an unsupervised method for the extraction of catchphrases from the abstracts of patents granted by the U.S. Patent and Trademark Office over the years. Our proposed system achieves substantial improvement, both in terms of precision and recall, against state-of-the-art techniques. As a second objective, we conduct an extensive empirical study to understand the temporal evolution of the catchphrases across various organizations. We also show how the overall innovation evolution in the form of introduction of newer catchphrases in an organization's patents correlates with the future citations received by the patents filed by that organization. Our code and data sets will be placed in the public domain soon.</summary>\n",
      "    <category term=\"cs.DL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-20T06:11:25Z</published>\n",
      "    <arxiv:comment>To be published in the proceedings of the ACM/IEEE Joint Conference on Digital Libraries (JCDL 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.DL\"/>\n",
      "    <author>\n",
      "      <name>Jagriti Jalal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mayank Singh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arindam Pal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lipika Dey</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Animesh Mukherjee</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.13524v2</id>\n",
      "    <title>Dynamic Relational Inference in Multi-Agent Trajectories</title>\n",
      "    <updated>2020-10-08T21:54:29Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.13524v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.13524v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Inferring interactions from multi-agent trajectories has broad applications in physics, vision and robotics. Neural relational inference (NRI) is a deep generative model that can reason about relations in complex dynamics without supervision. In this paper, we take a careful look at this approach for relational inference in multi-agent trajectories. First, we discover that NRI can be fundamentally limited without sufficient long-term observations. Its ability to accurately infer interactions degrades drastically for short output sequences. Next, we consider a more general setting of relational inference when interactions are changing overtime. We propose an extension ofNRI, which we call the DYnamic multi-AgentRelational Inference (DYARI) model that can reason about dynamic relations. We conduct exhaustive experiments to study the effect of model architecture, under-lying dynamics and training scheme on the performance of dynamic relational inference using a simulated physics system. We also showcase the usage of our model on real-world multi-agent basketball trajectories.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-16T19:15:16Z</published>\n",
      "    <arxiv:comment>submitted to ICLR 2021</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Ruichao Xiao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Manish Kumar Singh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rose Yu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.13547v1</id>\n",
      "    <title>Reconstruction Regularized Deep Metric Learning for Multi-label Image Classification</title>\n",
      "    <updated>2020-07-27T13:28:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.13547v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.13547v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we present a novel deep metric learning method to tackle the multi-label image classification problem. In order to better learn the correlations among images features, as well as labels, we attempt to explore a latent space, where images and labels are embedded via two unique deep neural networks, respectively. To capture the relationships between image features and labels, we aim to learn a \\emph{two-way} deep distance metric over the embedding space from two different views, i.e., the distance between one image and its labels is not only smaller than those distances between the image and its labels' nearest neighbors, but also smaller than the distances between the labels and other images corresponding to the labels' nearest neighbors. Moreover, a reconstruction module for recovering correct labels is incorporated into the whole framework as a regularization term, such that the label embedding space is more representative. Our model can be trained in an end-to-end manner. Experimental results on publicly available image datasets corroborate the efficacy of our method compared with the state-of-the-arts.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-27T13:28:50Z</published>\n",
      "    <arxiv:comment>Accepted by IEEE TNNLS</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Changsheng Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chong Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lixin Duan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peng Gao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kai Zheng</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.13609v1</id>\n",
      "    <title>Statistical Bootstrapping for Uncertainty Estimation in Off-Policy Evaluation</title>\n",
      "    <updated>2020-07-27T14:49:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.13609v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.13609v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In reinforcement learning, it is typical to use the empirically observed transitions and rewards to estimate the value of a policy via either model-based or Q-fitting approaches. Although straightforward, these techniques in general yield biased estimates of the true value of the policy. In this work, we investigate the potential for statistical bootstrapping to be used as a way to take these biased estimates and produce calibrated confidence intervals for the true value of the policy. We identify conditions - specifically, sufficient data size and sufficient coverage - under which statistical bootstrapping in this setting is guaranteed to yield correct confidence intervals. In practical situations, these conditions often do not hold, and so we discuss and propose mechanisms that can be employed to mitigate their effects. We evaluate our proposed method and show that it can yield accurate confidence intervals in a variety of conditions, including challenging continuous control environments and small data regimes.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-27T14:49:22Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Ilya Kostrikov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ofir Nachum</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.13250v2</id>\n",
      "    <title>Deep Active Learning for Solvability Prediction in Power Systems</title>\n",
      "    <updated>2020-12-22T07:30:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.13250v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.13250v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Traditional methods for solvability region analysis can only have inner approximations with inconclusive conservatism. Machine learning methods have been proposed to approach the real region. In this letter, we propose a deep active learning framework for power system solvability prediction. Compared with the passive learning methods where the training is performed after all instances are labeled, the active learning selects most informative instances to be label and therefore significantly reduce the size of labeled dataset for training. In the active learning framework, the acquisition functions, which correspond to different sampling strategies, are defined in terms of the on-the-fly posterior probability from the classifier. The IEEE 39-bus system is employed to validate the proposed framework, where a two-dimensional case is illustrated to visualize the effectiveness of the sampling method followed by the full-dimensional numerical experiments.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-27T00:13:09Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yichen Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jianzhe Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Feng Qiu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tianqi Hong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rui Yao</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12668v2</id>\n",
      "    <title>KPRNet: Improving projection-based LiDAR semantic segmentation</title>\n",
      "    <updated>2020-08-21T10:43:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12668v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12668v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Semantic segmentation is an important component in the perception systems of autonomous vehicles. In this work, we adopt recent advances in both image and point cloud segmentation to achieve a better accuracy in the task of segmenting LiDAR scans. KPRNet improves the convolutional neural network architecture of 2D projection methods and utilizes KPConv to replace the commonly used post-processing techniques with a learnable point-wise component which allows us to obtain more accurate 3D labels. With these improvements our model outperforms the current best method on the SemanticKITTI benchmark, reaching an mIoU of 63.1.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-24T17:35:14Z</published>\n",
      "    <arxiv:comment>\"ECCV 2020. Code and pre-trained models at https://github.com/DeyvidKochanov-TomTom/kprnet\"</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Deyvid Kochanov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fatemeh Karimi Nejadasl</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Olaf Booij</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12669v1</id>\n",
      "    <title>Scaling Graph Clustering with Distributed Sketches</title>\n",
      "    <updated>2020-07-24T17:38:04Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12669v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12669v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The unsupervised learning of community structure, in particular the partitioning vertices into clusters or communities, is a canonical and well-studied problem in exploratory graph analysis. However, like most graph analyses the introduction of immense scale presents challenges to traditional methods. Spectral clustering in distributed memory, for example, requires hundreds of expensive bulk-synchronous communication rounds to compute an embedding of vertices to a few eigenvectors of a graph associated matrix. Furthermore, the whole computation may need to be repeated if the underlying graph changes some low percentage of edge updates. We present a method inspired by spectral clustering where we instead use matrix sketches derived from random dimension-reducing projections. We show that our method produces embeddings that yield performant clustering results given a fully-dynamic stochastic block model stream using both the fast Johnson-Lindenstrauss and CountSketch transforms. We also discuss the effects of stochastic block model parameters upon the required dimensionality of the subsequent embeddings, and show how random projections could significantly improve the performance of graph clustering in distributed memory.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-24T17:38:04Z</published>\n",
      "    <arxiv:comment>9 pages, submitted to IEEE HPEC Graph Challenge 2020, comments welcome</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Benjamin W. Priest</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alec Dunton</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Geoffrey Sanders</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12685v3</id>\n",
      "    <title>Semantic Segmentation With Multi Scale Spatial Attention For Self Driving Cars</title>\n",
      "    <updated>2020-09-30T22:56:11Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12685v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12685v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we present a novel neural network using multi scale feature fusion at various scales for accurate and efficient semantic image segmentation. We used ResNet based feature extractor, dilated convolutional layers in downsampling part, atrous convolutional layers in the upsampling part and used concat operation to merge them. A new attention module is proposed to encode more contextual information and enhance the receptive field of the network. We present an in depth theoretical analysis of our network with training and optimization details. Our network was trained and tested on the Camvid dataset and Cityscapes dataset using mean accuracy per class and Intersection Over Union (IOU) as the evaluation metrics. Our model outperforms previous state of the art methods on semantic segmentation achieving mean IOU value of 74.12 while running at &gt;100 FPS.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-30T20:19:09Z</published>\n",
      "    <arxiv:comment>11 pages, 6 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Abhinav Sagar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>RajKumar Soundrapandiyan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12732v1</id>\n",
      "    <title>A PDE Approach to the Prediction of a Binary Sequence with Advice from Two History-Dependent Experts</title>\n",
      "    <updated>2020-07-24T18:46:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12732v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12732v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The prediction of a binary sequence is a classic example of online machine learning. We like to call it the 'stock prediction problem,' viewing the sequence as the price history of a stock that goes up or down one unit at each time step. In this problem, an investor has access to the predictions of two or more 'experts,' and strives to minimize her final-time regret with respect to the best-performing expert. Probability plays no role; rather, the market is assumed to be adversarial. We consider the case when there are two history-dependent experts, whose predictions are determined by the d most recent stock moves. Focusing on an appropriate continuum limit and using methods from optimal control, graph theory, and partial differential equations, we discuss strategies for the investor and the adversarial market, and we determine associated upper and lower bounds for the investor's final-time regret. When d is less than 4 our upper and lower bounds coalesce, so the proposed strategies are asymptotically optimal. Compared to other recent applications of partial differential equations to prediction, ours has a new element: there are two timescales, since the recent history changes at every step whereas regret accumulates more slowly.</summary>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-24T18:46:28Z</published>\n",
      "    <arxiv:primary_category term=\"math.OC\"/>\n",
      "    <author>\n",
      "      <name>Nadejda Drenska</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Robert V. Kohn</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12782v2</id>\n",
      "    <title>Linear discriminant initialization for feed-forward neural networks</title>\n",
      "    <updated>2020-08-18T17:12:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12782v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12782v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Informed by the basic geometry underlying feed forward neural networks, we initialize the weights of the first layer of a neural network using the linear discriminants which best distinguish individual classes. Networks initialized in this way take fewer training steps to reach the same level of training, and asymptotically have higher accuracy on training data.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.MG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-24T21:53:48Z</published>\n",
      "    <arxiv:comment>6 pages, 7 figures. Added comparison to larger randomly initialized networks. Updated tables to better illustrate trends in effect size</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Marissa Masden</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dev Sinha</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12799v2</id>\n",
      "    <title>Score-Based Explanations in Data Management and Machine Learning</title>\n",
      "    <updated>2020-08-19T01:53:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12799v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12799v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We describe some approaches to explanations for observed outcomes in data management and machine learning. They are based on the assignment of numerical scores to predefined and potentially relevant inputs. More specifically, we consider explanations for query answers in databases, and for results from classification models. The described approaches are mostly of a causal and counterfactual nature. We argue for the need to bring domain and semantic knowledge into score computations; and suggest some ways to do this.</summary>\n",
      "    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-24T23:13:27Z</published>\n",
      "    <arxiv:comment>Companion paper for a tutorial at the Scalable Uncertainty Management Conference (SUM'20). To appear in Proc. SUM'20. Minor fixes made</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.DB\"/>\n",
      "    <author>\n",
      "      <name>Leopoldo Bertossi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12802v1</id>\n",
      "    <title>Developing Personalized Models of Blood Pressure Estimation from Wearable Sensors Data Using Minimally-trained Domain Adversarial Neural Networks</title>\n",
      "    <updated>2020-07-24T23:26:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12802v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12802v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Blood pressure monitoring is an essential component of hypertension management and in the prediction of associated comorbidities. Blood pressure is a dynamic vital sign with frequent changes throughout a given day. Capturing blood pressure remotely and frequently (also known as ambulatory blood pressure monitoring) has traditionally been achieved by measuring blood pressure at discrete intervals using an inflatable cuff. However, there is growing interest in developing a cuffless ambulatory blood pressure monitoring system to measure blood pressure continuously. One such approach is by utilizing bioimpedance sensors to build regression models. A practical problem with this approach is that the amount of data required to confidently train such a regression model can be prohibitive. In this paper, we propose the application of the domain-adversarial training neural network (DANN) method on our multitask learning (MTL) blood pressure estimation model, allowing for knowledge transfer between subjects. Our proposed model obtains average root mean square error (RMSE) of $4.80 \\pm 0.74$ mmHg for diastolic blood pressure and $7.34 \\pm 1.88$ mmHg for systolic blood pressure when using three minutes of training data, $4.64 \\pm 0.60$ mmHg and $7.10 \\pm 1.79$ respectively when using four minutes of training data, and $4.48 \\pm 0.57$ mmHg and $6.79 \\pm 1.70$ respectively when using five minutes of training data. DANN improves training with minimal data in comparison to both directly training and to training with a pretrained model from another subject, decreasing RMSE by $0.19$ to $0.26$ mmHg (diastolic) and by $0.46$ to $0.67$ mmHg (systolic) in comparison to the best baseline models. We observe that four minutes of training data is the minimum requirement for our framework to exceed ISO standards within this cohort of patients.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-24T23:26:28Z</published>\n",
      "    <arxiv:comment>Published at MLHC 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Lida Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nathan C. Hurley</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bassem Ibrahim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Erica Spatz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Harlan M. Krumholz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Roozbeh Jafari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bobak J. Mortazavi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.13723v1</id>\n",
      "    <title>MaxDropout: Deep Neural Network Regularization Based on Maximum Output Values</title>\n",
      "    <updated>2020-07-27T17:55:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.13723v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.13723v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Different techniques have emerged in the deep learning scenario, such as Convolutional Neural Networks, Deep Belief Networks, and Long Short-Term Memory Networks, to cite a few. In lockstep, regularization methods, which aim to prevent overfitting by penalizing the weight connections, or turning off some units, have been widely studied either. In this paper, we present a novel approach called MaxDropout, a regularizer for deep neural network models that works in a supervised fashion by removing (shutting off) the prominent neurons (i.e., most active) in each hidden layer. The model forces fewer activated units to learn more representative information, thus providing sparsity. Regarding the experiments, we show that it is possible to improve existing neural networks and provide better results in neural networks when Dropout is replaced by MaxDropout. The proposed method was evaluated in image classification, achieving comparable results to existing regularizers, such as Cutout and RandomErasing, also improving the accuracy of neural networks that uses Dropout by replacing the existing layer by MaxDropout.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-27T17:55:54Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Claudio Filipi Goncalves do Santos</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Danilo Colombo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mateus Roder</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>João Paulo Papa</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.13729v1</id>\n",
      "    <title>Noisy Agents: Self-supervised Exploration by Predicting Auditory Events</title>\n",
      "    <updated>2020-07-27T17:59:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.13729v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.13729v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Humans integrate multiple sensory modalities (e.g. visual and audio) to build a causal understanding of the physical world. In this work, we propose a novel type of intrinsic motivation for Reinforcement Learning (RL) that encourages the agent to understand the causal effect of its actions through auditory event prediction. First, we allow the agent to collect a small amount of acoustic data and use K-means to discover underlying auditory event clusters. We then train a neural network to predict the auditory events and use the prediction errors as intrinsic rewards to guide RL exploration. Experimental results on Atari games show that our new intrinsic motivation significantly outperforms several state-of-the-art baselines. We further visualize our noisy agents' behavior in a physics environment and demonstrate that our newly designed intrinsic reward leads to the emergence of physical interaction behaviors (e.g. contact with objects).</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-27T17:59:08Z</published>\n",
      "    <arxiv:comment>Project page: http://noisy-agent.csail.mit.edu</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Chuang Gan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaoyu Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Phillip Isola</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Antonio Torralba</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joshua B. Tenenbaum</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.14175v2</id>\n",
      "    <title>PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph Embeddings</title>\n",
      "    <updated>2020-07-30T08:57:27Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.14175v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.14175v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recently, knowledge graph embeddings (KGEs) received significant attention, and several software libraries have been developed for training and evaluating KGEs. While each of them addresses specific needs, we re-designed and re-implemented PyKEEN, one of the first KGE libraries, in a community effort. PyKEEN 1.0 enables users to compose knowledge graph embedding models (KGEMs) based on a wide range of interaction models, training approaches, loss functions, and permits the explicit modeling of inverse relations. Besides, an automatic memory optimization has been realized in order to exploit the provided hardware optimally, and through the integration of Optuna extensive hyper-parameter optimization (HPO) functionalities are provided.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-28T12:54:28Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Mehdi Ali</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Max Berrendorf</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Charles Tapley Hoyt</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Laurent Vermue</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sahand Sharifzadeh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Volker Tresp</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jens Lehmann</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.14191v1</id>\n",
      "    <title>Tempered Sigmoid Activations for Deep Learning with Differential Privacy</title>\n",
      "    <updated>2020-07-28T13:19:45Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.14191v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.14191v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Because learning sometimes involves sensitive data, machine learning algorithms have been extended to offer privacy for training data. In practice, this has been mostly an afterthought, with privacy-preserving models obtained by re-running training with a different optimizer, but using the model architectures that already performed well in a non-privacy-preserving setting. This approach leads to less than ideal privacy/utility tradeoffs, as we show here. Instead, we propose that model architectures are chosen ab initio explicitly for privacy-preserving training.\n",
      "  To provide guarantees under the gold standard of differential privacy, one must bound as strictly as possible how individual training points can possibly affect model updates. In this paper, we are the first to observe that the choice of activation function is central to bounding the sensitivity of privacy-preserving deep learning. We demonstrate analytically and experimentally how a general family of bounded activation functions, the tempered sigmoids, consistently outperform unbounded activation functions like ReLU. Using this paradigm, we achieve new state-of-the-art accuracy on MNIST, FashionMNIST, and CIFAR10 without any modification of the learning procedure fundamentals or differential privacy analysis.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-28T13:19:45Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Nicolas Papernot</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abhradeep Thakurta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shuang Song</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Steve Chien</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Úlfar Erlingsson</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12858v1</id>\n",
      "    <title>Modal Uncertainty Estimation via Discrete Latent Representation</title>\n",
      "    <updated>2020-07-25T05:29:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12858v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12858v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Many important problems in the real world don't have unique solutions. It is thus important for machine learning models to be capable of proposing different plausible solutions with meaningful probability measures. In this work we introduce such a deep learning framework that learns the one-to-many mappings between the inputs and outputs, together with faithful uncertainty measures. We call our framework {\\it modal uncertainty estimation} since we model the one-to-many mappings to be generated through a set of discrete latent variables, each representing a latent mode hypothesis that explains the corresponding type of input-output relationship. The discrete nature of the latent representations thus allows us to estimate for any input the conditional probability distribution of the outputs very effectively. Both the discrete latent space and its uncertainty estimation are jointly learned during training. We motivate our use of discrete latent space through the multi-modal posterior collapse problem in current conditional generative models, then develop the theoretical background, and extensively validate our method on both synthetic and realistic tasks. Our framework demonstrates significantly more accurate uncertainty estimation than the current state-of-the-art methods, and is informative and convenient for practical use.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-25T05:29:34Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Di Qiu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lok Ming Lui</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12861v2</id>\n",
      "    <title>Adversarial Privacy-preserving Filter</title>\n",
      "    <updated>2020-08-04T05:12:11Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12861v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12861v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>While widely adopted in practical applications, face recognition has been critically discussed regarding the malicious use of face images and the potential privacy problems, e.g., deceiving payment system and causing personal sabotage. Online photo sharing services unintentionally act as the main repository for malicious crawler and face recognition applications. This work aims to develop a privacy-preserving solution, called Adversarial Privacy-preserving Filter (APF), to protect the online shared face images from being maliciously used.We propose an end-cloud collaborated adversarial attack solution to satisfy requirements of privacy, utility and nonaccessibility. Specifically, the solutions consist of three modules: (1) image-specific gradient generation, to extract image-specific gradient in the user end with a compressed probe model; (2) adversarial gradient transfer, to fine-tune the image-specific gradient in the server cloud; and (3) universal adversarial perturbation enhancement, to append image-independent perturbation to derive the final adversarial noise. Extensive experiments on three datasets validate the effectiveness and efficiency of the proposed solution. A prototype application is also released for further evaluation.We hope the end-cloud collaborated attack framework could shed light on addressing the issue of online multimedia sharing privacy-preserving issues from user side.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-25T05:41:00Z</published>\n",
      "    <arxiv:comment>Accepted by ACM Multimedia 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Jiaming Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jitao Sang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xian Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaowen Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yanfeng Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yongli Hu</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3394171.3413906</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3394171.3413906\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.12975v1</id>\n",
      "    <title>Deep Kernel Survival Analysis and Subject-Specific Survival Time Prediction Intervals</title>\n",
      "    <updated>2020-07-25T16:55:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.12975v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.12975v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Kernel survival analysis methods predict subject-specific survival curves and times using information about which training subjects are most similar to a test subject. These most similar training subjects could serve as forecast evidence. How similar any two subjects are is given by the kernel function. In this paper, we present the first neural network framework that learns which kernel functions to use in kernel survival analysis. We also show how to use kernel functions to construct prediction intervals of survival time estimates that are statistically valid for individuals similar to a test subject. These prediction intervals can use any kernel function, such as ones learned using our neural kernel learning framework or using random survival forests. Our experiments show that our neural kernel survival estimators are competitive with a variety of existing survival analysis methods, and that our prediction intervals can help compare different methods' uncertainties, even for estimators that do not use kernels. In particular, these prediction interval widths can be used as a new performance metric for survival analysis methods.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-25T16:55:16Z</published>\n",
      "    <arxiv:comment>Machine Learning for Healthcare conference (MLHC 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>George H. Chen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.13007v1</id>\n",
      "    <title>HATNet: An End-to-End Holistic Attention Network for Diagnosis of Breast Biopsy Images</title>\n",
      "    <updated>2020-07-25T20:42:21Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.13007v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.13007v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Training end-to-end networks for classifying gigapixel size histopathological images is computationally intractable. Most approaches are patch-based and first learn local representations (patch-wise) before combining these local representations to produce image-level decisions. However, dividing large tissue structures into patches limits the context available to these networks, which may reduce their ability to learn representations from clinically relevant structures. In this paper, we introduce a novel attention-based network, the Holistic ATtention Network (HATNet) to classify breast biopsy images. We streamline the histopathological image classification pipeline and show how to learn representations from gigapixel size images end-to-end. HATNet extends the bag-of-words approach and uses self-attention to encode global information, allowing it to learn representations from clinically relevant tissue structures without any explicit supervision. It outperforms the previous best network Y-Net, which uses supervision in the form of tissue-level segmentation masks, by 8%. Importantly, our analysis reveals that HATNet learns representations from clinically relevant structures, and it matches the classification accuracy of human pathologists for this challenging test set. Our source code is available at \\url{https://github.com/sacmehta/HATNet}</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-25T20:42:21Z</published>\n",
      "    <arxiv:comment>10 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Sachin Mehta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ximing Lu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Donald Weaver</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joann G. Elmore</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hannaneh Hajishirzi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Linda Shapiro</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.13024v2</id>\n",
      "    <title>Exploring Deep Hybrid Tensor-to-Vector Network Architectures for Regression Based Speech Enhancement</title>\n",
      "    <updated>2020-08-03T00:07:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.13024v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.13024v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper investigates different trade-offs between the number of model parameters and enhanced speech qualities by employing several deep tensor-to-vector regression models for speech enhancement. We find that a hybrid architecture, namely CNN-TT, is capable of maintaining a good quality performance with a reduced model parameter size. CNN-TT is composed of several convolutional layers at the bottom for feature extraction to improve speech quality and a tensor-train (TT) output layer on the top to reduce model parameters. We first derive a new upper bound on the generalization power of the convolutional neural network (CNN) based vector-to-vector regression models. Then, we provide experimental evidence on the Edinburgh noisy speech corpus to demonstrate that, in single-channel speech enhancement, CNN outperforms DNN at the expense of a small increment of model sizes. Besides, CNN-TT slightly outperforms the CNN counterpart by utilizing only 32\\% of the CNN model parameters. Besides, further performance improvement can be attained if the number of CNN-TT parameters is increased to 44\\% of the CNN model size. Finally, our experiments of multi-channel speech enhancement on a simulated noisy WSJ0 corpus demonstrate that our proposed hybrid CNN-TT architecture achieves better results than both DNN and CNN models in terms of better-enhanced speech qualities and smaller parameter sizes.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-25T22:21:05Z</published>\n",
      "    <arxiv:comment>Accepted to InterSpeech 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Jun Qi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hu Hu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yannan Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chao-Han Huck Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sabato Marco Siniscalchi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chin-Hui Lee</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.13296v1</id>\n",
      "    <title>Supervised Learning in Temporally-Coded Spiking Neural Networks with Approximate Backpropagation</title>\n",
      "    <updated>2020-07-27T03:39:49Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.13296v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.13296v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this work we propose a new supervised learning method for temporally-encoded multilayer spiking networks to perform classification. The method employs a reinforcement signal that mimics backpropagation but is far less computationally intensive. The weight update calculation at each layer requires only local data apart from this signal. We also employ a rule capable of producing specific output spike trains; by setting the target spike time equal to the actual spike time with a slight negative offset for key high-value neurons the actual spike time becomes as early as possible. In simulated MNIST handwritten digit classification, two-layer networks trained with this rule matched the performance of a comparable backpropagation based non-spiking network.</summary>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-27T03:39:49Z</published>\n",
      "    <arxiv:primary_category term=\"cs.NE\"/>\n",
      "    <author>\n",
      "      <name>Andrew Stephan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Brian Gardner</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Steven J. Koester</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andre Gruning</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.13406v2</id>\n",
      "    <title>Contraction Mapping of Feature Norms for Classifier Learning on the Data with Different Quality</title>\n",
      "    <updated>2020-07-28T01:07:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.13406v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.13406v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The popular softmax loss and its recent extensions have achieved great success in the deep learning-based image classification. However, the data for training image classifiers usually has different quality. Ignoring such problem, the correct classification of low quality data is hard to be solved. In this paper, we discover the positive correlation between the feature norm of an image and its quality through careful experiments on various applications and various deep neural networks. Based on this finding, we propose a contraction mapping function to compress the range of feature norms of training images according to their quality and embed this contraction mapping function into softmax loss or its extensions to produce novel learning objectives. The experiments on various classification applications, including handwritten digit recognition, lung nodule classification, face verification and face recognition, demonstrate that the proposed approach is promising to effectively deal with the problem of learning on the data with different quality and leads to the significant and stable improvements in the classification accuracy.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-27T09:53:55Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Weihua Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiabi Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Murong Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ling Ma</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.14911v2</id>\n",
      "    <title>Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?</title>\n",
      "    <updated>2020-09-02T08:22:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.14911v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.14911v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Out-of-training-distribution (OOD) scenarios are a common challenge of learning agents at deployment, typically leading to arbitrary deductions and poorly-informed decisions. In principle, detection of and adaptation to OOD scenes can mitigate their adverse effects. In this paper, we highlight the limitations of current approaches to novel driving scenes and propose an epistemic uncertainty-aware planning method, called \\emph{robust imitative planning} (RIP). Our method can detect and recover from some distribution shifts, reducing the overconfident and catastrophic extrapolations in OOD scenes. If the model's uncertainty is too great to suggest a safe course of action, the model can instead query the expert driver for feedback, enabling sample-efficient online adaptation, a variant of our method we term \\emph{adaptive robust imitative planning} (AdaRIP). Our methods outperform current state-of-the-art approaches in the nuScenes \\emph{prediction} challenge, but since no benchmark evaluating OOD detection and adaption currently exists to assess \\emph{control}, we introduce an autonomous car novel-scene benchmark, \\texttt{CARNOVEL}, to evaluate the robustness of driving agents to a suite of tasks with distribution shifts.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-26T11:07:32Z</published>\n",
      "    <arxiv:comment>The first two authors contributed equally. Accepted at ICML 2020. Supplementary videos and code available at: https://sites.google.com/view/av-detect-recover-adapt</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Angelos Filos</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Panagiotis Tigas</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rowan McAllister</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nicholas Rhinehart</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sergey Levine</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yarin Gal</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.14942v2</id>\n",
      "    <title>ELMV: an Ensemble-Learning Approach for Analyzing Electrical Health Records with Significant Missing Values</title>\n",
      "    <updated>2020-11-03T08:34:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.14942v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.14942v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Many real-world Electronic Health Record (EHR) data contains a large proportion of missing values. Leaving substantial portion of missing information unaddressed usually causes significant bias, which leads to invalid conclusion to be drawn. On the other hand, training a machine learning model with a much smaller nearly-complete subset can drastically impact the reliability and accuracy of model inference. Data imputation algorithms that attempt to replace missing data with meaningful values inevitably increase the variability of effect estimates with increased missingness, making it unreliable for hypothesis validation. We propose a novel Ensemble-Learning for Missing Value (ELMV) framework, which introduces an effective approach to construct multiple subsets of the original EHR data with a much lower missing rate, as well as mobilizing a dedicated support set for the ensemble learning in the purpose of reducing the bias caused by substantial missing values. ELMV has been evaluated on a real-world healthcare data for critical feature identification as well as a batch of simulation data with different missing rates for outcome prediction. On both experiments, ELMV clearly outperforms conventional missing value imputation methods and ensemble learning models.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-25T06:29:55Z</published>\n",
      "    <arxiv:comment>15 pages, 8 Figures, Typos corrected, Accepted to ACM-BCB 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Lucas J. Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hongwei Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jianzhong Di</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jin Chen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.15005v1</id>\n",
      "    <title>Resource Allocation via Graph Neural Networks in Free Space Optical Fronthaul Networks</title>\n",
      "    <updated>2020-06-26T14:20:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.15005v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.15005v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper investigates the optimal resource allocation in free space optical (FSO) fronthaul networks. The optimal allocation maximizes an average weighted sum-capacity subject to power limitation and data congestion constraints. Both adaptive power assignment and node selection are considered based on the instantaneous channel state information (CSI) of the links. By parameterizing the resource allocation policy, we formulate the problem as an unsupervised statistical learning problem. We consider the graph neural network (GNN) for the policy parameterization to exploit the FSO network structure with small-scale training parameters. The GNN is shown to retain the permutation equivariance that matches with the permutation equivariance of resource allocation policy in networks. The primal-dual learning algorithm is developed to train the GNN in a model-free manner, where the knowledge of system models is not required. Numerical simulations present the strong performance of the GNN relative to a baseline policy with equal power assignment and random node selection.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-26T14:20:48Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Zhan Gao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mark Eisen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alejandro Ribeiro</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.15043v1</id>\n",
      "    <title>Nearest Neighbour Based Estimates of Gradients: Sharp Nonasymptotic Bounds and Applications</title>\n",
      "    <updated>2020-06-26T15:19:43Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.15043v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.15043v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Motivated by a wide variety of applications, ranging from stochastic optimization to dimension reduction through variable selection, the problem of estimating gradients accurately is of crucial importance in statistics and learning theory. We consider here the classic regression setup, where a real valued square integrable r.v. $Y$ is to be predicted upon observing a (possibly high dimensional) random vector $X$ by means of a predictive function $f(X)$ as accurately as possible in the mean-squared sense and study a nearest-neighbour-based pointwise estimate of the gradient of the optimal predictive function, the regression function $m(x)=\\mathbb{E}[Y\\mid X=x]$. Under classic smoothness conditions combined with the assumption that the tails of $Y-m(X)$ are sub-Gaussian, we prove nonasymptotic bounds improving upon those obtained for alternative estimation methods. Beyond the novel theoretical results established, several illustrative numerical experiments have been carried out. The latter provide strong empirical evidence that the estimation method proposed works very well for various statistical problems involving gradient estimation, namely dimensionality reduction, stochastic gradient descent optimization and quantifying disentanglement.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-26T15:19:43Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Guillaume Ausset</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stephan Clémençon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>François Portier</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.15059v1</id>\n",
      "    <title>Computing Light Transport Gradients using the Adjoint Method</title>\n",
      "    <updated>2020-06-26T15:38:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.15059v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.15059v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper proposes a new equation from continuous adjoint theory to compute the gradient of quantities governed by the Transport Theory of light. Unlike discrete gradients ala autograd, which work at the code level, we first formulate the continuous theory and then discretize it. The key insight of this paper is that computing gradients in Transport Theory is akin to computing the importance, a quantity adjoint to radiance that satisfies an adjoint equation. Importance tells us where to look for light that matters. This is one of the key insights of this paper. In fact, this mathematical journey started from a whimsical thought that these adjoints might be related. Computing gradients is therefore no more complicated than computing the importance field. This insight and the following paper hopefully will shed some light on this complicated problem and ease the implementations of gradient computations in existing path tracers.</summary>\n",
      "    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-26T15:38:14Z</published>\n",
      "    <arxiv:comment>23 pages, 8 figures, unpublished manuscript</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.GR\"/>\n",
      "    <author>\n",
      "      <name>Jos Stam</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.15061v4</id>\n",
      "    <title>Intrinsic Reward Driven Imitation Learning via Generative Model</title>\n",
      "    <updated>2020-09-11T09:40:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.15061v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.15061v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Imitation learning in a high-dimensional environment is challenging. Most inverse reinforcement learning (IRL) methods fail to outperform the demonstrator in such a high-dimensional environment, e.g., Atari domain. To address this challenge, we propose a novel reward learning module to generate intrinsic reward signals via a generative model. Our generative method can perform better forward state transition and backward action encoding, which improves the module's dynamics modeling ability in the environment. Thus, our module provides the imitation agent both the intrinsic intention of the demonstrator and a better exploration ability, which is critical for the agent to outperform the demonstrator. Empirical results show that our method outperforms state-of-the-art IRL methods on multiple Atari games, even with one-life demonstration. Remarkably, our method achieves performance that is up to 5 times the performance of the demonstration.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-26T15:39:40Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xingrui Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yueming Lyu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ivor W. Tsang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.15081v1</id>\n",
      "    <title>On the Generalization Benefit of Noise in Stochastic Gradient Descent</title>\n",
      "    <updated>2020-06-26T16:18:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.15081v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.15081v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>It has long been argued that minibatch stochastic gradient descent can generalize better than large batch gradient descent in deep neural networks. However recent papers have questioned this claim, arguing that this effect is simply a consequence of suboptimal hyperparameter tuning or insufficient compute budgets when the batch size is large. In this paper, we perform carefully designed experiments and rigorous hyperparameter sweeps on a range of popular models, which verify that small or moderately large batch sizes can substantially outperform very large batches on the test set. This occurs even when both models are trained for the same number of iterations and large batches achieve smaller training losses. Our results confirm that the noise in stochastic gradients can enhance generalization. We study how the optimal learning rate schedule changes as the epoch budget grows, and we provide a theoretical account of our observations based on the stochastic differential equation perspective of SGD dynamics.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-26T16:18:54Z</published>\n",
      "    <arxiv:comment>Camera-ready version of ICML 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Samuel L. Smith</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Erich Elsen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Soham De</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.15103v1</id>\n",
      "    <title>DRACO: Co-Optimizing Hardware Utilization, and Performance of DNNs on Systolic Accelerator</title>\n",
      "    <updated>2020-06-26T17:06:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.15103v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.15103v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The number of processing elements (PEs) in a fixed-sized systolic accelerator is well matched for large and compute-bound DNNs; whereas, memory-bound DNNs suffer from PE underutilization and fail to achieve peak performance and energy efficiency. To mitigate this, specialized dataflow and/or micro-architectural techniques have been proposed. However, due to the longer development cycle and the rapid pace of evolution in the deep learning fields, these hardware-based solutions can be obsolete and ineffective in dealing with PE underutilization for state-of-the-art DNNs. In this work, we address the challenge of PE underutilization at the algorithm front and propose data reuse aware co-optimization (DRACO). This improves the PE utilization of memory-bound DNNs without any additional need for dataflow/micro-architecture modifications. Furthermore, unlike the previous co-optimization methods, DRACO not only maximizes performance and energy efficiency but also improves the predictive performance of DNNs. To the best of our knowledge, DRACO is the first work that resolves the resource underutilization challenge at the algorithm level and demonstrates a trade-off between computational efficiency, PE utilization, and predictive performance of DNN. Compared to the state-of-the-art row stationary dataflow, DRACO achieves 41.8% and 42.6% improvement in average PE utilization and inference latency (respectively) with negligible loss in predictive performance in MobileNetV1 on a $64\\times64$ systolic array. DRACO provides seminal insights for utilization-aware DNN design methodologies that can fully leverage the computation power of systolic array-based hardware accelerators.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-26T17:06:41Z</published>\n",
      "    <arxiv:comment>Accepted as a conference paper in the IEEE Computer Society Annual Symposium on VLSI (ISVLSI). Limassol, CYPRUS, July 6-8, 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Nandan Kumar Jha</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shreyas Ravishankar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sparsh Mittal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arvind Kaushik</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dipan Mandal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mahesh Chandra</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03856v1</id>\n",
      "    <title>BlockFLow: An Accountable and Privacy-Preserving Solution for Federated Learning</title>\n",
      "    <updated>2020-07-08T02:24:26Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03856v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03856v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Federated learning enables the development of a machine learning model among collaborating agents without requiring them to share their underlying data. However, malicious agents who train on random data, or worse, on datasets with the result classes inverted, can weaken the combined model. BlockFLow is an accountable federated learning system that is fully decentralized and privacy-preserving. Its primary goal is to reward agents proportional to the quality of their contribution while protecting the privacy of the underlying datasets and being resilient to malicious adversaries. Specifically, BlockFLow incorporates differential privacy, introduces a novel auditing mechanism for model contribution, and uses Ethereum smart contracts to incentivize good behavior. Unlike existing auditing and accountability methods for federated learning systems, our system does not require a centralized test dataset, sharing of datasets between the agents, or one or more trusted auditors; it is fully decentralized and resilient up to a 50% collusion attack in a malicious trust model. When run on the public Ethereum blockchain, BlockFLow uses the results from the audit to reward parties with cryptocurrency based on the quality of their contribution. We evaluated BlockFLow on two datasets that offer classification tasks solvable via logistic regression models. Our results show that the resultant auditing scores reflect the quality of the honest agents' datasets. Moreover, the scores from dishonest agents are statistically lower than those from the honest agents. These results, along with the reasonable blockchain costs, demonstrate the effectiveness of BlockFLow as an accountable federated learning system.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-08T02:24:26Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Vaikkunth Mugunthan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ravi Rahman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lalana Kagal</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03926v1</id>\n",
      "    <title>Non-parametric Models for Non-negative Functions</title>\n",
      "    <updated>2020-07-08T07:17:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03926v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03926v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Linear models have shown great effectiveness and flexibility in many fields such as machine learning, signal processing and statistics. They can represent  rich spaces of functions while preserving the convexity of the optimization problems where they are used, and are simple to evaluate, differentiate and integrate. However, for modeling non-negative functions, which are crucial for unsupervised learning, density estimation, or non-parametric Bayesian methods, linear models are not applicable directly. Moreover, current state-of-the-art models like generalized linear models either lead to non-convex optimization problems, or cannot be easily integrated. In this paper we provide the first model for non-negative functions which benefits from the same good properties of linear models. In particular, we prove that it admits a representer theorem and provide an efficient dual formulation for convex problems. We study its representation power, showing that the resulting space of functions is strictly richer than that of generalized linear models. Finally we extend the model and the theoretical results to functions with outputs in convex cones. The paper is complemented by an experimental evaluation of the model showing its effectiveness  in terms of formulation, algorithmic derivation and practical results on the problems of density estimation,  regression with heteroscedastic errors, and multiple quantile regression.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-08T07:17:28Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Ulysse Marteau-Ferey</name>\n",
      "      <arxiv:affiliation>PSL, DI-ENS, SIERRA</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Francis Bach</name>\n",
      "      <arxiv:affiliation>PSL, DI-ENS, SIERRA</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alessandro Rudi</name>\n",
      "      <arxiv:affiliation>PSL, DI-ENS, SIERRA</arxiv:affiliation>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03964v1</id>\n",
      "    <title>Responsive Safety in Reinforcement Learning by PID Lagrangian Methods</title>\n",
      "    <updated>2020-07-08T08:43:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03964v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03964v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Lagrangian methods are widely used algorithms for constrained optimization problems, but their learning dynamics exhibit oscillations and overshoot which, when applied to safe reinforcement learning, leads to constraint-violating behavior during agent training. We address this shortcoming by proposing a novel Lagrange multiplier update method that utilizes derivatives of the constraint function. We take a controls perspective, wherein the traditional Lagrange multiplier update behaves as \\emph{integral} control; our terms introduce \\emph{proportional} and \\emph{derivative} control, achieving favorable learning dynamics through damping and predictive measures. We apply our PID Lagrangian methods in deep RL, setting a new state of the art in Safety Gym, a safe RL benchmark. Lastly, we introduce a new method to ease controller tuning by providing invariance to the relative numerical scales of reward and cost. Our extensive experiments demonstrate improved performance and hyperparameter robustness, while our algorithms remain nearly as simple to derive and implement as the traditional Lagrangian approach.</summary>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-08T08:43:14Z</published>\n",
      "    <arxiv:comment>ICML 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"math.OC\"/>\n",
      "    <author>\n",
      "      <name>Adam Stooke</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joshua Achiam</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pieter Abbeel</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03995v3</id>\n",
      "    <title>MCU-Net: A framework towards uncertainty representations for decision support system patient referrals in healthcare contexts</title>\n",
      "    <updated>2020-08-25T11:12:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03995v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03995v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Incorporating a human-in-the-loop system when deploying automated decision support is critical in healthcare contexts to create trust, as well as provide reliable performance on a patient-to-patient basis. Deep learning methods while having high performance, do not allow for this patient-centered approach due to the lack of uncertainty representation. Thus, we present a framework of uncertainty representation evaluated for medical image segmentation, using MCU-Net which combines a U-Net with Monte Carlo Dropout, evaluated with four different uncertainty metrics. The framework augments this by adding a human-in-the-loop aspect based on an uncertainty threshold for automated referral of uncertain cases to a medical professional. We demonstrate that MCU-Net combined with epistemic uncertainty and an uncertainty threshold tuned for this application maximizes automated performance on an individual patient level, yet refers truly uncertain cases. This is a step towards uncertainty representations when deploying machine learning based decision support in healthcare settings.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-08T09:54:56Z</published>\n",
      "    <arxiv:comment>4 pages, 4 figures,Spotlight Talk at KDD 2020 - Applied Data Science for Healthcare Workshop &amp; presented at ICML 2020: Uncertainty and Robustness in Deep Learning</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Nabeel Seedat</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.04028v1</id>\n",
      "    <title>How benign is benign overfitting?</title>\n",
      "    <updated>2020-07-08T11:07:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.04028v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.04028v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We investigate two causes for adversarial vulnerability in deep neural networks: bad data and (poorly) trained models. When trained with SGD, deep neural networks essentially achieve zero training error, even in the presence of label noise, while also exhibiting good generalization on natural test data, something referred to as benign overfitting [2, 10]. However, these models are vulnerable to adversarial attacks. We identify label noise as one of the causes for adversarial vulnerability, and provide theoretical and empirical evidence in support of this. Surprisingly, we find several instances of label noise in datasets such as MNIST and CIFAR, and that robustly trained models incur training error on some of these, i.e. they don't fit the noise. However, removing noisy labels alone does not suffice to achieve adversarial robustness. Standard training procedures bias neural networks towards learning \"simple\" classification boundaries, which may be less robust than more complex ones. We observe that adversarial training does produce more complex decision boundaries. We conjecture that in part the need for complex decision boundaries arises from sub-optimal representation learning. By means of simple toy examples, we show theoretically how the choice of representation can drastically affect adversarial robustness.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-08T11:07:10Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Amartya Sanyal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Puneet K Dokania</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Varun Kanade</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Philip H. S. Torr</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.04082v2</id>\n",
      "    <title>Uncertainty-Aware Lookahead Factor Models for Quantitative Investing</title>\n",
      "    <updated>2020-07-15T16:52:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.04082v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.04082v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>On a periodic basis, publicly traded companies report fundamentals, financial data including revenue, earnings, debt, among others. Quantitative finance research has identified several factors, functions of the reported data that historically correlate with stock market performance. In this paper, we first show through simulation that if we could select stocks via factors calculated on future fundamentals (via oracle), that our portfolios would far outperform standard factor models. Motivated by this insight, we train deep nets to forecast future fundamentals from a trailing 5-year history. We propose lookahead factor models which plug these predicted future fundamentals into traditional factors. Finally, we incorporate uncertainty estimates from both neural heteroscedastic regression and a dropout-based heuristic, improving performance by adjusting our portfolios to avert risk. In retrospective analysis, we leverage an industry-grade portfolio simulator (backtester) to show simultaneous improvement in annualized return and Sharpe ratio. Specifically, the simulated annualized return for the uncertainty-aware model is 17.7% (vs 14.0% for a standard factor model) and the Sharpe ratio is 0.84 (vs 0.52).</summary>\n",
      "    <category term=\"q-fin.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-07T00:18:40Z</published>\n",
      "    <arxiv:primary_category term=\"q-fin.ST\"/>\n",
      "    <author>\n",
      "      <name>Lakshay Chauhan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>John Alberg</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zachary C. Lipton</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.04091v1</id>\n",
      "    <title>Bespoke vs. Prêt-à-Porter Lottery Tickets: Exploiting Mask Similarity for Trainable Sub-Network Finding</title>\n",
      "    <updated>2020-07-06T22:48:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.04091v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.04091v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The observation of sparse trainable sub-networks within over-parametrized networks - also known as Lottery Tickets (LTs) - has prompted inquiries around their trainability, scaling, uniqueness, and generalization properties. Across 28 combinations of image classification tasks and architectures, we discover differences in the connectivity structure of LTs found through different iterative pruning techniques, thus disproving their uniqueness and connecting emergent mask structure to the choice of pruning. In addition, we propose a consensus-based method for generating refined lottery tickets. This lottery ticket denoising procedure, based on the principle that parameters that always go unpruned across different tasks more reliably identify important sub-networks, is capable of selecting a meaningful portion of the architecture in an embarrassingly parallel way, while quickly discarding extra parameters without the need for further pruning iterations. We successfully train these sub-networks to performance comparable to that of ordinary lottery tickets.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-06T22:48:35Z</published>\n",
      "    <arxiv:comment>arXiv admin note: text overlap with arXiv:2001.05050</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Michela Paganini</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jessica Zosa Forde</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.04093v1</id>\n",
      "    <title>Machine learning and data analytics for the IoT</title>\n",
      "    <updated>2020-06-30T07:38:31Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.04093v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.04093v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The Internet of Things (IoT) applications have grown in exorbitant numbers, generating a large amount of data required for intelligent data processing. However, the varying IoT infrastructures (i.e., cloud, edge, fog) and the limitations of the IoT application layer protocols in transmitting/receiving messages become the barriers in creating intelligent IoT applications. These barriers prevent current intelligent IoT applications to adaptively learn from other IoT applications. In this paper, we critically review how IoT-generated data are processed for machine learning analysis and highlight the current challenges in furthering intelligent solutions in the IoT environment. Furthermore, we propose a framework to enable IoT applications to adaptively learn from other IoT applications and present a case study in how the framework can be applied to the real studies in the literature. Finally, we discuss the key factors that have an impact on future intelligent applications for the IoT.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-30T07:38:31Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Erwin Adi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Adnan Anwar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zubair Baig</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sherali Zeadally</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.04275v2</id>\n",
      "    <title>Graph Neural Networks for the Prediction of Substrate-Specific Organic Reaction Conditions</title>\n",
      "    <updated>2020-07-09T13:03:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.04275v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.04275v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present a systematic investigation using graph neural networks (GNNs) to model organic chemical reactions. To do so, we prepared a dataset collection of four ubiquitous reactions from the organic chemistry literature. We evaluate seven different GNN architectures for classification tasks pertaining to the identification of experimental reagents and conditions. We find that models are able to identify specific graph features that affect reaction conditions and lead to accurate predictions. The results herein show great promise in advancing molecular machine learning.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-08T17:21:00Z</published>\n",
      "    <arxiv:comment>23 pages, 10 tables, 13 figures, to appear in the ICML 2020 Workshop on Graph Representation Learning and Beyond (GRLB)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Serim Ryou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michael R. Maser</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexander Y. Cui</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Travis J. DeLano</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yisong Yue</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sarah E. Reisman</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.04399v1</id>\n",
      "    <title>Epidemic Exposure Notification with Smartwatch: A Proximity-Based Privacy-Preserving Approach</title>\n",
      "    <updated>2020-07-08T19:55:33Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.04399v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.04399v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Businesses planning for the post-pandemic world are looking for innovative ways to protect the health and welfare of their employees and customers. Wireless technologies can play a key role in assisting contact tracing to quickly halt a local infection outbreak and prevent further spread. In this work, we present a wearable proximity and exposure notification solution based on a smartwatch that also promotes safe physical distancing in business, hospitality, or recreational facilities. Our proximity-based privacy-preserving contact tracing (P$^3$CT) leverages the Bluetooth Low Energy (BLE) technology for reliable proximity sensing, and an ambient signature protocol for preserving identity. Proximity sensing exploits the received signal strength (RSS) to detect the user's interaction and thus classifying them into low- or high-risk with respect to a patient diagnosed with an infectious disease. More precisely, a user is notified of their exposure based on their interactions, in terms of distance and time, with a patient. Our privacy-preserving protocol uses the ambient signatures to ensure that users' identities be anonymized. We demonstrate the feasibility of our proposed solution through extensive experimentation.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-08T19:55:33Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Pai Chet Ng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Petros Spachos</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stefano Gregori</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Konstantinos Plataniotis</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.04446v1</id>\n",
      "    <title>StructureBoost: Efficient Gradient Boosting for Structured Categorical Variables</title>\n",
      "    <updated>2020-07-08T21:37:15Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.04446v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.04446v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Gradient boosting methods based on Structured Categorical Decision Trees (SCDT) have been demonstrated to outperform numerical and one-hot-encodings on problems where the categorical variable has a known underlying structure. However, the enumeration procedure in the SCDT is infeasible except for categorical variables with low or moderate cardinality. We propose and implement two methods to overcome the computational obstacles and efficiently perform Gradient Boosting on complex structured categorical variables. The resulting package, called StructureBoost, is shown to outperform established packages such as CatBoost and LightGBM on problems with categorical predictors that contain sophisticated structure. Moreover, we demonstrate that StructureBoost can make accurate predictions on unseen categorical values due to its knowledge of the underlying structure.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-08T21:37:15Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Brian Lucena</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.04660v1</id>\n",
      "    <title>Multi-task Regularization Based on Infrequent Classes for Audio Captioning</title>\n",
      "    <updated>2020-07-09T09:38:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.04660v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.04660v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Audio captioning is a multi-modal task, focusing on using natural language for describing the contents of general audio. Most audio captioning methods are based on deep neural networks, employing an encoder-decoder scheme and a dataset with audio clips and corresponding natural language descriptions (i.e. captions). A significant challenge for audio captioning is the distribution of words in the captions: some words are very frequent but acoustically non-informative, i.e. the function words (e.g. \"a\", \"the\"), and other words are infrequent but informative, i.e. the content words (e.g. adjectives, nouns). In this paper we propose two methods to mitigate this class imbalance problem. First, in an autoencoder setting for audio captioning, we weigh each word's contribution to the training loss inversely proportional to its number of occurrences in the whole dataset. Secondly, in addition to multi-class, word-level audio captioning task, we define a multi-label side task based on clip-level content word detection by training a separate decoder. We use the loss from the second task to regularize the jointly trained encoder for the audio captioning task. We evaluate our method using Clotho, a recently published, wide-scale audio captioning dataset, and our results show an increase of 37\\% relative improvement with SPIDEr metric over the baseline method.</summary>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-09T09:38:54Z</published>\n",
      "    <arxiv:primary_category term=\"cs.SD\"/>\n",
      "    <author>\n",
      "      <name>Emre Çakır</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Konstantinos Drossos</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tuomas Virtanen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.04693v1</id>\n",
      "    <title>Green Lighting ML: Confidentiality, Integrity, and Availability of Machine Learning Systems in Deployment</title>\n",
      "    <updated>2020-07-09T10:38:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.04693v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.04693v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Security and ethics are both core to ensuring that a machine learning system can be trusted. In production machine learning, there is generally a hand-off from those who build a model to those who deploy a model. In this hand-off, the engineers responsible for model deployment are often not privy to the details of the model and thus, the potential vulnerabilities associated with its usage, exposure, or compromise. Techniques such as model theft, model inversion, or model misuse may not be considered in model deployment, and so it is incumbent upon data scientists and machine learning engineers to understand these potential risks so they can communicate them to the engineers deploying and hosting their models. This is an open problem in the machine learning community and in order to help alleviate this issue, automated systems for validating privacy and security of models need to be developed, which will help to lower the burden of implementing these hand-offs and increasing the ubiquity of their adoption.</summary>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-09T10:38:59Z</published>\n",
      "    <arxiv:comment>2 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CY\"/>\n",
      "    <author>\n",
      "      <name>Abhishek Gupta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Erick Galinkin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.04456v1</id>\n",
      "    <title>An Efficient Data Imputation Technique for Human Activity Recognition</title>\n",
      "    <updated>2020-07-08T22:05:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.04456v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.04456v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The tremendous applications of human activity recognition are surging its span from health monitoring systems to virtual reality applications. Thus, the automatic recognition of daily life activities has become significant for numerous applications. In recent years, many datasets have been proposed to train the machine learning models for efficient monitoring and recognition of human daily living activities. However, the performance of machine learning models in activity recognition is crucially affected when there are incomplete activities in a dataset, i.e., having missing samples in dataset captures. Therefore, in this work, we propose a methodology for extrapolating the missing samples of a dataset to better recognize the human daily living activities. The proposed method efficiently pre-processes the data captures and utilizes the k-Nearest Neighbors (KNN) imputation technique to extrapolate the missing samples in dataset captures. The proposed methodology elegantly extrapolated a similar pattern of activities as they were in the real dataset.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-08T22:05:38Z</published>\n",
      "    <arxiv:comment>8 Pages, 8 Figures, 1 Table. Accepted in 14th Multi Conference on Computer Science and Information Systems 2020 (MCCSIS 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Ivan Miguel Pires</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Faisal Hussain</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nuno M. Garcia</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eftim Zdravevski</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.04458v1</id>\n",
      "    <title>Robust Bayesian Classification Using an Optimistic Score Ratio</title>\n",
      "    <updated>2020-07-08T22:25:29Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.04458v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.04458v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We build a Bayesian contextual classification model using an optimistic score ratio for robust binary classification when there is limited information on the class-conditional, or contextual, distribution. The optimistic score searches for the distribution that is most plausible to explain the observed outcomes in the testing sample among all distributions belonging to the contextual ambiguity set which is prescribed using a limited structural constraint on the mean vector and the covariance matrix of the underlying contextual distribution. We show that the Bayesian classifier using the optimistic score ratio is conceptually attractive, delivers solid statistical guarantees and is computationally tractable. We showcase the power of the proposed optimistic score ratio classifier on both synthetic and empirical data.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-08T22:25:29Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Viet Anh Nguyen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nian Si</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jose Blanchet</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.04472v1</id>\n",
      "    <title>Evaluation of Adversarial Training on Different Types of Neural Networks in Deep Learning-based IDSs</title>\n",
      "    <updated>2020-07-08T23:33:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.04472v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.04472v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Network security applications, including intrusion detection systems of deep neural networks, are increasing rapidly to make detection task of anomaly activities more accurate and robust. With the rapid increase of using DNN and the volume of data traveling through systems, different growing types of adversarial attacks to defeat them create a severe challenge. In this paper, we focus on investigating the effectiveness of different evasion attacks and how to train a resilience deep learning-based IDS using different Neural networks, e.g., convolutional neural networks (CNN) and recurrent neural networks (RNN). We use the min-max approach to formulate the problem of training robust IDS against adversarial examples using two benchmark datasets. Our experiments on different deep learning algorithms and different benchmark datasets demonstrate that defense using an adversarial training-based min-max approach improves the robustness against the five well-known adversarial attack methods.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-08T23:33:30Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Rana Abou Khamis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ashraf Matrawy</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.04532v1</id>\n",
      "    <title>A Study of Gradient Variance in Deep Learning</title>\n",
      "    <updated>2020-07-09T03:23:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.04532v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.04532v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The impact of gradient noise on training deep models is widely acknowledged but not well understood. In this context, we study the distribution of gradients during training. We introduce a method, Gradient Clustering, to minimize the variance of average mini-batch gradient with stratified sampling. We prove that the variance of average mini-batch gradient is minimized if the elements are sampled from a weighted clustering in the gradient space. We measure the gradient variance on common deep learning benchmarks and observe that, contrary to common assumptions, gradient variance increases during training, and smaller learning rates coincide with higher variance. In addition, we introduce normalized gradient variance as a statistic that better correlates with the speed of convergence compared to gradient variance.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-09T03:23:10Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Fartash Faghri</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Duvenaud</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David J. Fleet</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jimmy Ba</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03500v3</id>\n",
      "    <title>The Go Transformer: Natural Language Modeling for Game Play</title>\n",
      "    <updated>2020-09-07T19:37:21Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03500v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03500v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This work applies natural language modeling to generate plausible strategic moves in the ancient game of Go. We train the Generative Pretrained Transformer (GPT-2) to mimic the style of Go champions as archived in Smart Game Format (SGF), which offers a text description of move sequences. The trained model further generates valid but previously unseen strategies for Go. Because GPT-2 preserves punctuation and spacing, the raw output of the text generator provides inputs to game visualization and creative patterns, such as the Sabaki project's game engine using auto-replays. Results demonstrate that language modeling can capture both the sequencing format of championship Go games and their strategic formations. Compared to random game boards, the GPT-2 fine-tuning shows efficient opening move sequences favoring corner play over less advantageous center and side play. Game generation as a language modeling task offers novel approaches to more than 40 other board games where historical text annotation provides training data (e.g., Amazons &amp; Connect 4/6).</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-07T14:37:27Z</published>\n",
      "    <arxiv:comment>8 Pages, 5 Figures, 1 Table, IEEE Format, Ai4i 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Matthew Ciolino</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Noever</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Josh Kalin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03514v1</id>\n",
      "    <title>Imitation Learning Approach for AI Driving Olympics Trained on Real-world and Simulation Data Simultaneously</title>\n",
      "    <updated>2020-07-07T14:48:11Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03514v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03514v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we describe our winning approach to solving the Lane Following Challenge at the AI Driving Olympics Competition through imitation learning on a mixed set of simulation and real-world data. AI Driving Olympics is a two-stage competition: at stage one, algorithms compete in a simulated environment with the best ones advancing to a real-world final. One of the main problems that participants encounter during the competition is that algorithms trained for the best performance in simulated environments do not hold up in a real-world environment and vice versa. Classic control algorithms also do not translate well between tasks since most of them have to be tuned to specific driving conditions such as lighting, road type, camera position, etc. To overcome this problem, we employed the imitation learning algorithm and trained it on a dataset collected from sources both from simulation and real-world, forcing our model to perform equally well in all environments.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-07T14:48:11Z</published>\n",
      "    <arxiv:comment>Accepted to the Workshop on AI for Autonomous Driving (AIAD), the 37th International Conference on Machine Learning (ICML2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Mikita Sazanovich</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Konstantin Chaika</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kirill Krinkin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aleksei Shpilman</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03608v1</id>\n",
      "    <title>Backdoor attacks and defenses in feature-partitioned collaborative learning</title>\n",
      "    <updated>2020-07-07T16:45:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03608v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03608v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Since there are multiple parties in collaborative learning, malicious parties might manipulate the learning process for their own purposes through backdoor attacks. However, most of existing works only consider the federated learning scenario where data are partitioned by samples. The feature-partitioned learning can be another important scenario since in many real world applications, features are often distributed across different parties. Attacks and defenses in such scenario are especially challenging when the attackers have no labels and the defenders are not able to access the data and model parameters of other participants. In this paper, we show that even parties with no access to labels can successfully inject backdoor attacks, achieving high accuracy on both main and backdoor tasks. Next, we introduce several defense techniques, demonstrating that the backdoor can be successfully blocked by a combination of these techniques without hurting main task accuracy. To the best of our knowledge, this is the first systematical study to deal with backdoor attacks in the feature-partitioned collaborative learning framework.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-07T16:45:20Z</published>\n",
      "    <arxiv:comment>to be published in FL-ICML 2020 workshop</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yang Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhihao Yi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tianjian Chen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03618v1</id>\n",
      "    <title>Asymptotic behaviour of learning rates in Armijo's condition</title>\n",
      "    <updated>2020-07-07T16:49:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03618v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03618v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Fix a constant $0&lt;α&lt;1$. For a $C^1$ function $f:\\mathbb{R}^k\\rightarrow \\mathbb{R}$, a point $x$ and a positive number $δ&gt;0$, we say that Armijo's condition is satisfied if $f(x-δ\\nabla f(x))-f(x)\\leq -αδ||\\nabla f(x)||^2$. It is a basis for the well known Backtracking Gradient Descent (Backtracking GD) algorithm.\n",
      "  Consider a sequence $\\{x_n\\}$ defined by $x_{n+1}=x_n-δ_n\\nabla f(x_n)$, for positive numbers $δ_n$ for which Armijo's condition is satisfied. We show that if $\\{x_n\\}$ converges to a non-degenerate critical point, then $\\{δ_n\\}$ must be bounded. Moreover this boundedness can be quantified in terms of the norms of the Hessian $\\nabla ^2f$ and its inverse at the limit point. This complements the first author's results on Unbounded Backtracking GD, and shows that in case of convergence to a non-degenerate critical point the behaviour of Unbounded Backtracking GD is not too different from that of usual Backtracking GD. On the other hand, in case of convergence to a degenerate critical point the behaviours can be very much different. We run some experiments to illustrate that both scenrios can really happen.\n",
      "  In another part of the paper, we argue that Backtracking GD has the correct unit (according to a definition by Zeiler in his Adadelta's paper). The main point is that since learning rate in Backtracking GD is bound by Armijo's condition, it is not unitless.</summary>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-07T16:49:25Z</published>\n",
      "    <arxiv:comment>5 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"math.OC\"/>\n",
      "    <author>\n",
      "      <name>Tuyen Trung Truong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tuan Hang Nguyen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03626v1</id>\n",
      "    <title>What Gives the Answer Away? Question Answering Bias Analysis on Video QA Datasets</title>\n",
      "    <updated>2020-07-07T17:00:11Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03626v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03626v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Question answering biases in video QA datasets can mislead multimodal model to overfit to QA artifacts and jeopardize the model's ability to generalize. Understanding how strong these QA biases are and where they come from helps the community measure progress more accurately and provide researchers insights to debug their models. In this paper, we analyze QA biases in popular video question answering datasets and discover pretrained language models can answer 37-48% questions correctly without using any multimodal context information, far exceeding the 20% random guess baseline for 5-choose-1 multiple-choice questions. Our ablation study shows biases can come from annotators and type of questions. Specifically, annotators that have been seen during training are better predicted by the model and reasoning, abstract questions incur more biases than factual, direct questions. We also show empirically that using annotator-non-overlapping train-test splits can reduce QA biases for video QA datasets.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-07T17:00:11Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Jianing Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuying Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yongxin Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruitao Yi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amir Zadeh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Louis-Philippe Morency</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03281v1</id>\n",
      "    <title>Spectral Graph-based Features for Recognition of Handwritten Characters: A Case Study on Handwritten Devanagari Numerals</title>\n",
      "    <updated>2020-07-07T08:40:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03281v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03281v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Interpretation of different writing styles, unconstrained cursiveness and relationship between different primitive parts is an essential and challenging task for recognition of handwritten characters. As feature representation is inadequate, appropriate interpretation/description of handwritten characters seems to be a challenging task. Although existing research in handwritten characters is extensive, it still remains a challenge to get the effective representation of characters in feature space. In this paper, we make an attempt to circumvent these problems by proposing an approach that exploits the robust graph representation and spectral graph embedding concept to characterise and effectively represent handwritten characters, taking into account writing styles, cursiveness and relationships. For corroboration of the efficacy of the proposed method, extensive experiments were carried out on the standard handwritten numeral Computer Vision Pattern Recognition, Unit of Indian Statistical Institute Kolkata dataset. The experimental results demonstrate promising findings, which can be used in future studies.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-07T08:40:08Z</published>\n",
      "    <arxiv:comment>16 pages, 8 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <arxiv:journal_ref>Journal of Intelligent Systems,29,2018,799-813</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Mohammad Idrees Bhat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>B. Sharada</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1515/jisys-2017-0448</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1515/jisys-2017-0448\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03285v2</id>\n",
      "    <title>Stochastic Linear Bandits Robust to Adversarial Attacks</title>\n",
      "    <updated>2020-10-27T20:18:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03285v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03285v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We consider a stochastic linear bandit problem in which the rewards are not only subject to random noise, but also adversarial attacks subject to a suitable budget $C$ (i.e., an upper bound on the sum of corruption magnitudes across the time horizon). We provide two variants of a Robust Phased Elimination algorithm, one that knows $C$ and one that does not. Both variants are shown to attain near-optimal regret in the non-corrupted case $C = 0$, while incurring additional additive terms respectively having a linear and quadratic dependency on $C$ in general. We present algorithm independent lower bounds showing that these additive terms are near-optimal. In addition, in a contextual setting, we revisit a setup of diverse contexts, and show that a simple greedy algorithm is provably robust with a near-optimal additive regret term, despite performing no explicit exploration and not knowing $C$.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-07T09:00:57Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Ilija Bogunovic</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arpan Losalka</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andreas Krause</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jonathan Scarlett</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03331v1</id>\n",
      "    <title>GOLD-NAS: Gradual, One-Level, Differentiable</title>\n",
      "    <updated>2020-07-07T10:37:49Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03331v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03331v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>There has been a large literature of neural architecture search, but most existing work made use of heuristic rules that largely constrained the search flexibility. In this paper, we first relax these manually designed constraints and enlarge the search space to contain more than $10^{160}$ candidates. In the new space, most existing differentiable search methods can fail dramatically. We then propose a novel algorithm named Gradual One-Level Differentiable Neural Architecture Search (GOLD-NAS) which introduces a variable resource constraint to one-level optimization so that the weak operators are gradually pruned out from the super-network. In standard image classification benchmarks, GOLD-NAS can find a series of Pareto-optimal architectures within a single search procedure. Most of the discovered architectures were never studied before, yet they achieve a nice tradeoff between recognition accuracy and model complexity. We believe the new space and search algorithm can advance the search of differentiable NAS.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-07T10:37:49Z</published>\n",
      "    <arxiv:comment>14 pages, 5 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Kaifeng Bi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lingxi Xie</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xin Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Longhui Wei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qi Tian</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2007.03668v1</id>\n",
      "    <title>Near-tight closure bounds for Littlestone and threshold dimensions</title>\n",
      "    <updated>2020-07-07T17:56:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2007.03668v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2007.03668v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study closure properties for the Littlestone and threshold dimensions of binary hypothesis classes. Given classes $\\mathcal{H}_1, \\ldots, \\mathcal{H}_k$ of Boolean functions with bounded Littlestone (respectively, threshold) dimension, we establish an upper bound on the Littlestone (respectively, threshold) dimension of the class defined by applying an arbitrary binary aggregation rule to $\\mathcal{H}_1, \\ldots, \\mathcal{H}_k$. We also show that our upper bounds are nearly tight. Our upper bounds give an exponential (in $k$) improvement upon analogous bounds shown by Alon et al. (COLT 2020), thus answering a question posed by their work.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-07-07T17:56:06Z</published>\n",
      "    <arxiv:comment>7 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Badih Ghazi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Noah Golowich</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ravi Kumar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pasin Manurangsi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02244v1</id>\n",
      "    <title>Conditional Generative Adversarial Networks to Model Urban Outdoor Air Pollution</title>\n",
      "    <updated>2020-10-05T18:01:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02244v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02244v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This is a relevant problem because the design of most cities prioritizes the use of motorized vehicles, which has degraded air quality in recent years, having a negative effect on urban health. Modeling, predicting, and forecasting ambient air pollution is an important way to deal with this issue because it would be helpful for decision-makers and urban city planners to understand the phenomena and to take solutions. In general, data-driven methods for modeling, predicting, and forecasting outdoor pollution requires an important amount of data, which may limit their accuracy. In order to deal with such a lack of data, we propose to train models able to generate synthetic nitrogen dioxide daily time series according to a given classification that will allow an unlimited generation of realistic data. The main experimental results indicate that the proposed approach is able to generate accurate and diverse pollution daily time series, while requiring reduced computational time.</summary>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T18:01:10Z</published>\n",
      "    <arxiv:comment>Submitted to ICSC-CITIES 2020 conference</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.NE\"/>\n",
      "    <author>\n",
      "      <name>Jamal Toutouh</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02285v1</id>\n",
      "    <title>Model-Free Control of Dynamical Systems with Deep Reservoir Computing</title>\n",
      "    <updated>2020-10-05T18:59:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02285v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02285v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose and demonstrate a nonlinear control method that can be applied to unknown, complex systems where the controller is based on a type of artificial neural network known as a reservoir computer. In contrast to many modern neural-network-based control techniques, which are robust to system uncertainties but require a model nonetheless, our technique requires no prior knowledge of the system and is thus model-free. Further, our approach does not require an initial system identification step, resulting in a relatively simple and efficient learning process. Reservoir computers are well-suited to the control problem because they require small training data sets and remarkably low training times. By iteratively training and adding layers of reservoir computers to the controller, a precise and efficient control law is identified quickly. With examples on both numerical and high-speed experimental systems, we demonstrate that our approach is capable of controlling highly complex dynamical systems that display deterministic chaos to nontrivial target trajectories.</summary>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T18:59:51Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SY\"/>\n",
      "    <author>\n",
      "      <name>Daniel Canaday</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrew Pomerance</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniel J Gauthier</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02364v1</id>\n",
      "    <title>Understanding Classifier Mistakes with Generative Models</title>\n",
      "    <updated>2020-10-05T22:13:21Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02364v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02364v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Although deep neural networks are effective on supervised learning tasks, they have been shown to be brittle. They are prone to overfitting on their training distribution and are easily fooled by small adversarial perturbations. In this paper, we leverage generative models to identify and characterize instances where classifiers fail to generalize. We propose a generative model of the features extracted by a classifier, and show using rigorous hypothesis testing that errors tend to occur when features are assigned low-probability by our model. From this observation, we develop a detection criteria for samples on which a classifier is likely to fail at test time. In particular, we test against three different sources of classification failures: mistakes made on the test set due to poor model generalization, adversarial samples and out-of-distribution samples. Our approach is agnostic to class labels from the training set which makes it applicable to models trained in a semi-supervised way.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T22:13:21Z</published>\n",
      "    <arxiv:comment>17 pages, 10 figures. Submitted to ICLR2021</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Laëtitia Shao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yang Song</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stefano Ermon</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03192v1</id>\n",
      "    <title>Transformer Transducer: One Model Unifying Streaming and Non-streaming Speech Recognition</title>\n",
      "    <updated>2020-10-07T05:58:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03192v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03192v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper we present a Transformer-Transducer model architecture and a training technique to unify streaming and non-streaming speech recognition models into one model. The model is composed of a stack of transformer layers for audio encoding with no lookahead or right context and an additional stack of transformer layers on top trained with variable right context. In inference time, the context length for the variable context layers can be changed to trade off the latency and the accuracy of the model. We also show that we can run this model in a Y-model architecture with the top layers running in parallel in low latency and high latency modes. This allows us to have streaming speech recognition results with limited latency and delayed speech recognition results with large improvements in accuracy (20% relative improvement for voice-search task). We show that with limited right context (1-2 seconds of audio) and small additional latency (50-100 milliseconds) at the end of decoding, we can achieve similar accuracy with models using unlimited audio right context. We also present optimizations for audio and label encoders to speed up the inference in streaming and non-streaming speech decoding.</summary>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-07T05:58:28Z</published>\n",
      "    <arxiv:primary_category term=\"cs.SD\"/>\n",
      "    <author>\n",
      "      <name>Anshuman Tripathi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jaeyoung Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qian Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Han Lu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hasim Sak</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03207v1</id>\n",
      "    <title>Deep learning models for predictive maintenance: a survey, comparison, challenges and prospect</title>\n",
      "    <updated>2020-10-07T06:28:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03207v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03207v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Given the growing amount of industrial data spaces worldwide, deep learning solutions have become popular for predictive maintenance, which monitor assets to optimise maintenance tasks. Choosing the most suitable architecture for each use-case is complex given the number of examples found in literature. This work aims at facilitating this task by reviewing state-of-the-art deep learning architectures, and how they integrate with predictive maintenance stages to meet industrial companies' requirements (i.e. anomaly detection, root cause analysis, remaining useful life estimation). They are categorised and compared in industrial applications, explaining how to fill their gaps. Finally, open challenges and future research paths are presented.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-07T06:28:01Z</published>\n",
      "    <arxiv:comment>34 pages, 214 references, 8 tables, 2 equations, 2 figures, survey</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Oscar Serradilla</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ekhi Zugasti</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Urko Zurutuza</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04511v1</id>\n",
      "    <title>Sickle-cell disease diagnosis support selecting the most appropriate machinelearning method: Towards a general and interpretable approach for cellmorphology analysis from microscopy images</title>\n",
      "    <updated>2020-10-09T11:46:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04511v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04511v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this work we propose an approach to select the classification method and features, based on the state-of-the-art, with best performance for diagnostic support through peripheral blood smear images of red blood cells. In our case we used samples of patients with sickle-cell disease which can be generalized for other study cases. To trust the behavior of the proposed system, we also analyzed the interpretability.\n",
      "  We pre-processed and segmented microscopic images, to ensure high feature quality. We applied the methods used in the literature to extract the features from blood cells and the machine learning methods to classify their morphology. Next, we searched for their best parameters from the resulting data in the feature extraction phase. Then, we found the best parameters for every classifier using Randomized and Grid search.\n",
      "  For the sake of scientific progress, we published parameters for each classifier, the implemented code library, the confusion matrices with the raw data, and we used the public erythrocytesIDB dataset for validation. We also defined how to select the most important features for classification to decrease the complexity and the training time, and for interpretability purpose in opaque models. Finally, comparing the best performing classification methods with the state-of-the-art, we obtained better results even with interpretable model classifiers.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-09T11:46:38Z</published>\n",
      "    <arxiv:comment>35 pages, 10 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>Computers in Biology and Medicine, 2020, pending publication</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Nataša Petrović</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gabriel Moyà-Alcover</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Antoni Jaume-i-Capó</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Manuel González-Hidalgo</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04541v1</id>\n",
      "    <title>Upper Esophageal Sphincter Opening Segmentation with Convolutional Recurrent Neural Networks in High Resolution Cervical Auscultation</title>\n",
      "    <updated>2020-10-08T14:48:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04541v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04541v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Upper esophageal sphincter is an important anatomical landmark of the swallowing process commonly observed through the kinematic analysis of radiographic examinations that are vulnerable to subjectivity and clinical feasibility issues. Acting as the doorway of esophagus, upper esophageal sphincter allows the transition of ingested materials from pharyngeal into esophageal stages of swallowing and a reduced duration of opening can lead to penetration/aspiration and/or pharyngeal residue. Therefore, in this study we consider a non-invasive high resolution cervical auscultation-based screening tool to approximate the human ratings of upper esophageal sphincter opening and closure. Swallows were collected from 116 patients and a deep neural network was trained to produce a mask that demarcates the duration of upper esophageal sphincter opening. The proposed method achieved more than 90\\% accuracy and similar values of sensitivity and specificity when compared to human ratings even when tested over swallows from an independent clinical experiment. Moreover, the predicted opening and closure moments surprisingly fell within an inter-human comparable error of their human rated counterparts which demonstrates the clinical significance of high resolution cervical auscultation in replacing ionizing radiation-based evaluation of swallowing kinematics.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-08T14:48:14Z</published>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <arxiv:journal_ref>IEEE Journal of Biomedical and Health Informatics (2020)</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Yassin Khalifa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cara Donohue</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>James L. Coyle</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ervin Sejdić</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/JBHI.2020.3000057</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/JBHI.2020.3000057\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04554v1</id>\n",
      "    <title>Meta Graph Attention on Heterogeneous Graph with Node-Edge Co-evolution</title>\n",
      "    <updated>2020-10-09T13:19:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04554v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04554v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Graph neural networks have become an important tool for modeling structured data. In many real-world systems, intricate hidden information may exist, e.g., heterogeneity in nodes/edges, static node/edge attributes, and spatiotemporal node/edge features. However, most existing methods only take part of the information into consideration. In this paper, we present the Co-evolved Meta Graph Neural Network (CoMGNN), which applies meta graph attention to heterogeneous graphs with co-evolution of node and edge states. We further propose a spatiotemporal adaption of CoMGNN (ST-CoMGNN) for modeling spatiotemporal patterns on nodes and edges. We conduct experiments on two large-scale real-world datasets. Experimental results show that our models significantly outperform the state-of-the-art methods, demonstrating the effectiveness of encoding diverse information from different aspects.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-09T13:19:39Z</published>\n",
      "    <arxiv:comment>11pages, 4figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yucheng Lin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Huiting Hong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaoqing Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaodi Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pinghua Gong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jieping Ye</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04257v1</id>\n",
      "    <title>Fast Fourier Transformation for Optimizing Convolutional Neural Networks in Object Recognition</title>\n",
      "    <updated>2020-10-08T21:07:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04257v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04257v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper proposes to use Fast Fourier Transformation-based U-Net (a refined fully convolutional networks) and perform image convolution in neural networks. Leveraging the Fast Fourier Transformation, it reduces the image convolution costs involved in the Convolutional Neural Networks (CNNs) and thus reduces the overall computational costs. The proposed model identifies the object information from the images. We apply the Fast Fourier transform algorithm on an image data set to obtain more accessible information about the image data, before segmenting them through the U-Net architecture. More specifically, we implement the FFT-based convolutional neural network to improve the training time of the network. The proposed approach was applied to publicly available Broad Bioimage Benchmark Collection (BBBC) dataset. Our model demonstrated improvement in training time during convolution from $600-700$ ms/step to $400-500$ ms/step. We evaluated the accuracy of our model using Intersection over Union (IoU) metric showing significant improvements.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-08T21:07:55Z</published>\n",
      "    <arxiv:comment>Pre-print of a paper to appear in the proceedings of the IEEE International Conference on Machine Learning Applications (ICMLA 2020), 10 pages, 9 figures, 1 table</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Varsha Nair</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Moitrayee Chatterjee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Neda Tavakoli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Akbar Siami Namin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Craig Snoeyink</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04305v1</id>\n",
      "    <title>Neural Networks as Functional Classifiers</title>\n",
      "    <updated>2020-10-09T00:11:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04305v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04305v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In recent years, there has been considerable innovation in the world of predictive methodologies. This is evident by the relative domination of machine learning approaches in various classification competitions. While these algorithms have excelled at multivariate problems, they have remained dormant in the realm of functional data analysis. We extend notable deep learning methodologies to the domain of functional data for the purpose of classification problems. We highlight the effectiveness of our method in a number of classification applications such as classification of spectrographic data. Moreover, we demonstrate the performance of our classifier through simulation studies in which we compare our approach to the functional linear model and other conventional classification methods.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-09T00:11:01Z</published>\n",
      "    <arxiv:comment>28 pages, 4 figures, submitted to CSDA</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Barinder Thind</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kevin Multani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiguo Cao</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04308v2</id>\n",
      "    <title>Addressing the Real-world Class Imbalance Problem in Dermatology</title>\n",
      "    <updated>2020-11-14T03:45:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04308v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04308v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Class imbalance is a common problem in medical diagnosis, causing a standard classifier to be biased towards the common classes and perform poorly on the rare classes. This is especially true for dermatology, a specialty with thousands of skin conditions but many of which have low prevalence in the real world. Motivated by recent advances, we explore few-shot learning methods as well as conventional class imbalance techniques for the skin condition recognition problem and propose an evaluation setup to fairly assess the real-world utility of such approaches. We find the performance of few-show learning methods does not reach that of conventional class imbalance techniques, but combining the two approaches using a novel ensemble improves model performance, especially for rare classes. We conclude that ensembling can be useful to address the class imbalance problem, yet progress can further be accelerated by real-world evaluation setups for benchmarking new methods.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-09T00:24:55Z</published>\n",
      "    <arxiv:comment>Machine Learning for Health Workshop at NeurIPS 2020; 14 pages + 4 pages appendix, 8 figures, 6 appendix tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Wei-Hung Weng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jonathan Deaton</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vivek Natarajan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gamaleldin F. Elsayed</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuan Liu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04348v1</id>\n",
      "    <title>High-Order Relation Construction and Mining for Graph Matching</title>\n",
      "    <updated>2020-10-09T03:30:02Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04348v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04348v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Graph matching pairs corresponding nodes across two or more graphs. The problem is difficult as it is hard to capture the structural similarity across graphs, especially on large graphs. We propose to incorporate high-order information for matching large-scale graphs. Iterated line graphs are introduced for the first time to describe such high-order information, based on which we present a new graph matching method, called High-order Graph Matching Network (HGMN), to learn not only the local structural correspondence, but also the hyperedge relations across graphs. We theoretically prove that iterated line graphs are more expressive than graph convolution networks in terms of aligning nodes. By imposing practical constraints, HGMN is made scalable to large-scale graphs. Experimental results on a variety of settings have shown that, HGMN acquires more accurate matching results than the state-of-the-art, verifying our method effectively captures the structural similarity across different graphs.</summary>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-09T03:30:02Z</published>\n",
      "    <arxiv:primary_category term=\"cs.AI\"/>\n",
      "    <author>\n",
      "      <name>Hui Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liyao Xiang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Youmin Le</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaoying Gan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuting Jia</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luoyi Fu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xinbing Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04360v1</id>\n",
      "    <title>Few-shot Learning for Spatial Regression</title>\n",
      "    <updated>2020-10-09T04:05:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04360v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04360v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose a few-shot learning method for spatial regression. Although Gaussian processes (GPs) have been successfully used for spatial regression, they require many observations in the target task to achieve a high predictive performance. Our model is trained using spatial datasets on various attributes in various regions, and predicts values on unseen attributes in unseen regions given a few observed data. With our model, a task representation is inferred from given small data using a neural network. Then, spatial values are predicted by neural networks with a GP framework, in which task-specific properties are controlled by the task representations. The GP framework allows us to analytically obtain predictions that are adapted to small data. By using the adapted predictions in the objective function, we can train our model efficiently and effectively so that the test predictive performance improves when adapted to newly given small data. In our experiments, we demonstrate that the proposed method achieves better predictive performance than existing meta-learning methods using spatial datasets.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-09T04:05:01Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Tomoharu Iwata</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yusuke Tanaka</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04661v1</id>\n",
      "    <title>Using Graph Neural Networks for Mass Spectrometry Prediction</title>\n",
      "    <updated>2020-10-09T16:06:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04661v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04661v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Detecting and quantifying products of cellular metabolism using Mass Spectrometry (MS) has already shown great promise in many biological and biomedical applications. The biggest challenge in metabolomics is annotation, where measured spectra are assigned chemical identities. Despite advances, current methods provide limited annotation for measured spectra. Here, we explore using graph neural networks (GNNs) to predict the spectra. The input to our model is a molecular graph. The model is trained and tested on the NIST 17 LC-MS dataset. We compare our results to NEIMS, a neural network model that utilizes molecular fingerprints as inputs. Our results show that GNN-based models offer higher performance than NEIMS. Importantly, we show that ranking results heavily depend on the candidate set size and on the similarity of the candidates to the target molecule, thus highlighting the need for consistent, well-characterized evaluation protocols for this domain.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-09T16:06:57Z</published>\n",
      "    <arxiv:comment>4 pages, 3 figures, submitting to MLCB</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Hao Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liping Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Soha Hassoun</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04736v1</id>\n",
      "    <title>Evaluating and Characterizing Human Rationales</title>\n",
      "    <updated>2020-10-09T18:00:04Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04736v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04736v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics. To unpack this finding, we propose improved metrics to account for model-dependent baseline performance. We then propose two methods to further characterize rationale quality, one based on model retraining and one on using \"fidelity curves\" to reveal properties such as irrelevance and redundancy. Our work leads to actionable suggestions for evaluating and characterizing rationales.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-09T18:00:04Z</published>\n",
      "    <arxiv:comment>14 pages, 15 figures, to appear in EMNLP 2020. Code is available at https://github.com/BoulderDS/evaluating-human-rationales</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Samuel Carton</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anirudh Rathore</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chenhao Tan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04816v1</id>\n",
      "    <title>Characterizing Policy Divergence for Personalized Meta-Reinforcement Learning</title>\n",
      "    <updated>2020-10-09T21:31:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04816v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04816v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Despite ample motivation from costly exploration and limited trajectory data, rapidly adapting to new environments with few-shot reinforcement learning (RL) can remain a challenging task, especially with respect to personalized settings. Here, we consider the problem of recommending optimal policies to a set of multiple entities each with potentially different characteristics, such that individual entities may parameterize distinct environments with unique transition dynamics. Inspired by existing literature in meta-learning, we extend previous work by focusing on the notion that certain environments are more similar to each other than others in personalized settings, and propose a model-free meta-learning algorithm that prioritizes past experiences by relevance during gradient-based adaptation. Our algorithm involves characterizing past policy divergence through methods in inverse reinforcement learning, and we illustrate how such metrics are able to effectively distinguish past policy parameters by the environment they were deployed in, leading to more effective fast adaptation during test time. To study personalization more effectively we introduce a navigation testbed to specifically incorporate environment diversity across training episodes, and demonstrate that our approach outperforms meta-learning alternatives with respect to few-shot reinforcement learning in personalized settings.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-09T21:31:53Z</published>\n",
      "    <arxiv:comment>Deep Reinforcement Learning Workshop, NeurIPS 2019; Workshop on Meta-Learning (Meta-Learn), NeurIPS 2019</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Michael Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04827v1</id>\n",
      "    <title>Towards Self-Regulating AI: Challenges and Opportunities of AI Model Governance in Financial Services</title>\n",
      "    <updated>2020-10-09T22:12:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04827v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04827v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>AI systems have found a wide range of application areas in financial services. Their involvement in broader and increasingly critical decisions has escalated the need for compliance and effective model governance. Current governance practices have evolved from more traditional financial applications and modeling frameworks. They often struggle with the fundamental differences in AI characteristics such as uncertainty in the assumptions, and the lack of explicit programming. AI model governance frequently involves complex review flows and relies heavily on manual steps. As a result, it faces serious challenges in effectiveness, cost, complexity, and speed. Furthermore, the unprecedented rate of growth in the AI model complexity raises questions on the sustainability of the current practices. This paper focuses on the challenges of AI model governance in the financial services industry. As a part of the outlook, we present a system-level framework towards increased self-regulation for robustness and compliance. This approach aims to enable potential solution opportunities through increased automation and the integration of monitoring, management, and mitigation capabilities. The proposed framework also provides model governance and risk management improved capabilities to manage model risk during deployment.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-fin.GN\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-09T22:12:22Z</published>\n",
      "    <arxiv:comment>8 pages, 7 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>Proceedings of the 1st International Conference on AI in Finance (ICAIF '20), October 15-16, 2020, New York</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Eren Kurshan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hongda Shen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiahao Chen</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3383455.3422564</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3383455.3422564\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04836v1</id>\n",
      "    <title>Paying down metadata debt: learning the representation of concepts using topic models</title>\n",
      "    <updated>2020-10-09T22:42:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04836v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04836v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We introduce a data management problem called metadata debt, to identify the mapping between data concepts and their logical representations. We describe how this mapping can be learned using semisupervised topic models based on low-rank matrix factorizations that account for missing and noisy labels, coupled with sparsity penalties to improve localization and interpretability. We introduce a gauge transformation approach that allows us to construct explicit associations between topics and concept labels, and thus assign meaning to topics. We also show how to use this topic model for semisupervised learning tasks like extrapolating from known labels, evaluating possible errors in existing labels, and predicting missing features. We show results from this topic model in predicting subject tags on over 25,000 datasets from Kaggle.com, demonstrating the ability to learn semantically meaningful features.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-09T22:42:38Z</published>\n",
      "    <arxiv:comment>8 pages, 4 figures. Data set available in paper source</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>Proceedings of the 1st ACM International Conference on AI in Finance (ICAIF '20), October 15-16, 2020, New York, NY, USA</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Jiahao Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Manuela Veloso</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3383455.3422537</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3383455.3422537\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04838v1</id>\n",
      "    <title>Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator</title>\n",
      "    <updated>2020-10-09T22:54:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04838v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04838v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-09T22:54:38Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Max B. Paulus</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chris J. Maddison</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andreas Krause</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03821v1</id>\n",
      "    <title>Clustering Analysis of Interactive Learning Activities Based on Improved BIRCH Algorithm</title>\n",
      "    <updated>2020-10-08T07:46:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03821v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03821v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Group tendency is a research branch of computer assisted learning. The construction of good learning behavior is of great significance to learners' learning process and learning effect, and is the key basis of data-driven education decision-making. Clustering analysis is an effective method for the study of group tendency. Therefore, it is necessary to obtain the online learning behavior big data set of multi period and multi course, and describe the learning behavior as multi-dimensional learning interaction activities. First of all, on the basis of data initialization and standardization, we locate the classification conditions of data, realize the differentiation and integration of learning behavior, and form multiple subsets of data to be clustered; secondly, according to the topological relevance and dependence between learning interaction activities, we design an improved algorithm of BIRCH clustering based on random walking strategy, which realizes the retrieval evaluation and data of key learning interaction activities; Thirdly, through the calculation and comparison of several performance indexes, the improved algorithm has obvious advantages in learning interactive activity clustering, and the clustering process and results are feasible and reliable. The conclusion of this study can be used for reference and can be popularized. It has practical significance for the research of education big data and the practical application of learning analytics.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-08T07:46:46Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xiaona Xia</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03948v1</id>\n",
      "    <title>Artificial intelligence supported anemia control system (AISACS) to prevent anemia in maintenance hemodialysis patients</title>\n",
      "    <updated>2020-10-06T03:59:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03948v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03948v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Anemia, for which erythropoiesis-stimulating agents (ESAs) and iron supplements (ISs) are used as preventive measures, presents important difficulties for hemodialysis patients. Nevertheless, the number of physicians able to manage such medications appropriately is not keeping pace with the rapid increase of hemodialysis patients. Moreover, the high cost of ESAs imposes heavy burdens on medical insurance systems. An artificial-intelligence-supported anemia control system (AISACS) trained using administration direction data from experienced physicians has been developed by the authors. For the system, appropriate data selection and rectification techniques play important roles. Decision making related to ESAs poses a multi-class classification problem for which a two-step classification technique is introduced. Several validations have demonstrated that AISACS exhibits high performance with correct classification rates of 72-87% and clinically appropriate classification rates of 92-98%.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T03:59:37Z</published>\n",
      "    <arxiv:comment>14 pages and 7 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Toshiaki Ohara</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hiroshi Ikeda</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yoshiki Sugitani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hiroshi Suito</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Viet Quang Huy Huynh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Masaru Kinomura</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Soichiro Haraguchi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kazufumi Sakurama</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03954v1</id>\n",
      "    <title>A Survey on Deep Neural Network Compression: Challenges, Overview, and Solutions</title>\n",
      "    <updated>2020-10-05T13:12:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03954v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03954v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep Neural Network (DNN) has gained unprecedented performance due to its automated feature extraction capability. This high order performance leads to significant incorporation of DNN models in different Internet of Things (IoT) applications in the past decade. However, the colossal requirement of computation, energy, and storage of DNN models make their deployment prohibitive on resource constraint IoT devices. Therefore, several compression techniques were proposed in recent years for reducing the storage and computation requirements of the DNN model. These techniques on DNN compression have utilized a different perspective for compressing DNN with minimal accuracy compromise. It encourages us to make a comprehensive overview of the DNN compression techniques. In this paper, we present a comprehensive review of existing literature on compressing DNN model that reduces both storage and computation requirements. We divide the existing approaches into five broad categories, i.e., network pruning, sparse representation, bits precision, knowledge distillation, and miscellaneous, based upon the mechanism incorporated for compressing the DNN model. The paper also discussed the challenges associated with each category of DNN compression techniques. Finally, we provide a quick summary of existing work under each category with the future direction in DNN compression.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T13:12:46Z</published>\n",
      "    <arxiv:comment>19 pages, 9 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Rahul Mishra</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hari Prabhat Gupta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tanima Dutta</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03963v1</id>\n",
      "    <title>Neurodevelopmental Age Estimation of Infants Using a 3D-Convolutional Neural Network Model based on Fusion MRI Sequences</title>\n",
      "    <updated>2020-10-07T01:24:15Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03963v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03963v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The ability to determine if the brain is developing normally is a key component of pediatric neuroradiology and neurology. Brain magnetic resonance imaging (MRI) of infants demonstrates a specific pattern of development beyond simply myelination. While radiologists have used myelination patterns, brain morphology and size characteristics in determining if brain maturity matches the chronological age of the patient, this requires years of experience with pediatric neuroradiology. Due to the lack of standardized criteria, estimation of brain maturity before age three remains fraught with interobserver and intraobserver variability. An objective measure of brain developmental age estimation (BDAE) could be a useful tool in helping physicians identify developmental delay as well as other neurological diseases. We investigated a three-dimensional convolutional neural network (3D CNN) to rapidly classify brain developmental age using common MRI sequences. MRI datasets from normal newborns were obtained from the National Institute of Mental Health Data Archive from birth to 3 years. We developed a BDAE method using T1-weighted, as well as a fusion of T1-weighted, T2-weighted, and proton density (PD) sequences from 112 individual subjects using 3D CNN. We achieved a precision of 94.8% and a recall of 93.5% in utilizing multiple MRI sequences in determining BDAE.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-07T01:24:15Z</published>\n",
      "    <arxiv:comment>18 pages, 8 figures, 4 tables. Supplementary information: 7 figures, 3 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>M. Shabanian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>A. Siddiqui</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>H. Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>J. P. DeVincenzo</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03978v1</id>\n",
      "    <title>A Brief Review of Domain Adaptation</title>\n",
      "    <updated>2020-10-07T07:05:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03978v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03978v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Classical machine learning assumes that the training and test sets come from the same distributions. Therefore, a model learned from the labeled training data is expected to perform well on the test data. However, This assumption may not always hold in real-world applications where the training and the test data fall from different distributions, due to many factors, e.g., collecting the training and test sets from different sources, or having an out-dated training set due to the change of data over time. In this case, there would be a discrepancy across domain distributions, and naively applying the trained model on the new dataset may cause degradation in the performance. Domain adaptation is a sub-field within machine learning that aims to cope with these types of problems by aligning the disparity between domains such that the trained model can be generalized into the domain of interest. This paper focuses on unsupervised domain adaptation, where the labels are only available in the source domain. It addresses the categorization of domain adaptation from different viewpoints. Besides, It presents some successful shallow and deep domain adaptation approaches that aim to deal with domain adaptation problems.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-07T07:05:32Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Abolfazl Farahani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sahar Voghoei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Khaled Rasheed</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hamid R. Arabnia</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03691v2</id>\n",
      "    <title>Regularized Inverse Reinforcement Learning</title>\n",
      "    <updated>2020-12-03T01:34:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03691v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03691v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Inverse Reinforcement Learning (IRL) aims to facilitate a learner's ability to imitate expert behavior by acquiring reward functions that explain the expert's decisions. Regularized IRL applies strongly convex regularizers to the learner's policy in order to avoid the expert's behavior being rationalized by arbitrary constant rewards, also known as degenerate solutions. We propose tractable solutions, and practical methods to obtain them, for regularized IRL. Current methods are restricted to the maximum-entropy IRL framework, limiting them to Shannon-entropy regularizers, as well as proposing the solutions that are intractable in practice. We present theoretical backing for our proposed IRL method's applicability for both discrete and continuous controls, empirically validating our performance on a variety of tasks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-07T23:38:47Z</published>\n",
      "    <arxiv:comment>26 pages, 7 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Wonseok Jeon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chen-Yang Su</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paul Barde</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Thang Doan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Derek Nowrouzezahrai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joelle Pineau</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03694v2</id>\n",
      "    <title>Learning Intrinsic Symbolic Rewards in Reinforcement Learning</title>\n",
      "    <updated>2020-10-09T06:42:03Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03694v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03694v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Learning effective policies for sparse objectives is a key challenge in Deep Reinforcement Learning (RL). A common approach is to design task-related dense rewards to improve task learnability. While such rewards are easily interpreted, they rely on heuristics and domain expertise. Alternate approaches that train neural networks to discover dense surrogate rewards avoid heuristics, but are high-dimensional, black-box solutions offering little interpretability. In this paper, we present a method that discovers dense rewards in the form of low-dimensional symbolic trees - thus making them more tractable for analysis. The trees use simple functional operators to map an agent's observations to a scalar reward, which then supervises the policy gradient learning of a neural network policy. We test our method on continuous action spaces in Mujoco and discrete action spaces in Atari and Pygame environments. We show that the discovered dense rewards are an effective signal for an RL policy to solve the benchmark tasks. Notably, we significantly outperform a widely used, contemporary neural-network based reward-discovery algorithm in all environments considered.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-08T00:02:46Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Hassam Sheikh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shauharda Khadka</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Santiago Miret</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Somdeb Majumdar</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03701v1</id>\n",
      "    <title>Differentially Private Deep Learning with Direct Feedback Alignment</title>\n",
      "    <updated>2020-10-08T00:25:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03701v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03701v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Standard methods for differentially private training of deep neural networks replace back-propagated mini-batch gradients with biased and noisy approximations to the gradient. These modifications to training often result in a privacy-preserving model that is significantly less accurate than its non-private counterpart. We hypothesize that alternative training algorithms may be more amenable to differential privacy. Specifically, we examine the suitability of direct feedback alignment (DFA). We propose the first differentially private method for training deep neural networks with DFA and show that it achieves significant gains in accuracy (often by 10-20%) compared to backprop-based differentially private training on a variety of architectures (fully connected, convolutional) and datasets.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-08T00:25:22Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jaewoo Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniel Kifer</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03749v1</id>\n",
      "    <title>Tatum-Level Drum Transcription Based on a Convolutional Recurrent Neural Network with Language Model-Based Regularized Training</title>\n",
      "    <updated>2020-10-08T03:47:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03749v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03749v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper describes a neural drum transcription method that detects from music signals the onset times of drums at the $\\textit{tatum}$ level, where tatum times are assumed to be estimated in advance. In conventional studies on drum transcription, deep neural networks (DNNs) have often been used to take a music spectrogram as input and estimate the onset times of drums at the $\\textit{frame}$ level. The major problem with such frame-to-frame DNNs, however, is that the estimated onset times do not often conform with the typical tatum-level patterns appearing in symbolic drum scores because the long-term musically meaningful structures of those patterns are difficult to learn at the frame level. To solve this problem, we propose a regularized training method for a frame-to-tatum DNN. In the proposed method, a tatum-level probabilistic language model (gated recurrent unit (GRU) network or repetition-aware bi-gram model) is trained from an extensive collection of drum scores. Given that the musical naturalness of tatum-level onset times can be evaluated by the language model, the frame-to-tatum DNN is trained with a regularizer based on the pretrained language model. The experimental results demonstrate the effectiveness of the proposed regularized training method.</summary>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-08T03:47:25Z</published>\n",
      "    <arxiv:comment>Accepted to APSIPA 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.SD\"/>\n",
      "    <author>\n",
      "      <name>Ryoto Ishizuka</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ryo Nishikimi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eita Nakamura</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kazuyoshi Yoshii</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03753v1</id>\n",
      "    <title>Uncertainty in Neural Processes</title>\n",
      "    <updated>2020-10-08T04:10:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03753v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03753v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We explore the effects of architecture and training objective choice on amortized posterior predictive inference in probabilistic conditional generative models. We aim this work to be a counterpoint to a recent trend in the literature that stresses achieving good samples when the amount of conditioning data is large. We instead focus our attention on the case where the amount of conditioning data is small. We highlight specific architecture and objective choices that we find lead to qualitative and quantitative improvement to posterior inference in this low data regime. Specifically we explore the effects of choices of pooling operator and variational family on posterior quality in neural processes. Superior posterior predictive samples drawn from our novel neural process architectures are demonstrated via image completion/in-painting experiments.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-08T04:10:05Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Saeid Naderiparizi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kenny Chiu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Benjamin Bloem-Reddy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Frank Wood</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03029v1</id>\n",
      "    <title>Using Bayesian deep learning approaches for uncertainty-aware building energy surrogate models</title>\n",
      "    <updated>2020-10-05T15:04:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03029v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03029v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Fast machine learning-based surrogate models are trained to emulate slow, high-fidelity engineering simulation models to accelerate engineering design tasks. This introduces uncertainty as the surrogate is only an approximation of the original model.\n",
      "  Bayesian methods can quantify that uncertainty, and deep learning models exist that follow the Bayesian paradigm. These models, namely Bayesian neural networks and Gaussian process models, enable us to give predictions together with an estimate of the model's uncertainty. As a result we can derive uncertainty-aware surrogate models that can automatically suspect unseen design samples that cause large emulation errors. For these samples, the high-fidelity model can be queried instead. This outlines how the Bayesian paradigm allows us to hybridize fast, but approximate, and slow, but accurate models.\n",
      "  In this paper, we train two types of Bayesian models, dropout neural networks and stochastic variational Gaussian Process models, to emulate a complex high dimensional building energy performance simulation problem. The surrogate model processes 35 building design parameters (inputs) to estimate 12 different performance metrics (outputs). We benchmark both approaches, prove their accuracy to be competitive, and show that errors can be reduced by up to 30% when the 10% of samples with the highest uncertainty are transferred to the high-fidelity model.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T15:04:18Z</published>\n",
      "    <arxiv:comment>Submitted to Energy and AI</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Paul Westermann</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ralph Evins</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03053v1</id>\n",
      "    <title>Sequential Changepoint Detection in Neural Networks with Checkpoints</title>\n",
      "    <updated>2020-10-06T21:49:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03053v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03053v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We introduce a framework for online changepoint detection and simultaneous model learning which is applicable to highly parametrized models, such as deep neural networks. It is based on detecting changepoints across time by sequentially performing generalized likelihood ratio tests that require only evaluations of simple prediction score functions. This procedure makes use of checkpoints, consisting of early versions of the actual model parameters, that allow to detect distributional changes by performing predictions on future data. We define an algorithm that bounds the Type I error in the sequential testing procedure. We demonstrate the efficiency of our method in challenging continual learning applications with unknown task changepoints, and show improved performance compared to online Bayesian changepoint detection.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T21:49:54Z</published>\n",
      "    <arxiv:comment>17 pages, 7 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Michalis K. Titsias</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jakub Sygnowski</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yutian Chen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03095v1</id>\n",
      "    <title>Gradient-based Causal Structure Learning with Normalizing Flow</title>\n",
      "    <updated>2020-10-07T00:55:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03095v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03095v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we propose a score-based normalizing flow method called DAG-NF to learn dependencies of input observation data. Inspired by Grad-CAM in computer vision, we use jacobian matrix of output on input as causal relationships and this method can be generalized to any neural networks especially for flow-based generative neural networks such as Masked Autoregressive Flow(MAF) and Continuous Normalizing Flow(CNF) which compute the log likelihood loss and divergence of distribution of input data and target distribution. This method extends NOTEARS which enforces a important acylicity constraint on continuous adjacency matrix of graph nodes and significantly reduce the computational complexity of search space of graph.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-07T00:55:47Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xiongren Chen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03124v1</id>\n",
      "    <title>VCDM: Leveraging Variational Bi-encoding and Deep Contextualized Word Representations for Improved Definition Modeling</title>\n",
      "    <updated>2020-10-07T02:48:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03124v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03124v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases. Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rather than direct way. To tackle this issue we propose a generative model for the task, introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition. We rely on variational inference for estimation and leverage contextualized word embeddings for improved performance. Our approach is evaluated on four existing challenging benchmarks with the addition of two new datasets, \"Cambridge\" and the first non-English corpus \"Robert\", which we release to complement our empirical study. Our Variational Contextual Definition Modeler (VCDM) achieves state-of-the-art performance in terms of automatic and human evaluation metrics, demonstrating the effectiveness of our approach.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-07T02:48:44Z</published>\n",
      "    <arxiv:comment>EMNLP 2020, 10 Pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Machel Reid</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Edison Marrese-Taylor</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yutaka Matsuo</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03130v1</id>\n",
      "    <title>Computational analysis of pathological image enables interpretable prediction for microsatellite instability</title>\n",
      "    <updated>2020-10-07T03:05:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03130v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03130v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Microsatellite instability (MSI) is associated with several tumor types and its status has become increasingly vital in guiding patient treatment decisions. However, in clinical practice, distinguishing MSI from its counterpart is challenging since the diagnosis of MSI requires additional genetic or immunohistochemical tests. In this study, interpretable pathological image analysis strategies are established to help medical experts to automatically identify MSI. The strategies only require ubiquitous Haematoxylin and eosin-stained whole-slide images and can achieve decent performance in the three cohorts collected from The Cancer Genome Atlas. The strategies provide interpretability in two aspects. On the one hand, the image-level interpretability is achieved by generating localization heat maps of important regions based on the deep learning network; on the other hand, the feature-level interpretability is attained through feature importance and pathological feature interaction analysis. More interestingly, both from the image-level and feature-level interpretability, color features and texture characteristics are shown to contribute the most to the MSI predictions. Therefore, the classification models under the proposed strategies can not only serve as an efficient tool for predicting the MSI status of patients, but also provide more insights to pathologists with clinical understanding.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-07T03:05:05Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Jin Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wangwei Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuting Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shiyun Lin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yukang Jiang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruixian Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xueqin Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03132v2</id>\n",
      "    <title>Conditional Generative Modeling via Learning the Latent Space</title>\n",
      "    <updated>2020-10-09T03:29:17Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03132v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03132v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Although deep learning has achieved appealing results on several machine learning tasks, most of the models are deterministic at inference, limiting their application to single-modal settings. We propose a novel general-purpose framework for conditional generation in multimodal spaces, that uses latent variables to model generalizable learning patterns while minimizing a family of regression cost functions. At inference, the latent variables are optimized to find optimal solutions corresponding to multiple output modes. Compared to existing generative solutions, in multimodal spaces, our approach demonstrates faster and stable convergence, and can learn better representations for downstream tasks. Importantly, it provides a simple generic model that can beat highly engineered pipelines tailored using domain expertise on a variety of tasks, while generating diverse outputs. Our codes will be released.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-07T03:11:34Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Sameera Ramasinghe</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kanchana Ranasinghe</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Salman Khan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nick Barnes</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stephen Gould</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.03544v1</id>\n",
      "    <title>A Self-supervised Approach for Semantic Indexing in the Context of COVID-19 Pandemic</title>\n",
      "    <updated>2020-10-07T17:43:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.03544v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.03544v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The pandemic has accelerated the pace at which COVID-19 scientific papers are published. In addition, the process of manually assigning semantic indexes to these papers by experts is even more time-consuming and overwhelming in the current health crisis. Therefore, there is an urgent need for automatic semantic indexing models which can effectively scale-up to newly introduced concepts and rapidly evolving distributions of the hyperfocused related literature. In this research, we present a novel semantic indexing approach based on the state-of-the-art self-supervised representation learning and transformer encoding exclusively suitable for pandemic crises. We present a case study on a novel dataset that is based on COVID-19 papers published and manually indexed in PubMed. Our study shows that our self-supervised model outperforms the best performing models of BioASQ Task 8a by micro-F1 score of 0.1 and LCA-F score of 0.08 on average. Our model also shows superior performance on detecting the supplementary concepts which is quite important when the focus of the literature has drastically shifted towards specific concepts related to the pandemic. Our study sheds light on the main challenges confronting semantic indexing models during a pandemic, namely new domains and drastic changes of their distributions, and as a superior alternative for such situations, propose a model founded on approaches which have shown auspicious performance in improving generalization and data efficiency in various NLP tasks. We also show the joint indexing of major Medical Subject Headings (MeSH) and supplementary concepts improves the overall performance.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-07T17:43:55Z</published>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Nima Ebadi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peyman Najafirad</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01844v4</id>\n",
      "    <title>A Comprehensive Study of Class Incremental Learning Algorithms for Visual Tasks</title>\n",
      "    <updated>2020-12-15T16:40:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01844v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01844v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The ability of artificial agents to increment their capabilities when confronted with new data is an open challenge in artificial intelligence. The main challenge faced in such cases is catastrophic forgetting, i.e., the tendency of neural networks to underfit past data when new ones are ingested. A first group of approaches tackles forgetting by increasing deep model capacity to accommodate new knowledge. A second type of approaches fix the deep model size and introduce a mechanism whose objective is to ensure a good compromise between stability and plasticity of the model. While the first type of algorithms were compared thoroughly, this is not the case for methods which exploit a fixed size model. Here, we focus on the latter, place them in a common conceptual and experimental framework and propose the following contributions: (1) define six desirable properties of incremental learning algorithms and analyze them according to these properties, (2) introduce a unified formalization of the class-incremental learning problem, (3) propose a common evaluation framework which is more thorough than existing ones in terms of number of datasets, size of datasets, size of bounded memory and number of incremental states, (4) investigate the usefulness of herding for past exemplars selection, (5) provide experimental evidence that it is possible to obtain competitive performance without the use of knowledge distillation to tackle catastrophic forgetting and (6) facilitate reproducibility by integrating all tested methods in a common open-source repository. The main experimental finding is that none of the existing algorithms achieves the best results in all evaluated settings. Important differences arise notably if a bounded memory of past classes is allowed or not.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-03T16:59:21Z</published>\n",
      "    <arxiv:comment>Accepted for publication in the Elsevier's Neural Networks journal</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Eden Belouadah</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Adrian Popescu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ioannis Kanellos</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01861v1</id>\n",
      "    <title>DeL-haTE: A Deep Learning Tunable Ensemble for Hate Speech Detection</title>\n",
      "    <updated>2020-11-03T17:32:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01861v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01861v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Online hate speech on social media has become a fast-growing problem in recent times. Nefarious groups have developed large content delivery networks across several main-stream (Twitter and Facebook) and fringe (Gab, 4chan, 8chan, etc.) outlets to deliver cascades of hate messages directed both at individuals and communities. Thus addressing these issues has become a top priority for large-scale social media outlets. Three key challenges in automated detection and classification of hateful content are the lack of clearly labeled data, evolving vocabulary and lexicon - hashtags, emojis, etc. - and the lack of baseline models for fringe outlets such as Gab. In this work, we propose a novel framework with three major contributions. (a) We engineer an ensemble of deep learning models that combines the strengths of state-of-the-art approaches, (b) we incorporate a tuning factor into this framework that leverages transfer learning to conduct automated hate speech classification on unlabeled datasets, like Gab, and (c) we develop a weak supervised learning methodology that allows our framework to train on unlabeled data. Our ensemble models achieve an 83% hate recall on the HON dataset, surpassing the performance of the state-of-the-art deep models. We demonstrate that weak supervised training in combination with classifier tuning significantly increases model performance on unlabeled data from Gab, achieving a hate recall of 67%.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-03T17:32:50Z</published>\n",
      "    <arxiv:comment>Accepted at ICMLA20 Special Session: Machine Learning for Natural Language Processing</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Joshua Melton</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arunkumar Bagavathi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Siddharth Krishnan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00635v1</id>\n",
      "    <title>Screening for an Infectious Disease as a Problem in Stochastic Control</title>\n",
      "    <updated>2020-11-01T22:03:26Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00635v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00635v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>There has been much recent interest in screening populations for an infectious disease. Here, we present a stochastic-control model, wherein the optimum screening policy is provably difficult to find, but wherein Thompson sampling has provably optimal performance guarantees in the form of Bayesian regret. Thompson sampling seems applicable especially to diseases, for which we do not understand the dynamics well, such as to the super-spreading COVID-19.</summary>\n",
      "    <category term=\"physics.soc-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-01T22:03:26Z</published>\n",
      "    <arxiv:primary_category term=\"physics.soc-ph\"/>\n",
      "    <author>\n",
      "      <name>Jakub Marecek</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00697v1</id>\n",
      "    <title>Time Series Forecasting with Stacked Long Short-Term Memory Networks</title>\n",
      "    <updated>2020-11-02T03:09:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00697v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00697v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Long Short-Term Memory (LSTM) networks are often used to capture temporal dependency patterns. By stacking multi-layer LSTM networks, it can capture even more complex patterns. This paper explores the effectiveness of applying stacked LSTM networks in the time series prediction domain, specifically, the traffic volume forecasting. Being able to predict traffic volume more accurately can result in better planning, thus greatly reduce the operation cost and improve overall efficiency.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-02T03:09:23Z</published>\n",
      "    <arxiv:comment>7 pages, 8 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Frank Xiao</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00717v1</id>\n",
      "    <title>Noise-Contrastive Estimation for Multivariate Point Processes</title>\n",
      "    <updated>2020-11-02T04:09:33Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00717v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00717v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The log-likelihood of a generative model often involves both positive and negative terms. For a temporal multivariate point process, the negative term sums over all the possible event types at each time and also integrates over all the possible times. As a result, maximum likelihood estimation is expensive. We show how to instead apply a version of noise-contrastive estimation---a general parameter estimation method with a less expensive stochastic objective. Our specific instantiation of this general idea works out in an interestingly non-trivial way and has provable guarantees for its optimality, consistency and efficiency. On several synthetic and real-world datasets, our method shows benefits: for the model to achieve the same level of log-likelihood on held-out data, our method needs considerably fewer function evaluations and less wall-clock time.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-02T04:09:33Z</published>\n",
      "    <arxiv:comment>NeurIPS 2020 camera-ready</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Hongyuan Mei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tom Wan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jason Eisner</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00718v1</id>\n",
      "    <title>Fundamental Limits of Obfuscation for Linear Gaussian Dynamical Systems: An Information-Theoretic Approach</title>\n",
      "    <updated>2020-10-29T20:05:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00718v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00718v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we study the fundamental limits of obfuscation in terms of privacy-distortion tradeoffs for linear Gaussian dynamical systems via an information-theoretic approach. Particularly, we obtain analytical formulas that capture the fundamental privacy-distortion tradeoffs when privacy masks are to be added to the outputs of the dynamical systems, while indicating explicitly how to design the privacy masks in an optimal way: The privacy masks should be colored Gaussian with power spectra shaped specifically based upon the system and noise properties.</summary>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-29T20:05:50Z</published>\n",
      "    <arxiv:comment>arXiv admin note: text overlap with arXiv:2008.04893</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IT\"/>\n",
      "    <author>\n",
      "      <name>Song Fang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Quanyan Zhu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00771v1</id>\n",
      "    <title>Multitask Learning and Joint Optimization for Transformer-RNN-Transducer Speech Recognition</title>\n",
      "    <updated>2020-11-02T06:38:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00771v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00771v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recently, several types of end-to-end speech recognition methods named transformer-transducer were introduced. According to those kinds of methods, transcription networks are generally modeled by transformer-based neural networks, while prediction networks could be modeled by either transformers or recurrent neural networks (RNN). This paper explores multitask learning, joint optimization, and joint decoding methods for transformer-RNN-transducer systems. Our proposed methods have the main advantage in that the model can maintain information on the large text corpus. We prove their effectiveness by performing experiments utilizing the well-known ESPNET toolkit for the widely used Librispeech datasets. We also show that the proposed methods can reduce word error rate (WER) by 16.6 % and 13.3 % for test-clean and test-other datasets, respectively, without changing the overall model structure nor exploiting an external LM.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-02T06:38:06Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jae-Jin Jeon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eesung Kim</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00802v1</id>\n",
      "    <title>Hierarchical Bi-Directional Self-Attention Networks for Paper Review Rating Recommendation</title>\n",
      "    <updated>2020-11-02T08:07:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00802v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00802v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Review rating prediction of text reviews is a rapidly growing technology with a wide range of applications in natural language processing. However, most existing methods either use hand-crafted features or learn features using deep learning with simple text corpus as input for review rating prediction, ignoring the hierarchies among data. In this paper, we propose a Hierarchical bi-directional self-attention Network framework (HabNet) for paper review rating prediction and recommendation, which can serve as an effective decision-making tool for the academic paper review process. Specifically, we leverage the hierarchical structure of the paper reviews with three levels of encoders: sentence encoder (level one), intra-review encoder (level two) and inter-review encoder (level three). Each encoder first derives contextual representation of each level, then generates a higher-level representation, and after the learning process, we are able to identify useful predictors to make the final acceptance decision, as well as to help discover the inconsistency between numerical review ratings and text sentiment conveyed by reviewers. Furthermore, we introduce two new metrics to evaluate models in data imbalance situations. Extensive experiments on a publicly available dataset (PeerRead) and our own collected dataset (OpenReview) demonstrate the superiority of the proposed approach compared with state-of-the-art methods.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-02T08:07:50Z</published>\n",
      "    <arxiv:comment>Accepted by COLING 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Zhongfen Deng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hao Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Congying Xia</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jianxin Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lifang He</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Philip S. Yu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00825v1</id>\n",
      "    <title>Reinforcement Learning with Efficient Active Feature Acquisition</title>\n",
      "    <updated>2020-11-02T08:46:27Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00825v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00825v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Solving real-life sequential decision making problems under partial observability involves an exploration-exploitation problem. To be successful, an agent needs to efficiently gather valuable information about the state of the world for making rewarding decisions. However, in real-life, acquiring valuable information is often highly costly, e.g., in the medical domain, information acquisition might correspond to performing a medical test on a patient. This poses a significant challenge for the agent to perform optimally for the task while reducing the cost for information acquisition. In this paper, we propose a model-based reinforcement learning framework that learns an active feature acquisition policy to solve the exploration-exploitation problem during its execution. Key to the success is a novel sequential variational auto-encoder that learns high-quality representations from partially observed states, which are then used by the policy to maximize the task reward in a cost efficient manner. We demonstrate the efficacy of our proposed framework in a control domain as well as using a medical simulator. In both tasks, our proposed method outperforms conventional baselines and results in policies with greater cost efficiency.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-02T08:46:27Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Haiyan Yin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yingzhen Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sinno Jialin Pan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cheng Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sebastian Tschiatschek</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00866v1</id>\n",
      "    <title>An End-to-End ML System for Personalized Conversational Voice Models in Walmart E-Commerce</title>\n",
      "    <updated>2020-11-02T10:14:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00866v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00866v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Searching for and making decisions about products is becoming increasingly easier in the e-commerce space, thanks to the evolution of recommender systems. Personalization and recommender systems have gone hand-in-hand to help customers fulfill their shopping needs and improve their experiences in the process. With the growing adoption of conversational platforms for shopping, it has become important to build personalized models at scale to handle the large influx of data and perform inference in real-time. In this work, we present an end-to-end machine learning system for personalized conversational voice commerce. We include components for implicit feedback to the model, model training, evaluation on update, and a real-time inference engine. Our system personalizes voice shopping for Walmart Grocery customers and is currently available via Google Assistant, Siri and Google Home devices.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.PF\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-02T10:14:55Z</published>\n",
      "    <arxiv:comment>4 pages, 1 figure</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Rahul Radhakrishnan Iyer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Praveenkumar Kanumala</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stephen Guo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kannan Achan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00425v1</id>\n",
      "    <title>Analyzing the Effect of Multi-task Learning for Biomedical Named Entity Recognition</title>\n",
      "    <updated>2020-11-01T04:52:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00425v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00425v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Developing high-performing systems for detecting biomedical named entities has major implications. State-of-the-art deep-learning based solutions for entity recognition often require large annotated datasets, which is not available in the biomedical domain. Transfer learning and multi-task learning have been shown to improve performance for low-resource domains. However, the applications of these methods are relatively scarce in the biomedical domain, and a theoretical understanding of why these methods improve the performance is lacking. In this study, we performed an extensive analysis to understand the transferability between different biomedical entity datasets. We found useful measures to predict transferability between these datasets. Besides, we propose combining transfer learning and multi-task learning to improve the performance of biomedical named entity recognition systems, which is not applied before to the best of our knowledge.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-01T04:52:56Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Arda Akdemir</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tetsuo Shibuya</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00540v1</id>\n",
      "    <title>Unsupervised Intrusion Detection System for Unmanned Aerial Vehicle with Less Labeling Effort</title>\n",
      "    <updated>2020-11-01T15:52:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00540v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00540v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Along with the importance of safety, an IDS has become a significant task in the real world. Prior studies proposed various intrusion detection models for the UAV. Past rule-based approaches provided a concrete baseline IDS model, and the machine learning-based method achieved a precise intrusion detection performance on the UAV with supervised learning models. However, previous methods have room for improvement to be implemented in the real world. Prior methods required a large labeling effort on the dataset, and the model could not identify attacks that were not trained before. To jump over these hurdles, we propose an IDS with unsupervised learning. As unsupervised learning does not require labeling, our model let the practitioner not to label every type of attack from the flight data. Moreover, the model can identify an abnormal status of the UAV regardless of the type of attack. We trained an autoencoder with the benign flight data only and checked the model provides a different reconstruction loss at the benign flight and the flight under attack. We discovered that the model produces much higher reconstruction loss with the flight under attack than the benign flight; thus, this reconstruction loss can be utilized to recognize an intrusion to the UAV. With consideration of the computation overhead and the detection performance in the wild, we expect our model can be a concrete and practical baseline IDS on the UAV.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-01T15:52:22Z</published>\n",
      "    <arxiv:comment>14 pages, 4 tables, 3 figures, 5 equations, In Proceeding of WISA 2020 (THE 21ST WORLD CONFERENCE ON INFORMATION SECURITY APPLICATIONS)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Kyung Ho Park</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eunji Park</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Huy Kang Kim</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00618v2</id>\n",
      "    <title>Triage of Potential COVID-19 Patients from Chest X-ray Images using Hierarchical Convolutional Networks</title>\n",
      "    <updated>2020-12-15T15:47:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00618v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00618v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The current COVID-19 pandemic has motivated the researchers to use artificial intelligence techniques for a potential alternative to reverse transcription-polymerase chain reaction (RT-PCR) due to the limited scale of testing. The chest X-ray (CXR) is one of the alternatives to achieve fast diagnosis but the unavailability of large-scale annotated data makes the clinical implementation of machine learning-based COVID detection difficult. Another issue is the usage of ImageNet pre-trained networks which does not extract reliable feature representations from medical images. In this paper, we propose the use of hierarchical convolutional network (HCN) architecture to naturally augment the data along with diversified features. The HCN uses the first convolution layer from COVIDNet followed by the convolutional layers from well-known pre-trained networks to extract the features. The use of the convolution layer from COVIDNet ensures the extraction of representations relevant to the CXR modality. We also propose the use of ECOC for encoding multiclass problems to binary classification for improving the recognition performance. Experimental results show that HCN architecture is capable of achieving better results in comparison to the existing studies. The proposed method can accurately triage potential COVID-19 patients through CXR images for sharing the testing load and increasing the testing capacity.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-01T20:01:22Z</published>\n",
      "    <arxiv:comment>23 pages, 9 figures, 4 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Kapal Dev</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sunder Ali Khowaja</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ankur Singh Bist</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vaibhav Saini</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Surbhi Bhatia</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01334v1</id>\n",
      "    <title>Impact of Community Structure on Consensus Machine Learning</title>\n",
      "    <updated>2020-11-02T21:41:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01334v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01334v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Consensus dynamics support decentralized machine learning for data that is distributed across a cloud compute cluster or across the internet of things. In these and other settings, one seeks to minimize the time $τ_ε$ required to obtain consensus within some $ε&gt;0$ margin of error. $τ_ε$ typically depends on the topology of the underlying communication network, and for many algorithms $τ_ε$ depends on the second-smallest eigenvalue $λ_2\\in[0,1]$ of the network's normalized Laplacian matrix: $τ_ε\\sim\\mathcal{O}(λ_2^{-1})$. Here, we analyze the effect on $τ_ε$ of network community structure, which can arise when compute nodes/sensors are spatially clustered, for example. We study consensus machine learning over networks drawn from stochastic block models, which yield random networks that can contain heterogeneous communities with different sizes and densities. Using random matrix theory, we analyze the effects of communities on $λ_2$ and consensus, finding that $λ_2$ generally increases (i.e., $τ_ε$ decreases) as one decreases the extent of community structure. We further observe that there exists a critical level of community structure at which $τ_ε$ reaches a lower bound and is no longer limited by the presence of communities. We support our findings with empirical experiments for decentralized support vector machines.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.PR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-02T21:41:35Z</published>\n",
      "    <arxiv:comment>9 pages; 5 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Bao Huynh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Haimonti Dutta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dane Taylor</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01374v2</id>\n",
      "    <title>Synthetic Data Generation for Economists</title>\n",
      "    <updated>2020-11-06T21:03:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01374v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01374v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>As more tech companies engage in rigorous economic analyses, we are confronted with a data problem: in-house papers cannot be replicated due to use of sensitive, proprietary, or private data. Readers are left to assume that the obscured true data (e.g., internal Google information) indeed produced the results given, or they must seek out comparable public-facing data (e.g., Google Trends) that yield similar results. One way to ameliorate this reproducibility issue is to have researchers release synthetic datasets based on their true data; this allows external parties to replicate an internal researcher's methodology. In this brief overview, we explore synthetic data generation at a high level for economic analyses.</summary>\n",
      "    <category term=\"econ.GN\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-02T23:05:55Z</published>\n",
      "    <arxiv:primary_category term=\"econ.GN\"/>\n",
      "    <author>\n",
      "      <name>Allison Koenecke</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hal Varian</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01381v1</id>\n",
      "    <title>Optimal Policies for the Homogeneous Selective Labels Problem</title>\n",
      "    <updated>2020-11-02T23:32:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01381v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01381v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Selective labels are a common feature of consequential decision-making applications, referring to the lack of observed outcomes under one of the possible decisions. This paper reports work in progress on learning decision policies in the face of selective labels. The setting considered is both a simplified homogeneous one, disregarding individuals' features to facilitate determination of optimal policies, and an online one, to balance costs incurred in learning with future utility. For maximizing discounted total reward, the optimal policy is shown to be a threshold policy, and the problem is one of optimal stopping. In contrast, for undiscounted infinite-horizon average reward, optimal policies have positive acceptance probability in all states. Future work stemming from these results is discussed.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-02T23:32:53Z</published>\n",
      "    <arxiv:comment>12 pages, 1 figure. To be presented at the Workshop on Consequential Decision Making in Dynamic Environments at the 34th Conference on Neural Information Processing Systems (NeurIPS 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Dennis Wei</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01418v1</id>\n",
      "    <title>Meta-learning Transferable Representations with a Single Target Domain</title>\n",
      "    <updated>2020-11-03T01:57:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01418v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01418v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent works found that fine-tuning and joint training---two popular approaches for transfer learning---do not always improve accuracy on downstream tasks. First, we aim to understand more about when and why fine-tuning and joint training can be suboptimal or even harmful for transfer learning. We design semi-synthetic datasets where the source task can be solved by either source-specific features or transferable features. We observe that (1) pre-training may not have incentive to learn transferable features and (2) joint training may simultaneously learn source-specific features and overfit to the target. Second, to improve over fine-tuning and joint training, we propose Meta Representation Learning (MeRLin) to learn transferable features. MeRLin meta-learns representations by ensuring that a head fit on top of the representations with target training data also performs well on target validation data. We also prove that MeRLin recovers the target ground-truth model with a quadratic neural net parameterization and a source distribution that contains both transferable and source-specific features. On the same distribution, pre-training and joint training provably fail to learn transferable features. MeRLin empirically outperforms previous state-of-the-art transfer learning algorithms on various real-world vision and NLP transfer learning benchmarks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-03T01:57:37Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Hong Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jeff Z. HaoChen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Colin Wei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tengyu Ma</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01456v1</id>\n",
      "    <title>Frequency-compensated PINNs for Fluid-dynamic Design Problems</title>\n",
      "    <updated>2020-11-03T03:56:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01456v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01456v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Incompressible fluid flow around a cylinder is one of the classical problems in fluid-dynamics with strong relevance with many real-world engineering problems, for example, design of offshore structures or design of a pin-fin heat exchanger. Thus learning a high-accuracy surrogate for this problem can demonstrate the efficacy of a novel machine learning approach. In this work, we propose a physics-informed neural network (PINN) architecture for learning the relationship between simulation output and the underlying geometry and boundary conditions. In addition to using a physics-based regularization term, the proposed approach also exploits the underlying physics to learn a set of Fourier features, i.e. frequency and phase offset parameters, and then use them for predicting flow velocity and pressure over the spatio-temporal domain. We demonstrate this approach by predicting simulation results over out of range time interval and for novel design conditions. Our results show that incorporation of Fourier features improves the generalization performance over both temporal domain and design space.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-03T03:56:41Z</published>\n",
      "    <arxiv:comment>Machine Learning for Engineering Modeling, Simulation, and Design (ML4Eng) Workshop, NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Tongtao Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Biswadip Dey</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pratik Kakkar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arindam Dasgupta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amit Chakraborty</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01191v3</id>\n",
      "    <title>Reducing Neural Network Parameter Initialization Into an SMT Problem</title>\n",
      "    <updated>2020-11-09T06:28:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01191v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01191v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Training a neural network (NN) depends on multiple factors, including but not limited to the initial weights. In this paper, we focus on initializing deep NN parameters such that it performs better, comparing to random or zero initialization. We do this by reducing the process of initialization into an SMT solver. Previous works consider certain activation functions on small NNs, however the studied NN is a deep network with different activation functions. Our experiments show that the proposed approach for parameter initialization achieves better performance comparing to randomly initialized networks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-02T18:31:29Z</published>\n",
      "    <arxiv:comment>AAAI-21 SA Program</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Mohamad H. Danesh</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01297v1</id>\n",
      "    <title>Useful Policy Invariant Shaping from Arbitrary Advice</title>\n",
      "    <updated>2020-11-02T20:29:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01297v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01297v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Reinforcement learning is a powerful learning paradigm in which agents can learn to maximize sparse and delayed reward signals. Although RL has had many impressive successes in complex domains, learning can take hours, days, or even years of training data. A major challenge of contemporary RL research is to discover how to learn with less data. Previous work has shown that domain information can be successfully used to shape the reward; by adding additional reward information, the agent can learn with much less data. Furthermore, if the reward is constructed from a potential function, the optimal policy is guaranteed to be unaltered. While such potential-based reward shaping (PBRS) holds promise, it is limited by the need for a well-defined potential function. Ideally, we would like to be able to take arbitrary advice from a human or other agent and improve performance without affecting the optimal policy. The recently introduced dynamic potential based advice (DPBA) method tackles this challenge by admitting arbitrary advice from a human or other agent and improves performance without affecting the optimal policy. The main contribution of this paper is to expose, theoretically and empirically, a flaw in DPBA. Alternatively, to achieve the ideal goals, we present a simple method called policy invariant explicit shaping (PIES) and show theoretically and empirically that PIES succeeds where DPBA fails.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-02T20:29:09Z</published>\n",
      "    <arxiv:comment>9 pages, 6 figures, Adaptive and Learning Agents (ALA) 2020 Workshop</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Paniz Behboudian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yash Satsangi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matthew E. Taylor</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anna Harutyunyan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michael Bowling</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00966v1</id>\n",
      "    <title>Diverse Image Captioning with Context-Object Split Latent Spaces</title>\n",
      "    <updated>2020-11-02T13:33:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00966v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00966v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Diverse image captioning models aim to learn one-to-many mappings that are innate to cross-domain datasets, such as of images and texts. Current methods for this task are based on generative latent variable models, e.g. VAEs with structured latent spaces. Yet, the amount of multimodality captured by prior work is limited to that of the paired training data -- the true diversity of the underlying generative process is not fully captured. To address this limitation, we leverage the contextual descriptions in the dataset that explain similar contexts in different visual scenes. To this end, we introduce a novel factorization of the latent space, termed context-object split, to model diversity in contextual descriptions across images and texts within the dataset. Our framework not only enables diverse captioning through context-based pseudo supervision, but extends this to images with novel objects and without paired captions in the training data. We evaluate our COS-CVAE approach on the standard COCO dataset and on the held-out COCO dataset consisting of images with novel objects, showing significant gains in accuracy and diversity.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-02T13:33:20Z</published>\n",
      "    <arxiv:comment>To appear at NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Shweta Mahajan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stefan Roth</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01089v1</id>\n",
      "    <title>Instance based Generalization in Reinforcement Learning</title>\n",
      "    <updated>2020-11-02T16:19:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01089v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01089v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Agents trained via deep reinforcement learning (RL) routinely fail to generalize to unseen environments, even when these share the same underlying dynamics as the training levels. Understanding the generalization properties of RL is one of the challenges of modern machine learning. Towards this goal, we analyze policy learning in the context of Partially Observable Markov Decision Processes (POMDPs) and formalize the dynamics of training levels as instances. We prove that, independently of the exploration strategy, reusing instances introduces significant changes on the effective Markov dynamics the agent observes during training. Maximizing expected rewards impacts the learned belief state of the agent by inducing undesired instance specific speedrunning policies instead of generalizeable ones, which are suboptimal on the training set. We provide generalization bounds to the value gap in train and test environments based on the number of training instances, and use insights based on these to improve performance on unseen levels. We propose training a shared belief representation over an ensemble of specialized policies, from which we compute a consensus policy that is used for data collection, disallowing instance specific exploitation. We experimentally validate our theory, observations, and the proposed computational solution over the CoinRun benchmark.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-02T16:19:44Z</published>\n",
      "    <arxiv:comment>Accepted on NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Martin Bertran</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Natalia Martinez</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mariano Phielipp</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Guillermo Sapiro</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00178v1</id>\n",
      "    <title>Learning Open Set Network with Discriminative Reciprocal Points</title>\n",
      "    <updated>2020-10-31T03:20:31Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00178v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00178v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Open set recognition is an emerging research area that aims to simultaneously classify samples from predefined classes and identify the rest as 'unknown'. In this process, one of the key challenges is to reduce the risk of generalizing the inherent characteristics of numerous unknown samples learned from a small amount of known data. In this paper, we propose a new concept, Reciprocal Point, which is the potential representation of the extra-class space corresponding to each known category. The sample can be classified to known or unknown by the otherness with reciprocal points. To tackle the open set problem, we offer a novel open space risk regularization term. Based on the bounded space constructed by reciprocal points, the risk of unknown is reduced through multi-category interaction. The novel learning framework called Reciprocal Point Learning (RPL), which can indirectly introduce the unknown information into the learner with only known classes, so as to learn more compact and discriminative representations. Moreover, we further construct a new large-scale challenging aircraft dataset for open set recognition: Aircraft 300 (Air-300). Extensive experiments on multiple benchmark datasets indicate that our framework is significantly superior to other existing approaches and achieves state-of-the-art performance on standard open set benchmarks.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-31T03:20:31Z</published>\n",
      "    <arxiv:comment>ECCV 2020 (spotlight)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <arxiv:journal_ref>ECCV 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Guangyao Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Limeng Qiao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yemin Shi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peixi Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jia Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tiejun Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shiliang Pu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yonghong Tian</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1007/978-3-030-58580-8_30</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1007/978-3-030-58580-8_30\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00368v2</id>\n",
      "    <title>DL-Reg: A Deep Learning Regularization Technique using Linear Regression</title>\n",
      "    <updated>2020-11-03T23:22:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00368v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00368v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Regularization plays a vital role in the context of deep learning by preventing deep neural networks from the danger of overfitting. This paper proposes a novel deep learning regularization method named as DL-Reg, which carefully reduces the nonlinearity of deep networks to a certain extent by explicitly enforcing the network to behave as much linear as possible. The key idea is to add a linear constraint to the objective function of the deep neural networks, which is simply the error of a linear mapping from the inputs to the outputs of the model. More precisely, the proposed DL-Reg carefully forces the network to behave in a linear manner. This linear constraint, which is further adjusted by a regularization factor, prevents the network from the risk of overfitting. The performance of DL-Reg is evaluated by training state-of-the-art deep network models on several benchmark datasets. The experimental results show that the proposed regularization method: 1) gives major improvements over the existing regularization techniques, and 2) significantly improves the performance of deep neural networks, especially in the case of small-sized training datasets.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-31T21:53:24Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Maryam Dialameh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ali Hamzeh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hossein Rahmani</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01647v2</id>\n",
      "    <title>Uncertainty Quantification of Darcy Flow through Porous Media using Deep Gaussian Process</title>\n",
      "    <updated>2020-11-05T11:31:03Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01647v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01647v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A computational method based on the non-linear Gaussian process (GP), known as deep Gaussian processes (deep GPs) for uncertainty quantification &amp; propagation in modelling of flow through heterogeneous porous media is presented. The method is also used for reducing dimensionality of model output and consequently emulating highly complex relationship between hydrogeological properties and reduced order fluid velocity field in a tractable manner. Deep GPs are multi-layer hierarchical generalisations of GPs with multiple, infinitely wide hidden layers that are very efficient models for deep learning and modelling of high-dimensional complex systems by tackling the complexity through several hidden layers connected with non-linear mappings. According to this approach, the hydrogeological data is modelled as the output of a multivariate GP whose inputs are governed by another GP such that each single layer is either a standard GP or the Gaussian process latent variable model. A variational approximation framework is used so that the posterior distribution of the model outputs associated to given inputs can be analytically approximated. In contrast to the other dimensionality reduction, methods that do not provide any information about the dimensionality of each hidden layer, the proposed method automatically selects the dimensionality of each hidden layer and it can be used to propagate uncertainty obtained in each layer across the hierarchy. Using this, dimensionality of the full input space consists of both geometrical parameters of modelling domain and stochastic hydrogeological parameters can be simultaneously reduced without the need for any simplifications generally being assumed for stochastic modelling of subsurface flow problems. It allows estimation of the flow statistics with greatly reduced computational efforts compared to other stochastic approaches such as Monte Carlo method.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-03T11:57:38Z</published>\n",
      "    <arxiv:comment>27 pages, 11 figures, presented in SIAM Conference on Uncertainty Quantification (UQ16)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>A. Daneshkhah</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>O. Chatrabgoun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>M. Esmaeilbeigi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>T. Sedighi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>S. Abolfathi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01715v1</id>\n",
      "    <title>Brain Predictability toolbox: a Python library for neuroimaging based machine learning</title>\n",
      "    <updated>2020-11-03T14:06:43Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01715v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01715v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Summary Brain Predictability toolbox (BPt) represents a unified framework of machine learning (ML) tools designed to work with both tabulated data (in particular brain, psychiatric, behavioral, and physiological variables) and neuroimaging specific derived data (e.g., brain volumes and surfaces). This package is suitable for investigating a wide range of different neuroimaging based ML questions, in particular, those queried from large human datasets.\n",
      "  Availability and Implementation BPt has been developed as an open-source Python 3.6+ package hosted at https://github.com/sahahn/BPt under MIT License, with documentation provided at https://bpt.readthedocs.io/en/latest/, and continues to be actively developed. The project can be downloaded through the github link provided. A web GUI interface based on the same code is currently under development and can be set up through docker with instructions at https://github.com/sahahn/BPt_app.\n",
      "  Contact Please contact Sage Hahn at sahahn@uvm.edu</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-03T14:06:43Z</published>\n",
      "    <arxiv:comment>3 Pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Sage Hahn</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dekang Yuan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wesley Thompson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Max M Owens</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nicholas Allgaier</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hugh Garavan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01725v1</id>\n",
      "    <title>Recommendations for Bayesian hierarchical model specifications for case-control studies in mental health</title>\n",
      "    <updated>2020-11-03T14:19:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01725v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01725v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Hierarchical model fitting has become commonplace for case-control studies of cognition and behaviour in mental health. However, these techniques require us to formalise assumptions about the data-generating process at the group level, which may not be known. Specifically, researchers typically must choose whether to assume all subjects are drawn from a common population, or to model them as deriving from separate populations. These assumptions have profound implications for computational psychiatry, as they affect the resulting inference (latent parameter recovery) and may conflate or mask true group-level differences. To test these assumptions we ran systematic simulations on synthetic multi-group behavioural data from a commonly used multi-armed bandit task (reinforcement learning task). We then examined recovery of group differences in latent parameter space under the two commonly used generative modelling assumptions: (1) modelling groups under a common shared group-level prior (assuming all participants are generated from a common distribution, and are likely to share common characteristics); (2) modelling separate groups based on symptomatology or diagnostic labels, resulting in separate group-level priors. We evaluated the robustness of these approaches to variations in data quality and prior specifications on a variety of metrics. We found that fitting groups separately (assumptions 2), provided the most accurate and robust inference across all conditions. Our results suggest that when dealing with data from multiple clinical groups, researchers should analyse patient and control groups separately as it provides the most accurate and robust recovery of the parameters of interest.</summary>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-03T14:19:59Z</published>\n",
      "    <arxiv:comment>Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended Abstract</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CY\"/>\n",
      "    <author>\n",
      "      <name>Vincent Valton</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Toby Wise</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Oliver J. Robinson</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01734v1</id>\n",
      "    <title>Differentiable Physics Models for Real-world Offline Model-based Reinforcement Learning</title>\n",
      "    <updated>2020-11-03T14:37:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01734v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01734v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A limitation of model-based reinforcement learning (MBRL) is the exploitation of errors in the learned models. Black-box models can fit complex dynamics with high fidelity, but their behavior is undefined outside of the data distribution.Physics-based models are better at extrapolating, due to the general validity of their informed structure, but underfit in the real world due to the presence of unmodeled phenomena. In this work, we demonstrate experimentally that for the offline model-based reinforcement learning setting, physics-based models can be beneficial compared to high-capacity function approximators if the mechanical structure is known. Physics-based models can learn to perform the ball in a cup (BiC) task on a physical manipulator using only 4 minutes of sampled data using offline MBRL. We find that black-box models consistently produce unviable policies for BiC as all predicted trajectories diverge to physically impossible state, despite having access to more data than the physics-based model. In addition, we generalize the approach of physics parameter identification from modeling holonomic multi-body systems to systems with nonholonomic dynamics using end-to-end automatic differentiation.\n",
      "  Videos: https://sites.google.com/view/ball-in-a-cup-in-4-minutes/</summary>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-03T14:37:53Z</published>\n",
      "    <arxiv:primary_category term=\"cs.RO\"/>\n",
      "    <author>\n",
      "      <name>Michael Lutter</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Johannes Silberbauer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joe Watson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jan Peters</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01754v1</id>\n",
      "    <title>ControlVAE: Tuning, Analytical Properties, and Performance Analysis</title>\n",
      "    <updated>2020-10-31T12:32:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01754v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01754v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper reviews the novel concept of controllable variational autoencoder (ControlVAE), discusses its parameter tuning to meet application needs, derives its key analytic properties, and offers useful extensions and applications. ControlVAE is a new variational autoencoder (VAE) framework that combines the automatic control theory with the basic VAE to stabilize the KL-divergence of VAE models to a specified value. It leverages a non-linear PI controller, a variant of the proportional-integral-derivative (PID) control, to dynamically tune the weight of the KL-divergence term in the evidence lower bound (ELBO) using the output KL-divergence as feedback. This allows us to precisely control the KL-divergence to a desired value (set point), which is effective in avoiding posterior collapse and learning disentangled representations. In order to improve the ELBO over the regular VAE, we provide simplified theoretical analysis to inform setting the set point of KL-divergence for ControlVAE. We observe that compared to other methods that seek to balance the two terms in VAE's objective, ControlVAE leads to better learning dynamics. In particular, it can achieve a good trade-off between reconstruction quality and KL-divergence. We evaluate the proposed method on three tasks: image generation, language modeling and disentangled representation learning. The results show that ControlVAE can achieve much better reconstruction quality than the other methods for comparable disentanglement. On the language modeling task, ControlVAE can avoid posterior collapse (KL vanishing) and improve the diversity of generated text. Moreover, our method can change the optimization trajectory, improving the ELBO and the reconstruction quality for image generation.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-31T12:32:39Z</published>\n",
      "    <arxiv:comment>arXiv admin note: substantial text overlap with arXiv:2004.05988</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Huajie Shao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhisheng Xiao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shuochao Yao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aston Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shengzhong Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tarek Abdelzaher</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01755v1</id>\n",
      "    <title>MAD-VAE: Manifold Awareness Defense Variational Autoencoder</title>\n",
      "    <updated>2020-10-31T09:04:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01755v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01755v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Although deep generative models such as Defense-GAN and Defense-VAE have made significant progress in terms of adversarial defenses of image classification neural networks, several methods have been found to circumvent these defenses. Based on Defense-VAE, in our research we introduce several methods to improve the robustness of defense models. The methods introduced in this paper are straight forward yet show promise over the vanilla Defense-VAE. With extensive experiments on MNIST data set, we have demonstrated the effectiveness of our algorithms against different attacks. Our experiments also include attacks on the latent space of the defensive model. We also discuss the applicability of existing adversarial latent space attacks as they may have a significant flaw.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-31T09:04:25Z</published>\n",
      "    <arxiv:comment>15 pages, 13 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Frederick Morlock</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dingsu Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01761v1</id>\n",
      "    <title>Problems using deep generative models for probabilistic audio source separation</title>\n",
      "    <updated>2020-11-03T15:01:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01761v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01761v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent advancements in deep generative modeling make it possible to learn prior distributions from complex data that subsequently can be used for Bayesian inference. However, we find that distributions learned by deep generative models for audio signals do not exhibit the right properties that are necessary for tasks like audio source separation using a probabilistic approach. We observe that the learned prior distributions are either discriminative and extremely peaked or smooth and non-discriminative. We quantify this behavior for two types of deep generative models on two audio datasets.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-03T15:01:47Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>1st I Can't Believe It's Not Better Workshop (ICBINB @ NeurIPS 2020), Vancouver, Canada</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Maurice Frank</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maximilian Ilse</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.01963v1</id>\n",
      "    <title>A Scalable Approach for Privacy-Preserving Collaborative Machine Learning</title>\n",
      "    <updated>2020-11-03T19:09:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.01963v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.01963v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We consider a collaborative learning scenario in which multiple data-owners wish to jointly train a logistic regression model, while keeping their individual datasets private from the other parties. We propose COPML, a fully-decentralized training framework that achieves scalability and privacy-protection simultaneously. The key idea of COPML is to securely encode the individual datasets to distribute the computation load effectively across many parties and to perform the training computations as well as the model updates in a distributed manner on the securely encoded data. We provide the privacy analysis of COPML and prove its convergence. Furthermore, we experimentally demonstrate that COPML can achieve significant speedup in training over the benchmark protocols. Our protocol provides strong statistical privacy guarantees against colluding parties (adversaries) with unbounded computational power, while achieving up to $16\\times$ speedup in the training time against the benchmark protocols.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-11-03T19:09:55Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jinhyun So</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Basak Guler</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>A. Salman Avestimehr</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11251v1</id>\n",
      "    <title>A machine learning based software pipeline to pick the variable ordering for algorithms with polynomial inputs</title>\n",
      "    <updated>2020-05-22T16:00:04Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11251v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11251v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We are interested in the application of Machine Learning (ML) technology to improve mathematical software. It may seem that the probabilistic nature of ML tools would invalidate the exact results prized by such software, however, the algorithms which underpin the software often come with a range of choices which are good candidates for ML application. We refer to choices which have no effect on the mathematical correctness of the software, but do impact its performance.\n",
      "  In the past we experimented with one such choice: the variable ordering to use when building a Cylindrical Algebraic Decomposition (CAD). We used the Python library Scikit-Learn (sklearn) to experiment with different ML models, and developed new techniques for feature generation and hyper-parameter selection.\n",
      "  These techniques could easily be adapted for making decisions other than our immediate application of CAD variable ordering. Hence in this paper we present a software pipeline to use sklearn to pick the variable ordering for an algorithm that acts on a polynomial system. The code described is freely available online.</summary>\n",
      "    <category term=\"cs.SC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-22T16:00:04Z</published>\n",
      "    <arxiv:comment>Accepted into Proc ICMS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.SC\"/>\n",
      "    <arxiv:journal_ref>Mathematical Software (Proc. ICMS '20), pp. 302-322, (Lecture Notes in Computer Science, 12097). Springer International Publishing, 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Dorian Florescu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matthew England</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1007/978-3-030-52200-1_30</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1007/978-3-030-52200-1_30\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11313v1</id>\n",
      "    <title>Comparative Study of Machine Learning Models and BERT on SQuAD</title>\n",
      "    <updated>2020-05-22T17:58:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11313v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11313v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This study aims to provide a comparative analysis of performance of certain models popular in machine learning and the BERT model on the Stanford Question Answering Dataset (SQuAD). The analysis shows that the BERT model, which was once state-of-the-art on SQuAD, gives higher accuracy in comparison to other models. However, BERT requires a greater execution time even when only 100 samples are used. This shows that with increasing accuracy more amount of time is invested in training the data. Whereas in case of preliminary machine learning models, execution time for full data is lower but accuracy is compromised.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-22T17:58:30Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Devshree Patel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Param Raval</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ratnam Parikh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yesha Shastri</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.12181v1</id>\n",
      "    <title>SunDown: Model-driven Per-Panel Solar Anomaly Detection for Residential Arrays</title>\n",
      "    <updated>2020-05-25T15:54:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.12181v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.12181v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>There has been significant growth in both utility-scale and residential-scale solar installations in recent years, driven by rapid technology improvements and falling prices. Unlike utility-scale solar farms that are professionally managed and maintained, smaller residential-scale installations often lack sensing and instrumentation for performance monitoring and fault detection. As a result, faults may go undetected for long periods of time, resulting in generation and revenue losses for the homeowner. In this paper, we present SunDown, a sensorless approach designed to detect per-panel faults in residential solar arrays. SunDown does not require any new sensors for its fault detection and instead uses a model-driven approach that leverages correlations between the power produced by adjacent panels to detect deviations from expected behavior. SunDown can handle concurrent faults in multiple panels and perform anomaly classification to determine probable causes. Using two years of solar generation data from a real home and a manually generated dataset of multiple solar faults, we show that our approach has a MAPE of 2.98\\% when predicting per-panel output. Our results also show that SunDown is able to detect and classify faults, including from snow cover, leaves and debris, and electrical failures with 99.13% accuracy, and can detect multiple concurrent faults with 97.2% accuracy.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-25T15:54:30Z</published>\n",
      "    <arxiv:comment>13 pages, 13 figures. Extended version of a paper that will appear in the Proceedings of the ACM SIGCAS Conference on Computing and Sustainable Societies (COMPASS '20), June 2020, Ecuador</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Menghong Feng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Noman Bashir</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Prashant Shenoy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Irwin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Beka Kosanovic</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.12186v1</id>\n",
      "    <title>Learnability of Timescale Graphical Event Models</title>\n",
      "    <updated>2020-05-25T15:57:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.12186v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.12186v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This technical report tries to fill a gap in current literature on Timescale Graphical Event Models. I propose and evaluate different heuristics to determine hyper-parameters during the structure learning algorithm and refine an existing distance measure. A comprehensive benchmark on synthetic data will be conducted allowing conclusions about the applicability of the different heuristics.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-25T15:57:22Z</published>\n",
      "    <arxiv:comment>Technical Report</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Philipp Behrendt</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.12187v2</id>\n",
      "    <title>AMR Quality Rating with a Lightweight CNN</title>\n",
      "    <updated>2020-12-16T17:15:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.12187v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.12187v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Structured semantic sentence representations such as Abstract Meaning Representations (AMRs) are potentially useful in various NLP tasks. However, the quality of automatic parses can vary greatly and jeopardizes their usefulness. This can be mitigated by models that can accurately rate AMR quality in the absence of costly gold data, allowing us to inform downstream systems about an incorporated parse's trustworthiness or select among different candidate parses.\n",
      "  In this work, we propose to transfer the AMR graph to the domain of images. This allows us to create a simple convolutional neural network (CNN) that imitates a human judge tasked with rating graph quality. Our experiments show that the method can rate quality more accurately than strong baselines, in several quality dimensions. Moreover, the method proves to be efficient and reduces the incurred energy consumption.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-25T15:58:00Z</published>\n",
      "    <arxiv:comment>AACL-IJCNLP 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Juri Opitz</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.12193v1</id>\n",
      "    <title>Feature Statistics Guided Efficient Filter Pruning</title>\n",
      "    <updated>2020-05-21T01:50:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.12193v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.12193v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Building compact convolutional neural networks (CNNs) with reliable performance is a critical but challenging task, especially when deploying them in real-world applications. As a common approach to reduce the size of CNNs, pruning methods delete part of the CNN filters according to some metrics such as $l1$-norm. However, previous methods hardly leverage the information variance in a single feature map and the similarity characteristics among feature maps. In this paper, we propose a novel filter pruning method, which incorporates two kinds of feature map selections: diversity-aware selection (DFS) and similarity-aware selection (SFS). DFS aims to discover features with low information diversity while SFS removes features that have high similarities with others. We conduct extensive empirical experiments with various CNN architectures on publicly available datasets. The experimental results demonstrate that our model obtains up to 91.6% parameter decrease and 83.7% FLOPs reduction with almost no accuracy loss.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-21T01:50:55Z</published>\n",
      "    <arxiv:comment>To appear in Proceedings of IJCAI 2020 (copyright held by IJCAI)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Hang Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chen Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wei Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xue Liu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.12210v1</id>\n",
      "    <title>How Useful are Reviews for Recommendation? A Critical Review and Potential Improvements</title>\n",
      "    <updated>2020-05-25T16:30:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.12210v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.12210v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We investigate a growing body of work that seeks to improve recommender systems through the use of review text. Generally, these papers argue that since reviews 'explain' users' opinions, they ought to be useful to infer the underlying dimensions that predict ratings or purchases. Schemes to incorporate reviews range from simple regularizers to neural network approaches. Our initial findings reveal several discrepancies in reported results, partly due to (e.g.) copying results across papers despite changes in experimental settings or data pre-processing. First, we attempt a comprehensive analysis to resolve these ambiguities. Further investigation calls for discussion on a much larger problem about the \"importance\" of user reviews for recommendation. Through a wide range of experiments, we observe several cases where state-of-the-art methods fail to outperform existing baselines, especially as we deviate from a few narrowly-defined settings where reviews are useful. We conclude by providing hypotheses for our observations, that seek to characterize under what conditions reviews are likely to be helpful. Through this work, we aim to evaluate the direction in which the field is progressing and encourage robust empirical evaluation.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-25T16:30:05Z</published>\n",
      "    <arxiv:comment>4 pages, 3 figures. Accepted for publication at SIGIR '20</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Noveen Sachdeva</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Julian McAuley</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3397271.3401281</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3397271.3401281\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.12254v1</id>\n",
      "    <title>Dynamic Value Estimation for Single-Task Multi-Scene Reinforcement Learning</title>\n",
      "    <updated>2020-05-25T17:56:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.12254v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.12254v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Training deep reinforcement learning agents on environments with multiple levels / scenes / conditions from the same task, has become essential for many applications aiming to achieve generalization and domain transfer from simulation to the real world. While such a strategy is helpful with generalization, the use of multiple scenes significantly increases the variance of samples collected for policy gradient computations. Current methods continue to view this collection of scenes as a single Markov Decision Process (MDP) with a common value function; however, we argue that it is better to treat the collection as a single environment with multiple underlying MDPs. To this end, we propose a dynamic value estimation (DVE) technique for these multiple-MDP environments, motivated by the clustering effect observed in the value function distribution across different scenes. The resulting agent is able to learn a more accurate and scene-specific value function estimate (and hence the advantage function), leading to a lower sample variance. Our proposed approach is simple to accommodate with several existing implementations (like PPO, A3C) and results in consistent improvements for a range of ProcGen environments and the AI2-THOR framework based visual navigation task.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-25T17:56:08Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jaskirat Singh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liang Zheng</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.12263v1</id>\n",
      "    <title>Principal Component Analysis Based on T$\\ell_1$-norm Maximization</title>\n",
      "    <updated>2020-05-23T04:28:45Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.12263v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.12263v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Classical principal component analysis (PCA) may suffer from the sensitivity to outliers and noise. Therefore PCA based on $\\ell_1$-norm and $\\ell_p$-norm ($0 &lt; p &lt; 1$) have been studied. Among them, the ones based on $\\ell_p$-norm seem to be most interesting from the robustness point of view. However, their numerical performance is not satisfactory. Note that, although T$\\ell_1$-norm is similar to $\\ell_p$-norm ($0 &lt; p &lt; 1$) in some sense, it has the stronger suppression effect to outliers and better continuity. So PCA based on T$\\ell_1$-norm is proposed in this paper. Our numerical experiments have shown that its performance is superior than PCA-$\\ell_p$ and $\\ell_p$SPCA as well as PCA, PCA-$\\ell_1$ obviously.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-23T04:28:45Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xiang-Fei Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuan-Hai Shao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chun-Na Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Li-Ming Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nai-Yang Deng</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11932v1</id>\n",
      "    <title>Towards a Robust WiFi-based Fall Detection with Adversarial Data Augmentation</title>\n",
      "    <updated>2020-05-25T05:46:27Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11932v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11932v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent WiFi-based fall detection systems have drawn much attention due to their advantages over other sensory systems. Various implementations have achieved impressive progress in performance, thanks to machine learning and deep learning techniques. However, many of such high accuracy systems have low reliability as they fail to achieve robustness in unseen environments. To address that, this paper investigates a method of generalization through adversarial data augmentation. Our results show a slight improvement in deep learning-systems in unseen domains, though the performance is not significant.</summary>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-25T05:46:27Z</published>\n",
      "    <arxiv:comment>Will appear in Proceedings of the 54th Annual Conference on Information Sciences and Systems (CISS2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.HC\"/>\n",
      "    <arxiv:journal_ref>2020 54th Annual Conference on Information Sciences and Systems (CISS), Princeton, NJ, USA, 2020, pp. 1-6</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Tuan-Duy H. Nguyen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Huu-Nghia H. Nguyen</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/CISS48834.2020.1570617398</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/CISS48834.2020.1570617398\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11988v1</id>\n",
      "    <title>Deep Learning Models for Automatic Summarization</title>\n",
      "    <updated>2020-05-25T09:12:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11988v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11988v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Text summarization is an NLP task which aims to convert a textual document into a shorter one while keeping as much meaning as possible. This pedagogical article reviews a number of recent Deep Learning architectures that have helped to advance research in this field. We will discuss in particular applications of pointer networks, hierarchical Transformers and Reinforcement Learning. We assume basic knowledge of Seq2Seq architecture and Transformer networks within NLP.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-25T09:12:37Z</published>\n",
      "    <arxiv:comment>13 pages, 5 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Pirmin Lemberger</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.12627v1</id>\n",
      "    <title>Memory-Efficient Sampling for Minimax Distance Measures</title>\n",
      "    <updated>2020-05-26T11:00:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.12627v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.12627v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Minimax distance measure extracts the underlying patterns and manifolds in an unsupervised manner. The existing methods require a quadratic memory with respect to the number of objects. In this paper, we investigate efficient sampling schemes in order to reduce the memory requirement and provide a linear space complexity. In particular, we propose a novel sampling technique that adapts well with Minimax distances. We evaluate the methods on real-world datasets from different domains and analyze the results.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-26T11:00:34Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Fazeleh Sadat Hoseini</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Morteza Haghir Chehreghani</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.12719v2</id>\n",
      "    <title>Exhaustive Neural Importance Sampling applied to Monte Carlo event generation</title>\n",
      "    <updated>2020-07-21T06:26:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.12719v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.12719v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The generation of accurate neutrino-nucleus cross-section models needed for neutrino oscillation experiments require simultaneously the description of many degrees of freedom and precise calculations to model nuclear responses. The detailed calculation of complete models makes the Monte Carlo generators slow and impractical. We present Exhaustive Neural Importance Sampling (ENIS), a method based on normalizing flows to find a suitable proposal density for rejection sampling automatically and efficiently, and discuss how this technique solves common issues of the rejection algorithm.</summary>\n",
      "    <category term=\"hep-ex\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-26T13:45:45Z</published>\n",
      "    <arxiv:comment>Published in Physical Review D 102, 013003 (2020). Appeared at the ICML 2020 Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models (INNF+ 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"hep-ex\"/>\n",
      "    <arxiv:journal_ref>Phys. Rev. D 102, 013003 (2020)</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Sebastian Pina-Otey</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Federico Sánchez</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Thorsten Lux</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vicens Gaitan</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1103/PhysRevD.102.013003</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1103/PhysRevD.102.013003\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.10804v3</id>\n",
      "    <title>Reinforcement Learning with General Value Function Approximation: Provably Efficient Approach via Bounded Eluder Dimension</title>\n",
      "    <updated>2020-06-19T17:49:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.10804v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.10804v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Value function approximation has demonstrated phenomenal empirical success in reinforcement learning (RL). Nevertheless, despite a handful of recent progress on developing theory for RL with linear function approximation, the understanding of general function approximation schemes largely remains missing. In this paper, we establish a provably efficient RL algorithm with general value function approximation. We show that if the value functions admit an approximation with a function class $\\mathcal{F}$, our algorithm achieves a regret bound of $\\widetilde{O}(\\mathrm{poly}(dH)\\sqrt{T})$ where $d$ is a complexity measure of $\\mathcal{F}$ that depends on the eluder dimension [Russo and Van Roy, 2013] and log-covering numbers, $H$ is the planning horizon, and $T$ is the number interactions with the environment. Our theory generalizes recent progress on RL with linear value function approximation and does not make explicit assumptions on the model of the environment. Moreover, our algorithm is model-free and provides a framework to justify the effectiveness of algorithms used in practice.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-21T17:36:09Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Ruosong Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruslan Salakhutdinov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lin F. Yang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.10815v1</id>\n",
      "    <title>Can Shallow Neural Networks Beat the Curse of Dimensionality? A mean field training perspective</title>\n",
      "    <updated>2020-05-21T17:50:15Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.10815v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.10815v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We prove that the gradient descent training of a two-layer neural network on empirical or population risk may not decrease population risk at an order faster than $t^{-4/(d-2)}$ under mean field scaling. Thus gradient descent training for fitting reasonably smooth, but truly high-dimensional data may be subject to the curse of dimensionality. We present numerical evidence that gradient descent training with general Lipschitz target functions becomes slower and slower as the dimension increases, but converges at approximately the same rate in all dimensions when the target function lies in the natural function space for two-layer ReLU networks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-21T17:50:15Z</published>\n",
      "    <arxiv:comment>5 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Stephan Wojtowytsch</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Weinan E</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.10851v1</id>\n",
      "    <title>Conditionally Deep Hybrid Neural Networks Across Edge and Cloud</title>\n",
      "    <updated>2020-05-21T18:18:43Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.10851v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.10851v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The pervasiveness of \"Internet-of-Things\" in our daily life has led to a recent surge in fog computing, encompassing a collaboration of cloud computing and edge intelligence. To that effect, deep learning has been a major driving force towards enabling such intelligent systems. However, growing model sizes in deep learning pose a significant challenge towards deployment in resource-constrained edge devices. Moreover, in a distributed intelligence environment, efficient workload distribution is necessary between edge and cloud systems. To address these challenges, we propose a conditionally deep hybrid neural network for enabling AI-based fog computing. The proposed network can be deployed in a distributed manner, consisting of quantized layers and early exits at the edge and full-precision layers on the cloud. During inference, if an early exit has high confidence in the classification results, it would allow samples to exit at the edge, and the deeper layers on the cloud are activated conditionally, which can lead to improved energy efficiency and inference latency. We perform an extensive design space exploration with the goal of minimizing energy consumption at the edge while achieving state-of-the-art classification accuracies on image classification tasks. We show that with binarized layers at the edge, the proposed conditional hybrid network can process 65% of inferences at the edge, leading to 5.5x computational energy reduction with minimal accuracy degradation on CIFAR-10 dataset. For the more complex dataset CIFAR-100, we observe that the proposed network with 4-bit quantization at the edge achieves 52% early classification at the edge with 4.8x energy reduction. The analysis gives us insights on designing efficient hybrid networks which achieve significantly higher energy efficiency than full-precision networks for edge-cloud based distributed intelligence systems.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-21T18:18:43Z</published>\n",
      "    <arxiv:comment>6 pages, 5 figures, 4 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yinghan Long</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Indranil Chakraborty</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kaushik Roy</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.10917v2</id>\n",
      "    <title>Succinct Trit-array Trie for Scalable Trajectory Similarity Search</title>\n",
      "    <updated>2020-09-21T21:01:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.10917v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.10917v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Massive datasets of spatial trajectories representing the mobility of a diversity of moving objects are ubiquitous in research and industry. Similarity search of a large collection of trajectories is indispensable for turning these datasets into knowledge. Locality sensitive hashing (LSH) is a powerful technique for fast similarity searches. Recent methods employ LSH and attempt to realize an efficient similarity search of trajectories; however, those methods are inefficient in terms of search time and memory when applied to massive datasets. To address this problem, we present the trajectory-indexing succinct trit-array trie (tSTAT), which is a scalable method leveraging LSH for trajectory similarity searches. tSTAT quickly performs the search on a tree data structure called trie. We also present two novel techniques that enable to dramatically enhance the memory efficiency of tSTAT. One is a node reduction technique that substantially omits redundant trie nodes while maintaining the time performance. The other is a space-efficient representation that leverages the idea behind succinct data structures (i.e., a compressed data structure supporting fast data operations). We experimentally test tSTAT on its ability to retrieve similar trajectories for a query from large collections of trajectories and show that tSTAT performs superiorly in comparison to state-of-the-art similarity search methods.</summary>\n",
      "    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-21T21:42:30Z</published>\n",
      "    <arxiv:comment>Accepted by ACM SIGSPATIAL 2020 as a full paper</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.DS\"/>\n",
      "    <author>\n",
      "      <name>Shunsuke Kanda</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Koh Takeuchi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Keisuke Fujii</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yasuo Tabei</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.09801v2</id>\n",
      "    <title>FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval</title>\n",
      "    <updated>2020-05-29T05:56:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.09801v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.09801v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we address the text and image matching in cross-modal retrieval of the fashion industry. Different from the matching in the general domain, the fashion matching is required to pay much more attention to the fine-grained information in the fashion images and texts. Pioneer approaches detect the region of interests (i.e., RoIs) from images and use the RoI embeddings as image representations. In general, RoIs tend to represent the \"object-level\" information in the fashion images, while fashion texts are prone to describe more detailed information, e.g. styles, attributes. RoIs are thus not fine-grained enough for fashion text and image matching. To this end, we propose FashionBERT, which leverages patches as image features. With the pre-trained BERT model as the backbone network, FashionBERT learns high level representations of texts and images. Meanwhile, we propose an adaptive loss to trade off multitask learning in the FashionBERT modeling. Two tasks (i.e., text and image matching and cross-modal retrieval) are incorporated to evaluate FashionBERT. On the public dataset, experiments demonstrate FashionBERT achieves significant improvements in performances than the baseline and state-of-the-art approaches. In practice, FashionBERT is applied in a concrete cross-modal retrieval application. We provide the detailed matching performance and inference efficiency analysis.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-20T00:41:00Z</published>\n",
      "    <arxiv:comment>10 pages, to be published in SIGIR20 Industry Track</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Dehong Gao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Linbo Jin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ben Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Minghui Qiu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peng Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yi Wei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yi Hu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hao Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.09810v3</id>\n",
      "    <title>$p$-Norm Flow Diffusion for Local Graph Clustering</title>\n",
      "    <updated>2020-07-20T09:09:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.09810v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.09810v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Local graph clustering and the closely related seed set expansion problem are primitives on graphs that are central to a wide range of analytic and learning tasks such as local clustering, community detection, nodes ranking and feature inference. Prior work on local graph clustering mostly falls into two categories with numerical and combinatorial roots respectively. In this work, we draw inspiration from both fields and propose a family of convex optimization formulations based on the idea of diffusion with p-norm network flow for $p\\in (1,\\infty)$. In the context of local clustering, we characterize the optimal solutions for these optimization problems and show their usefulness in finding low conductance cuts around input seed set. In particular, we achieve quadratic approximation of conductance in the case of $p=2$ similar to the Cheeger-type bounds of spectral methods, constant factor approximation when $p\\rightarrow\\infty$ similar to max-flow based methods, and a smooth transition for general $p$ values in between. Thus, our optimization formulation can be viewed as bridging the numerical and combinatorial approaches, and we can achieve the best of both worlds in terms of speed and noise robustness. We show that the proposed problem can be solved in strongly local running time for $p\\ge 2$ and conduct empirical evaluations on both synthetic and real-world graphs to illustrate our approach compares favorably with existing methods.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-20T01:08:17Z</published>\n",
      "    <arxiv:comment>28 pages, 5 figures, 3 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Kimon Fountoulakis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Di Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shenghao Yang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.09841v1</id>\n",
      "    <title>Best Arm Identification in Spectral Bandits</title>\n",
      "    <updated>2020-05-20T04:12:04Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.09841v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.09841v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study best-arm identification with fixed confidence in bandit models with graph smoothness constraint. We provide and analyze an efficient gradient ascent algorithm to compute the sample complexity of this problem as a solution of a non-smooth max-min problem (providing in passing a simplified analysis for the unconstrained case). Building on this algorithm, we propose an asymptotically optimal strategy. We furthermore illustrate by numerical experiments both the strategy's efficiency and the impact of the smoothness constraint on the sample complexity. Best Arm Identification (BAI) is an important challenge in many applications ranging from parameter tuning to clinical trials. It is now very well understood in vanilla bandit models, but real-world problems typically involve some dependency between arms that requires more involved models. Assuming a graph structure on the arms is an elegant practical way to encompass this phenomenon, but this had been done so far only for regret minimization. Addressing BAI with graph constraints involves delicate optimization problems for which the present paper offers a solution.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-20T04:12:04Z</published>\n",
      "    <arxiv:comment>To be published in International Joint Conference on Artificial Intelligence</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Tomáš Kocák</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aurélien Garivier</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.09856v2</id>\n",
      "    <title>A Novel Meta Learning Framework for Feature Selection using Data Synthesis and Fuzzy Similarity</title>\n",
      "    <updated>2020-05-21T03:41:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.09856v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.09856v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper presents a novel meta learning framework for feature selection (FS) based on fuzzy similarity. The proposed method aims to recommend the best FS method from four candidate FS methods for any given dataset. This is achieved by firstly constructing a large training data repository using data synthesis. Six meta features that represent the characteristics of the training dataset are then extracted. The best FS method for each of the training datasets is used as the meta label. Both the meta features and the corresponding meta labels are subsequently used to train a classification model using a fuzzy similarity measure based framework. Finally the trained model is used to recommend the most suitable FS method for a given unseen dataset. This proposed method was evaluated based on eight public datasets of real-world applications. It successfully recommended the best method for five datasets and the second best method for one dataset, which outperformed any of the four individual FS methods. Besides, the proposed method is computationally efficient for algorithm selection, leading to negligible additional time for the feature selection process. Thus, the paper contributes a novel method for effectively recommending which feature selection method to use for any new given dataset.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-20T06:03:41Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Zixiao Shen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xin Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jonathan M. Garibaldi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.09910v2</id>\n",
      "    <title>Multitask Learning with Single Gradient Step Update for Task Balancing</title>\n",
      "    <updated>2020-06-02T12:29:42Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.09910v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.09910v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Multitask learning is a methodology to boost generalization performance and also reduce computational intensity and memory usage. However, learning multiple tasks simultaneously can be more difficult than learning a single task because it can cause imbalance among tasks. To address the imbalance problem, we propose an algorithm to balance between tasks at the gradient level by applying gradient-based meta-learning to multitask learning. The proposed method trains shared layers and task-specific layers separately so that the two layers with different roles in a multitask network can be fitted to their own purposes. In particular, the shared layer that contains informative knowledge shared among tasks is trained by employing single gradient step update and inner/outer loop training to mitigate the imbalance problem at the gradient level. We apply the proposed method to various multitask computer vision problems and achieve state-of-the-art performance.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-20T08:34:20Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Sungjae Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Youngdoo Son</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.09971v1</id>\n",
      "    <title>Hidden Markov Models and their Application for Predicting Failure Events</title>\n",
      "    <updated>2020-05-20T11:30:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.09971v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.09971v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We show how Markov mixed membership models (MMMM) can be used to predict the degradation of assets. We model the degradation path of individual assets, to predict overall failure rates. Instead of a separate distribution for each hidden state, we use hierarchical mixtures of distributions in the exponential family. In our approach the observation distribution of the states is a finite mixture distribution of a small set of (simpler) distributions shared across all states. Using tied-mixture observation distributions offers several advantages. The mixtures act as a regularization for typically very sparse problems, and they reduce the computational effort for the learning algorithm since there are fewer distributions to be found. Using shared mixtures enables sharing of statistical strength between the Markov states and thus transfer learning. We determine for individual assets the trade-off between the risk of failure and extended operating hours by combining a MMMM with a partially observable Markov decision process (POMDP) to dynamically optimize the policy for when and how to maintain the asset.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-20T11:30:16Z</published>\n",
      "    <arxiv:comment>Will be published in the proceedings of ICCS 2020; @Booklet{EasyChair:3183, author = {Paul Hofmann and Zaid Tashman}, title = {Hidden Markov Models and their Application for Predicting Failure Events}, howpublished = {EasyChair Preprint no. 3183}, year = {EasyChair, 2020}}</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Paul Hofmann</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zaid Tashman</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1007/978-3-030-50420-5_35</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1007/978-3-030-50420-5_35\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.10211v2</id>\n",
      "    <title>A Metric Learning Approach to Anomaly Detection in Video Games</title>\n",
      "    <updated>2020-07-01T13:27:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.10211v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.10211v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>With the aim of designing automated tools that assist in the video game quality assurance process, we frame the problem of identifying bugs in video games as an anomaly detection (AD) problem. We develop State-State Siamese Networks (S3N) as an efficient deep metric learning approach to AD in this context and explore how it may be used as part of an automated testing tool. Finally, we show by empirical evaluation on a series of Atari games, that S3N is able to learn a meaningful embedding, and consequently is able to identify various common types of video game bugs.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-20T17:23:21Z</published>\n",
      "    <arxiv:comment>4 pages, 3 figures, Accepted in IEEE 2020 CONFERENCE ON GAMES (COG), Dataset https://www.kaggle.com/benedictwilkinsai/atari-anomaly-dataset-aad , Code and pre-trained models https://github.com/BenedictWilkinsAI/S3N</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Benedict Wilkins</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chris Watkins</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kostas Stathis</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.10227v1</id>\n",
      "    <title>Hemogram Data as a Tool for Decision-making in COVID-19 Management: Applications to Resource Scarcity Scenarios</title>\n",
      "    <updated>2020-05-10T01:45:03Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.10227v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.10227v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>COVID-19 pandemics has challenged emergency response systems worldwide, with widespread reports of essential services breakdown and collapse of health care structure. A critical element involves essential workforce management since current protocols recommend release from duty for symptomatic individuals, including essential personnel. Testing capacity is also problematic in several countries, where diagnosis demand outnumbers available local testing capacity. This work describes a machine learning model derived from hemogram exam data performed in symptomatic patients and how they can be used to predict qRT-PCR test results. Methods: A Naive-Bayes model for machine learning is proposed for handling different scarcity scenarios, including managing symptomatic essential workforce and absence of diagnostic tests. Hemogram result data was used to predict qRT-PCR results in situations where the latter was not performed, or results are not yet available. Adjusts in assumed prior probabilities allow fine-tuning of the model, according to actual prediction context. Proposed models can predict COVID-19 qRT-PCR results in symptomatic individuals with high accuracy, sensitivity and specificity. Data assessment can be performed in an individual or simultaneous basis, according to desired outcome. Based on hemogram data and background scarcity context, resource distribution is significantly optimized when model-based patient selection is observed, compared to random choice. The model can help manage testing deficiency and other critical circumstances. Machine learning models can be derived from widely available, quick, and inexpensive exam data in order to predict qRT-PCR results used in COVID-19 diagnosis. These models can be used to assist strategic decision-making in resource scarcity scenarios, including personnel shortage, lack of medical resources, and testing insufficiency.</summary>\n",
      "    <category term=\"q-bio.OT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-10T01:45:03Z</published>\n",
      "    <arxiv:comment>14 pages, 5 figures, 2 tables, Tool Available at: http://sbcb.inf.ufrgs.br/covid</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"q-bio.OT\"/>\n",
      "    <author>\n",
      "      <name>Eduardo Avila</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marcio Dorn</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Clarice Sampaio Alho</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alessandro Kahmann</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.10985v2</id>\n",
      "    <title>Apply VGGNet-based deep learning model of vibration data for prediction model of gravity acceleration equipment</title>\n",
      "    <updated>2020-08-19T02:49:31Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.10985v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.10985v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Hypergravity accelerators are a type of large machinery used for gravity training or medical research. A failure of such large equipment can be a serious problem in terms of safety or costs. This paper proposes a prediction model that can proactively prevent failures that may occur in a hypergravity accelerator. The method proposed in this paper was to convert vibration signals to spectograms and perform classification training using a deep learning model. An experiment was conducted to evaluate the performance of the method proposed in this paper. A 4-channel accelerometer was attached to the bearing housing, which is a rotor, and time-amplitude data were obtained from the measured values by sampling. The data were converted to a two-dimensional spectrogram, and classification training was performed using a deep learning model for four conditions of the equipment: Unbalance, Misalignment, Shaft Rubbing, and Normal. The experimental results showed that the proposed method had a 99.5% F1-Score, which was up to 23% higher than the 76.25% for existing feature-based learning models.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-22T03:36:06Z</published>\n",
      "    <arxiv:comment>15 pages, 10 figures \"for associated publication of paper is as follow: Journal of Mechanics in Medicine and Biology, https://www.worldscientific.com/worldscinet/jmmb\"</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>SeonWoo Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>HyeonTak Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>HoJun Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>JaeHeung Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>GangMin Lim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>KyuSung Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>ByeongKeun Choi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>JangWoo Kwon</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11031v1</id>\n",
      "    <title>Feature selection for gesture recognition in Internet-of-Things for healthcare</title>\n",
      "    <updated>2020-05-22T06:54:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11031v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11031v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Internet of Things is rapidly spreading across several fields, including healthcare, posing relevant questions related to communication capabilities, energy efficiency and sensors unobtrusiveness. Particularly, in the context of recognition of gestures, e.g., grasping of different objects, brain and muscular activity could be simultaneously recorded via EEG and EMG, respectively, and analyzed to identify the gesture that is being accomplished, and the quality of its performance. This paper proposes a new algorithm that aims (i) to robustly extract the most relevant features to classify different grasping tasks, and (ii) to retain the natural meaning of the selected features. This, in turn, gives the opportunity to simplify the recording setup to minimize the data traffic over the communication network, including Internet, and provide physiologically significant features for medical interpretation. The algorithm robustness is ensured both by consensus clustering as a feature selection strategy, and by nested cross-validation scheme to evaluate its classification performance.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-22T06:54:53Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <arxiv:journal_ref>ICC 2020 - 2020 IEEE International Conference on Communications (ICC)</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Giulia Cisotto</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Martina Capuzzo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anna V. Guglielmi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrea Zanella</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/ICC40277.2020.9149381</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/ICC40277.2020.9149381\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11035v4</id>\n",
      "    <title>Position-based Scaled Gradient for Model Quantization and Pruning</title>\n",
      "    <updated>2020-11-11T03:43:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11035v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11035v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose the position-based scaled gradient (PSG) that scales the gradient depending on the position of a weight vector to make it more compression-friendly. First, we theoretically show that applying PSG to the standard gradient descent (GD), which is called PSGD, is equivalent to the GD in the warped weight space, a space made by warping the original weight space via an appropriately designed invertible function. Second, we empirically show that PSG acting as a regularizer to a weight vector is favorable for model compression domains such as quantization and pruning. PSG reduces the gap between the weight distributions of a full-precision model and its compressed counterpart. This enables the versatile deployment of a model either as an uncompressed mode or as a compressed mode depending on the availability of resources. The experimental results on CIFAR-10/100 and ImageNet datasets show the effectiveness of the proposed PSG in both domains of pruning and quantization even for extremely low bits. The code is released in Github.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-22T07:11:27Z</published>\n",
      "    <arxiv:comment>Advances in Neural Information Processing Systems</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Jangho Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>KiYoon Yoo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nojun Kwak</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11100v1</id>\n",
      "    <title>Premium Access to Convolutional Neural Networks</title>\n",
      "    <updated>2020-05-22T10:54:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11100v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11100v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Neural Networks (NNs) are today used for all our daily tasks; for instance, in mobile phones. We here want to show how to restrict their access to privileged users. Our solution relies on a degraded implementation which can be corrected thanks to a PIN. We explain how to select a few parameters in an NN so as to maximize the gap in the accuracy between the premium and the degraded modes. We report experiments on an implementation of our proposal on a deep NN to prove its practicability.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-22T10:54:23Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Julien Bringer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hervé Chabanne</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Linda Guiga</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11558v1</id>\n",
      "    <title>Invariant 3D Shape Recognition using Predictive Modular Neural Networks</title>\n",
      "    <updated>2020-05-23T16:16:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11558v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11558v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper PREMONN (PREdictive MOdular Neural Networks) model/architecture is generalized to functions of two variables and to non-Euclidean spaces. It is presented in the context of 3D invariant shape recognition and texture recognition. PREMONN uses local relation, it is modular and exhibits incremental learning. The recognition process can start at any point on a shape or texture, so a reference point is not needed. Its local relation characteristic enables it to recognize shape and texture even in presence of occlusion. The analysis is mainly mathematical. However, we present some experimental results. The methods presented in this paper can be applied to many problems such as gesture recognition, action recognition, dynamic texture recognition etc.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-23T16:16:37Z</published>\n",
      "    <arxiv:comment>17 pages, 2 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Vasileios Petridis</name>\n",
      "      <arxiv:affiliation>Dept. of Electrical and Computer Engineering, Aristotle University, Thessaloniki, Greece</arxiv:affiliation>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11603v2</id>\n",
      "    <title>Geometric algorithms for predicting resilience and recovering damage in neural networks</title>\n",
      "    <updated>2020-06-02T19:20:49Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11603v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11603v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Biological neural networks have evolved to maintain performance despite significant circuit damage. To survive damage, biological network architectures have both intrinsic resilience to component loss and also activate recovery programs that adjust network weights through plasticity to stabilize performance. Despite the importance of resilience in technology applications, the resilience of artificial neural networks is poorly understood, and autonomous recovery algorithms have yet to be developed. In this paper, we establish a mathematical framework to analyze the resilience of artificial neural networks through the lens of differential geometry. Our geometric language provides natural algorithms that identify local vulnerabilities in trained networks as well as recovery algorithms that dynamically adjust networks to compensate for damage. We reveal striking vulnerabilities in commonly used image analysis networks, like MLP's and CNN's trained on MNIST and CIFAR10 respectively. We also uncover high-performance recovery paths that enable the same networks to dynamically re-adjust their parameters to compensate for damage. Broadly, our work provides procedures that endow artificial systems with resilience and rapid-recovery routines to enhance their integration with IoT devices as well as enable their deployment for critical applications.</summary>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-23T21:13:26Z</published>\n",
      "    <arxiv:comment>10 pages and 4 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.NE\"/>\n",
      "    <author>\n",
      "      <name>Guruprasad Raghavan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiayi Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matt Thomson</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11640v3</id>\n",
      "    <title>Jointly Encoding Word Confusion Network and Dialogue Context with BERT for Spoken Language Understanding</title>\n",
      "    <updated>2020-09-08T02:45:43Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11640v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11640v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Spoken Language Understanding (SLU) converts hypotheses from automatic speech recognizer (ASR) into structured semantic representations. ASR recognition errors can severely degenerate the performance of the subsequent SLU module. To address this issue, word confusion networks (WCNs) have been used to encode the input for SLU, which contain richer information than 1-best or n-best hypotheses list. To further eliminate ambiguity, the last system act of dialogue context is also utilized as additional input. In this paper, a novel BERT based SLU model (WCN-BERT SLU) is proposed to encode WCNs and the dialogue context jointly. It can integrate both structural information and ASR posterior probabilities of WCNs in the BERT architecture. Experiments on DSTC2, a benchmark of SLU, show that the proposed method is effective and can outperform previous state-of-the-art models significantly.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-24T02:26:13Z</published>\n",
      "    <arxiv:comment>Accepted to INTERSPEECH 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Chen Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Su Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zijian Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruisheng Cao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lu Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kai Yu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11715v1</id>\n",
      "    <title>A Lightweight CNN and Joint Shape-Joint Space (JS2) Descriptor for Radiological Osteoarthritis Detection</title>\n",
      "    <updated>2020-05-24T10:48:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11715v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11715v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Knee osteoarthritis (OA) is very common progressive and degenerative musculoskeletal disease worldwide creates a heavy burden on patients with reduced quality of life and also on society due to financial impact. Therefore, any attempt to reduce the burden of the disease could help both patients and society. In this study, we propose a fully automated novel method, based on combination of joint shape and convolutional neural network (CNN) based bone texture features, to distinguish between the knee radiographs with and without radiographic osteoarthritis. Moreover, we report the first attempt at describing the bone texture using CNN. Knee radiographs from Osteoarthritis Initiative (OAI) and Multicenter Osteoarthritis (MOST) studies were used in the experiments. Our models were trained on 8953 knee radiographs from OAI and evaluated on 3445 knee radiographs from MOST. Our results demonstrate that fusing the proposed shape and texture parameters achieves the state-of-the art performance in radiographic OA detection yielding area under the ROC curve (AUC) of 95.21%</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-24T10:48:38Z</published>\n",
      "    <arxiv:comment>MIUA2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Neslihan Bayramoglu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Miika T. Nieminen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Simo Saarakkala</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2005.11716v1</id>\n",
      "    <title>Multi-view Alignment and Generation in CCA via Consistent Latent Encoding</title>\n",
      "    <updated>2020-05-24T10:50:15Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2005.11716v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2005.11716v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Multi-view alignment, achieving one-to-one correspondence of multi-view inputs, is critical in many real-world multi-view applications, especially for cross-view data analysis problems. Recently, an increasing number of works study this alignment problem with Canonical Correlation Analysis (CCA). However, existing CCA models are prone to misalign the multiple views due to either the neglect of uncertainty or the inconsistent encoding of the multiple views. To tackle these two issues, this paper studies multi-view alignment from the Bayesian perspective. Delving into the impairments of inconsistent encodings, we propose to recover correspondence of the multi-view inputs by matching the marginalization of the joint distribution of multi-view random variables under different forms of factorization. To realize our design, we present Adversarial CCA (ACCA) which achieves consistent latent encodings by matching the marginalized latent encodings through the adversarial training paradigm. Our analysis based on conditional mutual information reveals that ACCA is flexible for handling implicit distributions. Extensive experiments on correlation analysis and cross-view generation under noisy input settings demonstrate the superiority of our model.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-24T10:50:15Z</published>\n",
      "    <arxiv:comment>37 pages, 22 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yaxin Shi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuangang Pan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Donna Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ivor W. Tsang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03160v2</id>\n",
      "    <title>Hierarchical Optimal Transport for Robust Multi-View Learning</title>\n",
      "    <updated>2020-06-08T14:54:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03160v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03160v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Traditional multi-view learning methods often rely on two assumptions: ($i$) the samples in different views are well-aligned, and ($ii$) their representations in latent space obey the same distribution. Unfortunately, these two assumptions may be questionable in practice, which limits the application of multi-view learning. In this work, we propose a hierarchical optimal transport (HOT) method to mitigate the dependency on these two assumptions. Given unaligned multi-view data, the HOT method penalizes the sliced Wasserstein distance between the distributions of different views. These sliced Wasserstein distances are used as the ground distance to calculate the entropic optimal transport across different views, which explicitly indicates the clustering structure of the views. The HOT method is applicable to both unsupervised and semi-supervised learning, and experimental results show that it performs robustly on both synthetic and real-world tasks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-04T22:24:45Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Dixin Luo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hongteng Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lawrence Carin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03167v1</id>\n",
      "    <title>Inject Machine Learning into Significance Test for Misspecified Linear Models</title>\n",
      "    <updated>2020-06-04T23:22:04Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03167v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03167v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Due to its strong interpretability, linear regression is widely used in social science, from which significance test provides the significance level of models or coefficients in the traditional statistical inference. However, linear regression methods rely on the linear assumptions of the ground truth function, which do not necessarily hold in practice. As a result, even for simple non-linear cases, linear regression may fail to report the correct significance level.\n",
      "  In this paper, we present a simple and effective assumption-free method for linear approximation in both linear and non-linear scenarios. First, we apply a machine learning method to fit the ground truth function on the training set and calculate its linear approximation. Afterward, we get the estimator by adding adjustments based on the validation set. We prove the concentration inequalities and asymptotic properties of our estimator, which leads to the corresponding significance test. Experimental results show that our estimator significantly outperforms linear regression for non-linear ground truth functions, indicating that our estimator might be a better tool for the significance test.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-04T23:22:04Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jiaye Teng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yang Yuan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03177v2</id>\n",
      "    <title>Hardness of Learning Neural Networks with Natural Weights</title>\n",
      "    <updated>2020-10-13T23:18:45Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03177v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03177v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Neural networks are nowadays highly successful despite strong hardness results. The existing hardness results focus on the network architecture, and assume that the network's weights are arbitrary. A natural approach to settle the discrepancy is to assume that the network's weights are \"well-behaved\" and posses some generic properties that may allow efficient learning. This approach is supported by the intuition that the weights in real-world networks are not arbitrary, but exhibit some \"random-like\" properties with respect to some \"natural\" distributions. We prove negative results in this regard, and show that for depth-$2$ networks, and many \"natural\" weights distributions such as the normal and the uniform distribution, most networks are hard to learn. Namely, there is no efficient learning algorithm that is provably successful for most weights, and every input distribution. It implies that there is no generic property that holds with high probability in such random networks and allows efficient learning.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-05T00:14:20Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Amit Daniely</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gal Vardi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03416v1</id>\n",
      "    <title>Entropy-Regularized $2$-Wasserstein Distance between Gaussian Measures</title>\n",
      "    <updated>2020-06-05T13:18:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03416v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03416v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Gaussian distributions are plentiful in applications dealing in uncertainty quantification and diffusivity. They furthermore stand as important special cases for frameworks providing geometries for probability measures, as the resulting geometry on Gaussians is often expressible in closed-form under the frameworks. In this work, we study the Gaussian geometry under the entropy-regularized 2-Wasserstein distance, by providing closed-form solutions for the distance and interpolations between elements. Furthermore, we provide a fixed-point characterization of a population barycenter when restricted to the manifold of Gaussians, which allows computations through the fixed-point iteration algorithm. As a consequence, the results yield closed-form expressions for the 2-Sinkhorn divergence. As the geometries change by varying the regularization magnitude, we study the limiting cases of vanishing and infinite magnitudes, reconfirming well-known results on the limits of the Sinkhorn divergence. Finally, we illustrate the resulting geometries with a numerical study.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-05T13:18:57Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Anton Mallasto</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Augusto Gerolin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hà Quang Minh</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03451v2</id>\n",
      "    <title>Sparsified Linear Programming for Zero-Sum Equilibrium Finding</title>\n",
      "    <updated>2020-06-29T22:23:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03451v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03451v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Computational equilibrium finding in large zero-sum extensive-form imperfect-information games has led to significant recent AI breakthroughs. The fastest algorithms for the problem are new forms of counterfactual regret minimization [Brown and Sandholm, 2019]. In this paper we present a totally different approach to the problem, which is competitive and often orders of magnitude better than the prior state of the art. The equilibrium-finding problem can be formulated as a linear program (LP) [Koller et al., 1994], but solving it as an LP has not been scalable due to the memory requirements of LP solvers, which can often be quadratically worse than CFR-based algorithms. We give an efficient practical algorithm that factors a large payoff matrix into a product of two matrices that are typically dramatically sparser. This allows us to express the equilibrium-finding problem as a linear program with size only a logarithmic factor worse than CFR, and thus allows linear program solvers to run on such games. With experiments on poker endgames, we demonstrate in practice, for the first time, that modern linear program solvers are competitive against even game-specific modern variants of CFR in solving large extensive-form games, and can be used to compute exact solutions unlike iterative algorithms like CFR.</summary>\n",
      "    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-05T13:48:26Z</published>\n",
      "    <arxiv:comment>Accepted for publication at ICML 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.GT\"/>\n",
      "    <author>\n",
      "      <name>Brian Hu Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tuomas Sandholm</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03465v1</id>\n",
      "    <title>Visual Transfer for Reinforcement Learning via Wasserstein Domain Confusion</title>\n",
      "    <updated>2020-06-04T16:31:26Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03465v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03465v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We introduce Wasserstein Adversarial Proximal Policy Optimization (WAPPO), a novel algorithm for visual transfer in Reinforcement Learning that explicitly learns to align the distributions of extracted features between a source and target task. WAPPO approximates and minimizes the Wasserstein-1 distance between the distributions of features from source and target domains via a novel Wasserstein Confusion objective. WAPPO outperforms the prior state-of-the-art in visual transfer and successfully transfers policies across Visual Cartpole and two instantiations of 16 OpenAI Procgen environments.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-04T16:31:26Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Josh Roy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>George Konidaris</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03486v1</id>\n",
      "    <title>Segmentation of Surgical Instruments for Minimally-Invasive Robot-Assisted Procedures Using Generative Deep Neural Networks</title>\n",
      "    <updated>2020-06-05T14:39:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03486v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03486v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This work proves that semantic segmentation on minimally invasive surgical instruments can be improved by using training data that has been augmented through domain adaptation. The benefit of this method is twofold. Firstly, it suppresses the need of manually labeling thousands of images by transforming synthetic data into realistic-looking data. To achieve this, a CycleGAN model is used, which transforms a source dataset to approximate the domain distribution of a target dataset. Secondly, this newly generated data with perfect labels is utilized to train a semantic segmentation neural network, U-Net. This method shows generalization capabilities on data with variability regarding its rotation- position- and lighting conditions. Nevertheless, one of the caveats of this approach is that the model is unable to generalize well to other surgical instruments with a different shape from the one used for training. This is driven by the lack of a high variance in the geometric distribution of the training data. Future work will focus on making the model more scale-invariant and able to adapt to other types of surgical instruments previously unseen by the training.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-05T14:39:41Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Iñigo Azqueta-Gavaldon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Florian Fröhlich</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Klaus Strobl</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rudolph Triebel</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03550v1</id>\n",
      "    <title>The Expected Jacobian Outerproduct: Theory and Empirics</title>\n",
      "    <updated>2020-06-05T16:42:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03550v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03550v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The expected gradient outerproduct (EGOP) of an unknown regression function is an operator that arises in the theory of multi-index regression, and is known to recover those directions that are most relevant to predicting the output. However, work on the EGOP, including that on its cheap estimators, is restricted to the regression setting. In this work, we adapt this operator to the multi-class setting, which we dub the expected Jacobian outerproduct (EJOP). Moreover, we propose a simple rough estimator of the EJOP and show that somewhat surprisingly, it remains statistically consistent under mild assumptions. Furthermore, we show that the eigenvalues and eigenspaces also remain consistent. Finally, we show that the estimated EJOP can be used as a metric to yield improvements in real-world non-parametric classification tasks: both by its use as a metric, and also as cheap initialization in metric learning tasks.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-05T16:42:09Z</published>\n",
      "    <arxiv:comment>Technical Report</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Shubhendu Trivedi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>J. Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03555v3</id>\n",
      "    <title>Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers</title>\n",
      "    <updated>2020-10-01T00:41:49Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03555v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03555v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Transformer models have achieved state-of-the-art results across a diverse range of domains. However, concern over the cost of training the attention mechanism to learn complex dependencies between distant inputs continues to grow. In response, solutions that exploit the structure and sparsity of the learned attention matrix have blossomed. However, real-world applications that involve long sequences, such as biological sequence analysis, may fall short of meeting these assumptions, precluding exploration of these models. To address this challenge, we present a new Transformer architecture, Performer, based on Fast Attention Via Orthogonal Random features (FAVOR). Our mechanism scales linearly rather than quadratically in the number of tokens in the sequence, is characterized by sub-quadratic space complexity and does not incorporate any sparsity pattern priors. Furthermore, it provides strong theoretical guarantees: unbiased estimation of the attention matrix and uniform convergence. It is also backwards-compatible with pre-trained regular Transformers. We demonstrate its effectiveness on the challenging task of protein sequence modeling and provide detailed theoretical analysis.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-05T17:09:16Z</published>\n",
      "    <arxiv:comment>This arXiv submission has been deprecated. Please see \"Rethinking Attention with Performers\" at arXiv:2009.14794 for the most updated version of the paper</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Krzysztof Choromanski</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Valerii Likhosherstov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Dohan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xingyou Song</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andreea Gane</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tamas Sarlos</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peter Hawkins</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jared Davis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Belanger</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lucy Colwell</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Adrian Weller</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.02879v1</id>\n",
      "    <title>Auto-decoding Graphs</title>\n",
      "    <updated>2020-06-04T14:23:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.02879v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.02879v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present an approach to synthesizing new graph structures from empirically specified distributions. The generative model is an auto-decoder that learns to synthesize graphs from latent codes. The graph synthesis model is learned jointly with an empirical distribution over the latent codes. Graphs are synthesized using self-attention modules that are trained to identify likely connectivity patterns. Graph-based normalizing flows are used to sample latent codes from the distribution learned by the auto-decoder. The resulting model combines accuracy and scalability. On benchmark datasets of large graphs, the presented model outperforms the state of the art by a factor of 1.5 in mean accuracy and average rank across at least three different graph statistics, with a 2x speedup during inference.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-04T14:23:01Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Sohil Atul Shah</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vladlen Koltun</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.02409v4</id>\n",
      "    <title>On the Promise of the Stochastic Generalized Gauss-Newton Method for Training DNNs</title>\n",
      "    <updated>2020-06-09T08:58:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.02409v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.02409v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Following early work on Hessian-free methods for deep learning, we study a stochastic generalized Gauss-Newton method (SGN) for training DNNs. SGN is a second-order optimization method, with efficient iterations, that we demonstrate to often require substantially fewer iterations than standard SGD to converge. As the name suggests, SGN uses a Gauss-Newton approximation for the Hessian matrix, and, in order to compute an approximate search direction, relies on the conjugate gradient method combined with forward and reverse automatic differentiation. Despite the success of SGD and its first-order variants, and despite Hessian-free methods based on the Gauss-Newton Hessian approximation having been already theoretically proposed as practical methods for training DNNs, we believe that SGN has a lot of undiscovered and yet not fully displayed potential in big mini-batch scenarios. For this setting, we demonstrate that SGN does not only substantially improve over SGD in terms of the number of iterations, but also in terms of runtime. This is made possible by an efficient, easy-to-use and flexible implementation of SGN we propose in the Theano deep learning platform, which, unlike Tensorflow and Pytorch, supports forward automatic differentiation. This enables researchers to further study and improve this promising optimization technique and hopefully reconsider stochastic second-order methods as competitive optimization techniques for training DNNs; we also hope that the promise of SGN may lead to forward automatic differentiation being added to Tensorflow or Pytorch. Our results also show that in big mini-batch scenarios SGN is more robust than SGD with respect to its hyperparameters (we never had to tune its step-size for our benchmarks!), which eases the expensive process of hyperparameter tuning that is instead crucial for the performance of first-order methods.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-03T17:35:54Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Matilde Gargiani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrea Zanelli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Moritz Diehl</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Frank Hutter</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.02460v1</id>\n",
      "    <title>Shallow Neural Hawkes: Non-parametric kernel estimation for Hawkes processes</title>\n",
      "    <updated>2020-06-03T18:15:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.02460v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.02460v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Multi-dimensional Hawkes process (MHP) is a class of self and mutually exciting point processes that find wide range of applications -- from prediction of earthquakes to modelling of order books in high frequency trading. This paper makes two major contributions, we first find an unbiased estimator for the log-likelihood estimator of the Hawkes process to enable efficient use of the stochastic gradient descent method for maximum likelihood estimation. The second contribution is, we propose a specific single hidden layered neural network for the non-parametric estimation of the underlying kernels of the MHP. We evaluate the proposed model on both synthetic and real datasets, and find the method has comparable or better performance than existing estimation methods. The use of shallow neural network ensures that we do not compromise on the interpretability of the Hawkes model, while at the same time have the flexibility to estimate any non-standard Hawkes excitation kernel.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-fin.TR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-03T18:15:38Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Sobin Joseph</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lekhapriya Dheeraj Kashyap</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shashi Jain</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.02516v2</id>\n",
      "    <title>Anomaly Detection with Tensor Networks</title>\n",
      "    <updated>2020-06-16T18:03:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.02516v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.02516v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Originating from condensed matter physics, tensor networks are compact representations of high-dimensional tensors. In this paper, the prowess of tensor networks is demonstrated on the particular task of one-class anomaly detection. We exploit the memory and computational efficiency of tensor networks to learn a linear transformation over a space with dimension exponential in the number of original features. The linearity of our model enables us to ensure a tight fit around training instances by penalizing the model's global tendency to a predict normality via its Frobenius norm---a task that is infeasible for most deep learning models. Our method outperforms deep and classical algorithms on tabular datasets and produces competitive results on image datasets, despite not exploiting the locality of images.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-03T20:41:30Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jinhui Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chase Roberts</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Guifre Vidal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stefan Leichenauer</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.02575v1</id>\n",
      "    <title>Debiased Sinkhorn barycenters</title>\n",
      "    <updated>2020-06-03T23:06:02Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.02575v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.02575v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Entropy regularization in optimal transport (OT) has been the driver of many recent interests for Wasserstein metrics and barycenters in machine learning. It allows to keep the appealing geometrical properties of the unregularized Wasserstein distance while having a significantly lower complexity thanks to Sinkhorn's algorithm. However, entropy brings some inherent smoothing bias, resulting for example in blurred barycenters. This side effect has prompted an increasing temptation in the community to settle for a slower algorithm such as log-domain stabilized Sinkhorn which breaks the parallel structure that can be leveraged on GPUs, or even go back to unregularized OT. Here we show how this bias is tightly linked to the reference measure that defines the entropy regularizer and propose debiased Wasserstein barycenters that preserve the best of both worlds: fast Sinkhorn-like iterations without entropy smoothing. Theoretically, we prove that the entropic OT barycenter of univariate Gaussians is a Gaussian and quantify its variance bias. This result is obtained by extending the differentiability and convexity of entropic OT to sub-Gaussian measures with unbounded supports. Empirically, we illustrate the reduced blurring and the computational advantage on various applications.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-03T23:06:02Z</published>\n",
      "    <arxiv:comment>ICML 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Hicham Janati</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marco Cuturi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexandre Gramfort</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.02579v1</id>\n",
      "    <title>Causality and Batch Reinforcement Learning: Complementary Approaches To Planning In Unknown Domains</title>\n",
      "    <updated>2020-06-03T23:14:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.02579v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.02579v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Reinforcement learning algorithms have had tremendous successes in online learning settings. However, these successes have relied on low-stakes interactions between the algorithmic agent and its environment. In many settings where RL could be of use, such as health care and autonomous driving, the mistakes made by most online RL algorithms during early training come with unacceptable costs. These settings require developing reinforcement learning algorithms that can operate in the so-called batch setting, where the algorithms must learn from set of data that is fixed, finite, and generated from some (possibly unknown) policy. Evaluating policies different from the one that collected the data is called off-policy evaluation, and naturally poses counter-factual questions. In this project we show how off-policy evaluation and the estimation of treatment effects in causal inference are two approaches to the same problem, and compare recent progress in these two areas.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-03T23:14:14Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>James Bannon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Brad Windsor</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wenbo Song</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tao Li</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.02582v1</id>\n",
      "    <title>Local SGD With a Communication Overhead Depending Only on the Number of Workers</title>\n",
      "    <updated>2020-06-03T23:33:02Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.02582v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.02582v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We consider speeding up stochastic gradient descent (SGD) by parallelizing it across multiple workers. We assume the same data set is shared among $n$ workers, who can take SGD steps and coordinate with a central server. Unfortunately, this could require a lot of communication between the workers and the server, which can dramatically reduce the gains from parallelism. The Local SGD method, proposed and analyzed in the earlier literature, suggests machines should make many local steps between such communications. While the initial analysis of Local SGD showed it needs $Ω( \\sqrt{T} )$ communications for $T$ local gradient steps in order for the error to scale proportionately to $1/(nT)$, this has been successively improved in a string of papers, with the state-of-the-art requiring $Ω\\left( n \\left( \\mbox{ polynomial in log } (T) \\right) \\right)$ communications. In this paper, we give a new analysis of Local SGD. A consequence of our analysis is that Local SGD can achieve an error that scales as $1/(nT)$ with only a fixed number of communications independent of $T$: specifically, only $Ω(n)$ communications are required.</summary>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-03T23:33:02Z</published>\n",
      "    <arxiv:primary_category term=\"math.OC\"/>\n",
      "    <author>\n",
      "      <name>Artin Spiridonoff</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alex Olshevsky</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ioannis Ch. Paschalidis</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.02588v1</id>\n",
      "    <title>Meta Dialogue Policy Learning</title>\n",
      "    <updated>2020-06-03T23:53:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.02588v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.02588v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Dialog policy determines the next-step actions for agents and hence is central to a dialogue system. However, when migrated to novel domains with little data, a policy model can fail to adapt due to insufficient interactions with the new environment. We propose Deep Transferable Q-Network (DTQN) to utilize shareable low-level signals between domains, such as dialogue acts and slots. We decompose the state and action representation space into feature subspaces corresponding to these low-level components to facilitate cross-domain knowledge transfer. Furthermore, we embed DTQN in a meta-learning framework and introduce Meta-DTQN with a dual-replay mechanism to enable effective off-policy training and adaptation. In experiments, our model outperforms baseline models in terms of both success rate and dialogue efficiency on the multi-domain dialogue dataset MultiWOZ 2.0.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-03T23:53:06Z</published>\n",
      "    <arxiv:comment>10 pages, 3 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Yumo Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chenguang Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Baolin Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michael Zeng</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03210v1</id>\n",
      "    <title>Sentence Compression as Deletion with Contextual Embeddings</title>\n",
      "    <updated>2020-06-05T02:40:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03210v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03210v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Sentence compression is the task of creating a shorter version of an input sentence while keeping important information. In this paper, we extend the task of compression by deletion with the use of contextual embeddings. Different from prior work usually using non-contextual embeddings (Glove or Word2Vec), we exploit contextual embeddings that enable our model capturing the context of inputs. More precisely, we utilize contextual embeddings stacked by bidirectional Long-short Term Memory and Conditional Random Fields for dealing with sequence labeling. Experimental results on a benchmark Google dataset show that by utilizing contextual embeddings, our model achieves a new state-of-the-art F-score compared to strong methods reported on the leader board.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-05T02:40:46Z</published>\n",
      "    <arxiv:comment>12 pages, 3 figures, accepted by ICCCI 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Minh-Tien Nguyen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bui Cong Minh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dung Tien Le</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Le Thai Linh</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03267v2</id>\n",
      "    <title>Convolutional Neural Networks for Global Human Settlements Mapping from Sentinel-2 Satellite Imagery</title>\n",
      "    <updated>2020-10-29T09:14:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03267v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03267v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Spatially consistent and up-to-date maps of human settlements are crucial for addressing policies related to urbanization and sustainability, especially in the era of an increasingly urbanized world.The availability of open and free Sentinel-2 data of the Copernicus Earth Observation program offers a new opportunity for wall-to-wall mapping of human settlements at a global scale.This paper presents a deep-learning-based framework for a fully automated extraction of built-up areas at a spatial resolution of 10 m from a global composite of Sentinel-2 imagery.A multi-neuro modeling methodology building on a simple Convolution Neural Networks architecture for pixel-wise image classification of built-up areas is developed.The core features of the proposed model are the image patch of size 5 x 5 pixels adequate for describing built-up areas from Sentinel-2 imagery and the lightweight topology with a total number of 1,448,578 trainable parameters and 4 2D convolutional layers and 2 flattened layers.The deployment of the model on the global Sentinel-2 image composite provides the most detailed and complete map reporting about built-up areas for reference year 2018. The validation of the results with an independent reference data-set of building footprints covering 277 sites across the world establishes the reliability of the built-up layer produced by the proposed framework and the model robustness.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-05T07:28:19Z</published>\n",
      "    <arxiv:comment>51 pages including supplementary material, 13 Figures in the main manuscript, under review in Neural Computing and Applications journal</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <arxiv:journal_ref>Neural computing and Applications, 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Christina Corbane</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vasileios Syrris</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Filip Sabo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Panagiotis Politis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michele Melchiorri</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Martino Pesaresi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pierre Soille</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Thomas Kemper</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1007/s00521-020-05449-7</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1007/s00521-020-05449-7\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03318v1</id>\n",
      "    <title>Daydream: Accurately Estimating the Efficacy of Optimizations for DNN Training</title>\n",
      "    <updated>2020-06-05T09:08:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03318v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03318v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Modern deep neural network (DNN) training jobs use complex and heterogeneous software/hardware stacks. The efficacy of software-level optimizations can vary significantly when used in different deployment configurations. It is onerous and error-prone for ML practitioners and system developers to implement each optimization separately, and determine which ones will improve performance in their own configurations. Unfortunately, existing profiling tools do not aim to answer predictive questions such as \"How will optimization X affect the performance of my model?\". We address this critical limitation, and proposes a new profiling tool, Daydream, to help programmers efficiently explore the efficacy of DNN optimizations. Daydream models DNN execution with a fine-grained dependency graph based on low-level traces collected by CUPTI, and predicts runtime by simulating execution based on the dependency graph. Daydream maps the low-level traces using DNN domain-specific knowledge, and introduces a set of graph-transformation primitives that can easily model a wide variety of optimizations. We show that Daydream is able to model most mainstream DNN optimization techniques, and accurately predict the efficacy of optimizations that will result in significant performance improvements.</summary>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.PF\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-05T09:08:16Z</published>\n",
      "    <arxiv:primary_category term=\"cs.DC\"/>\n",
      "    <author>\n",
      "      <name>Hongyu Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amar Phanishayee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gennady Pekhimenko</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03350v1</id>\n",
      "    <title>Concurrent Decentralized Channel Allocation and Access Point Selection using Multi-Armed Bandits in multi BSS WLANs</title>\n",
      "    <updated>2020-06-05T10:20:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03350v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03350v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Enterprise Wireless Local Area Networks (WLANs) consist of multiple Access Points (APs) covering a given area. Finding a suitable network configuration able to maximize the performance of enterprise WLANs is a challenging task given the complex dependencies between APs and stations. Recently, in wireless networking, the use of reinforcement learning techniques has emerged as an effective solution to efficiently explore the impact of different network configurations in the system performance, identifying those that provide better performance. In this paper, we study if Multi-Armed Bandits (MABs) are able to offer a feasible solution to the decentralized channel allocation and AP selection problems in Enterprise WLAN scenarios. To do so, we empower APs and stations with agents that, by means of implementing the Thompson sampling algorithm, explore and learn which is the best channel to use, and which is the best AP to associate, respectively. Our evaluation is performed over randomly generated scenarios, which enclose different network topologies and traffic loads. The presented results show that the proposed adaptive framework using MABs outperform the static approach (i.e., using always the initial default configuration, usually random) regardless of the network density and the traffic requirements. Moreover, we show that the use of the proposed framework reduces the performance variability between different scenarios. Results also show that we achieve the same performance (or better) than static strategies with less APs for the same number of stations. Finally, special attention is placed on how the agents interact. Even if the agents operate in a completely independent manner, their decisions have interrelated effects, as they take actions over the same set of channel resources.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-05T10:20:40Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Álvaro López-Raventós</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Boris Bellalta</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1016/j.comnet.2020.107381</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1016/j.comnet.2020.107381\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.02655v1</id>\n",
      "    <title>Neuroevolutionary Transfer Learning of Deep Recurrent Neural Networks through Network-Aware Adaptation</title>\n",
      "    <updated>2020-06-04T06:07:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.02655v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.02655v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Transfer learning entails taking an artificial neural network (ANN) that is trained on a source dataset and adapting it to a new target dataset. While this has been shown to be quite powerful, its use has generally been restricted by architectural constraints. Previously, in order to reuse and adapt an ANN's internal weights and structure, the underlying topology of the ANN being transferred across tasks must remain mostly the same while a new output layer is attached, discarding the old output layer's weights. This work introduces network-aware adaptive structure transfer learning (N-ASTL), an advancement over prior efforts to remove this restriction. N-ASTL utilizes statistical information related to the source network's topology and weight distribution in order to inform how new input and output neurons are to be integrated into the existing structure. Results show improvements over prior state-of-the-art, including the ability to transfer in challenging real-world datasets not previously possible and improved generalization over RNNs trained without transfer.</summary>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-04T06:07:30Z</published>\n",
      "    <arxiv:primary_category term=\"cs.NE\"/>\n",
      "    <author>\n",
      "      <name>AbdElRahman ElSaid</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joshua Karns</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexander Ororbia</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniel Krutz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zimeng Lyu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Travis Desell</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.02797v1</id>\n",
      "    <title>Overcoming Overfitting and Large Weight Update Problem in Linear Rectifiers: Thresholded Exponential Rectified Linear Units</title>\n",
      "    <updated>2020-06-04T11:55:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.02797v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.02797v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In past few years, linear rectified unit activation functions have shown its significance in the neural networks, surpassing the performance of sigmoid activations. RELU (Nair &amp; Hinton, 2010), ELU (Clevert et al., 2015), PRELU (He et al., 2015), LRELU (Maas et al., 2013), SRELU (Jin et al., 2016), ThresholdedRELU, all these linear rectified activation functions have its own significance over others in some aspect. Most of the time these activation functions suffer from bias shift problem due to non-zero output mean, and high weight update problem in deep complex networks due to unit gradient, which results in slower training, and high variance in model prediction respectively. In this paper, we propose, \"Thresholded exponential rectified linear unit\" (TERELU) activation function that works better in alleviating in overfitting: large weight update problem. Along with alleviating overfitting problem, this method also gives good amount of non-linearity as compared to other linear rectifiers. We will show better performance on the various datasets using neural networks, considering TERELU activation method compared to other activations.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-04T11:55:47Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Vijay Pandey</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03669v2</id>\n",
      "    <title>An Overview of Neural Network Compression</title>\n",
      "    <updated>2020-08-01T16:55:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03669v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03669v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Overparameterized networks trained to convergence have shown impressive performance in domains such as computer vision and natural language processing. Pushing state of the art on salient tasks within these domains corresponds to these models becoming larger and more difficult for machine learning practitioners to use given the increasing memory and storage requirements, not to mention the larger carbon footprint. Thus, in recent years there has been a resurgence in model compression techniques, particularly for deep convolutional neural networks and self-attention based networks such as the Transformer.\n",
      "  Hence, this paper provides a timely overview of both old and current compression techniques for deep neural networks, including pruning, quantization, tensor decomposition, knowledge distillation and combinations thereof.\n",
      "  We assume a basic familiarity with deep learning architectures\\footnote{For an introduction to deep learning, see ~\\citet{goodfellow2016deep}}, namely, Recurrent Neural Networks~\\citep[(RNNs)][]{rumelhart1985learning,hochreiter1997long}, Convolutional Neural Networks~\\citep{fukushima1980neocognitron}~\\footnote{For an up to date overview see~\\citet{khan2019survey}} and Self-Attention based networks~\\citep{vaswani2017attention}\\footnote{For a general overview of self-attention networks, see ~\\citet{chaudhari2019attentive}.},\\footnote{For more detail and their use in natural language processing, see~\\citet{hu2019introductory}}. Most of the papers discussed are proposed in the context of at least one of these DNN architectures.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-05T20:28:56Z</published>\n",
      "    <arxiv:comment>53 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>James O' Neill</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03677v4</id>\n",
      "    <title>Visual Transformers: Token-based Image Representation and Processing for Computer Vision</title>\n",
      "    <updated>2020-11-20T00:10:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03677v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03677v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Computer vision has achieved remarkable success by (a) representing images as uniformly-arranged pixel arrays and (b) convolving highly-localized features. However, convolutions treat all image pixels equally regardless of importance; explicitly model all concepts across all images, regardless of content; and struggle to relate spatially-distant concepts. In this work, we challenge this paradigm by (a) representing images as semantic visual tokens and (b) running transformers to densely model token relationships. Critically, our Visual Transformer operates in a semantic token space, judiciously attending to different image parts based on context. This is in sharp contrast to pixel-space transformers that require orders-of-magnitude more compute. Using an advanced training recipe, our VTs significantly outperform their convolutional counterparts, raising ResNet accuracy on ImageNet top-1 by 4.6 to 7 points while using fewer FLOPs and parameters. For semantic segmentation on LIP and COCO-stuff, VT-based feature pyramid networks (FPN) achieve 0.35 points higher mIoU while reducing the FPN module's FLOPs by 6.5x.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-05T20:49:49Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Bichen Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chenfeng Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaoliang Dai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alvin Wan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peizhao Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhicheng Yan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Masayoshi Tomizuka</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joseph Gonzalez</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kurt Keutzer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peter Vajda</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03686v1</id>\n",
      "    <title>Adversarial Robustness of Deep Convolutional Candlestick Learner</title>\n",
      "    <updated>2020-05-29T02:58:04Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03686v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03686v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep learning (DL) has been applied extensively in a wide range of fields. However, it has been shown that DL models are susceptible to a certain kinds of perturbations called \\emph{adversarial attacks}. To fully unlock the power of DL in critical fields such as financial trading, it is necessary to address such issues. In this paper, we present a method of constructing perturbed examples and use these examples to boost the robustness of the model. Our algorithm increases the stability of DL models for candlestick classification with respect to perturbations in the input data.</summary>\n",
      "    <category term=\"q-fin.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-05-29T02:58:04Z</published>\n",
      "    <arxiv:comment>arXiv admin note: text overlap with arXiv:2005.06731</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"q-fin.ST\"/>\n",
      "    <author>\n",
      "      <name>Jun-Hao Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Samuel Yen-Chi Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yun-Cheng Tsai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chih-Shiang Shur</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03750v2</id>\n",
      "    <title>Learning to Solve Combinatorial Optimization Problems on Real-World Graphs in Linear Time</title>\n",
      "    <updated>2020-06-12T00:56:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03750v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03750v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Combinatorial optimization algorithms for graph problems are usually designed afresh for each new problem with careful attention by an expert to the problem structure. In this work, we develop a new framework to solve any combinatorial optimization problem over graphs that can be formulated as a single player game defined by states, actions, and rewards, including minimum spanning tree, shortest paths, traveling salesman problem, and vehicle routing problem, without expert knowledge. Our method trains a graph neural network using reinforcement learning on an unlabeled training set of graphs. The trained network then outputs approximate solutions to new graph instances in linear running time. In contrast, previous approximation algorithms or heuristics tailored to NP-hard problems on graphs generally have at least quadratic running time. We demonstrate the applicability of our approach on both polynomial and NP-hard problems with optimality gaps close to 1, and show that our method is able to generalize well: (i) from training on small graphs to testing on large graphs; (ii) from training on random graphs of one type to testing on random graphs of another type; and (iii) from training on random graphs to running on real world graphs.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-06T01:35:45Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Iddo Drori</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anant Kharkar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>William R. Sickinger</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Brandon Kates</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qiang Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Suwen Ge</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eden Dolev</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Brenda Dietrich</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David P. Williamson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Madeleine Udell</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03873v1</id>\n",
      "    <title>Unique properties of adversarially trained linear classifiers on Gaussian data</title>\n",
      "    <updated>2020-06-06T14:06:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03873v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03873v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Machine learning models are vulnerable to adversarial perturbations, that when added to an input, can cause high confidence misclassifications. The adversarial learning research community has made remarkable progress in the understanding of the root causes of adversarial perturbations. However, most problems that one may consider important to solve for the deployment of machine learning in safety critical tasks involve high dimensional complex manifolds that are difficult to characterize and study. It is common to develop adversarially robust learning theory on simple problems, in the hope that insights will transfer to `real world datasets'. In this work, we discuss a setting where this approach fails. In particular, we show with a linear classifier, it is always possible to solve a binary classification problem on Gaussian data under arbitrary levels of adversarial corruption during training, and that this property is not observed with non-linear classifiers on the CIFAR-10 dataset.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-06T14:06:38Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jamie Hayes</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03937v3</id>\n",
      "    <title>Memory-Efficient Learning of Stable Linear Dynamical Systems for Prediction and Control</title>\n",
      "    <updated>2020-10-22T19:38:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03937v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03937v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Learning a stable Linear Dynamical System (LDS) from data involves creating models that both minimize reconstruction error and enforce stability of the learned representation. We propose a novel algorithm for learning stable LDSs. Using a recent characterization of stable matrices, we present an optimization method that ensures stability at every step and iteratively improves the reconstruction error using gradient directions derived in this paper. When applied to LDSs with inputs, our approach---in contrast to current methods for learning stable LDSs---updates both the state and control matrices, expanding the solution space and allowing for models with lower reconstruction error. We apply our algorithm in simulations and experiments to a variety of problems, including learning dynamic textures from image sequences and controlling a robotic manipulator. Compared to existing approaches, our proposed method achieves an orders-of-magnitude improvement in reconstruction error and superior results in terms of control performance. In addition, it is provably more memory-efficient, with an O(n^2) space complexity compared to O(n^4) of competing alternatives, thus scaling to higher-dimensional systems when the other methods fail.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-06T18:44:08Z</published>\n",
      "    <arxiv:comment>Neural Information Processing Systems (NeurIPS) 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Giorgos Mamakoukas</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Orest Xherija</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>T. D. Murphey</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03960v1</id>\n",
      "    <title>Frank-Wolfe optimization for deep networks</title>\n",
      "    <updated>2020-06-06T20:20:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03960v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03960v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep neural networks is today one of the most popular choices in classification, regression and function approximation. However, the training of such deep networks is far from trivial as there are often millions of parameters to tune. Typically, one use some optimization method that hopefully converges towards some minimum. The most popular and successful methods are based on gradient descent. In this paper, another optimization method, Frank-Wolfe optimization, is applied to a small deep network and compared to gradient descent. Although the optimization does converge, it does so slowly and not close to the speed of gradient descent. Further, in a stochastic setting, the optimization becomes very unstable and does not seem to converge unless one uses a line search approach.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-06T20:20:18Z</published>\n",
      "    <arxiv:comment>3 pages, 4 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jakob Stigenberg</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2006.03972v1</id>\n",
      "    <title>Regularization of Inverse Problems by Neural Networks</title>\n",
      "    <updated>2020-06-06T20:49:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2006.03972v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2006.03972v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Inverse problems arise in a variety of imaging applications including computed tomography, non-destructive testing, and remote sensing. The characteristic features of inverse problems are the non-uniqueness and instability of their solutions. Therefore, any reasonable solution method requires the use of regularization tools that select specific solutions and at the same time stabilize the inversion process. Recently, data-driven methods using deep learning techniques and neural networks demonstrated to significantly outperform classical solution methods for inverse problems. In this chapter, we give an overview of inverse problems and demonstrate the necessity of regularization concepts for their solution. We show that neural networks can be used for the data-driven solution of inverse problems and review existing deep learning methods for inverse problems. In particular, we view these deep learning methods from the perspective of regularization theory, the mathematical foundation of stable solution methods for inverse problems. This chapter is more than just a review as many of the presented theoretical results extend existing ones.</summary>\n",
      "    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-06-06T20:49:12Z</published>\n",
      "    <arxiv:primary_category term=\"math.NA\"/>\n",
      "    <author>\n",
      "      <name>Markus Haltmeier</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Linh V. Nguyen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.00638v1</id>\n",
      "    <title>Tabular GANs for uneven distribution</title>\n",
      "    <updated>2020-10-01T18:39:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.00638v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.00638v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>GANs are well known for success in the realistic image generation. However, they can be applied in tabular data generation as well. We will review and examine some recent papers about tabular GANs in action. We will generate data to make train distribution bring closer to the test. Then compare model performance trained on the initial train dataset, with trained on the train with GAN generated data, also we train the model by sampling train by adversarial training. We show that using GAN might be an option in case of uneven data distribution between train and test data.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-01T18:39:32Z</published>\n",
      "    <arxiv:comment>11 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Insaf Ashrapov</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.00684v2</id>\n",
      "    <title>Towards Scalable Bayesian Learning of Causal DAGs</title>\n",
      "    <updated>2020-11-18T13:48:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.00684v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.00684v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We give methods for Bayesian inference of directed acyclic graphs, DAGs, and the induced causal effects from passively observed complete data. Our methods build on a recent Markov chain Monte Carlo scheme for learning Bayesian networks, which enables efficient approximate sampling from the graph posterior, provided that each node is assigned a small number $K$ of candidate parents. We present algorithmic techniques to significantly reduce the space and time requirements, which make the use of substantially larger values of $K$ feasible. Furthermore, we investigate the problem of selecting the candidate parents per node so as to maximize the covered posterior mass. Finally, we combine our sampling method with a novel Bayesian approach for estimating causal effects in linear Gaussian DAG models. Numerical experiments demonstrate the performance of our methods in detecting ancestor-descendant relations, and in causal effect estimation our Bayesian method is shown to outperform previous approaches.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-30T08:46:46Z</published>\n",
      "    <arxiv:comment>We have updated the manuscript based on reviewer feedback</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jussi Viinikka</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Antti Hyttinen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Johan Pensar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mikko Koivisto</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02024v1</id>\n",
      "    <title>Deep Incomplete Multi-View Multiple Clusterings</title>\n",
      "    <updated>2020-10-02T08:01:24Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02024v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02024v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Multi-view clustering aims at exploiting information from multiple heterogeneous views to promote clustering. Most previous works search for only one optimal clustering based on the predefined clustering criterion, but devising such a criterion that captures what users need is difficult. Due to the multiplicity of multi-view data, we can have meaningful alternative clusterings. In addition, the incomplete multi-view data problem is ubiquitous in real world but has not been studied for multiple clusterings.  To address these issues, we introduce a deep incomplete multi-view multiple clusterings (DiMVMC) framework, which achieves the completion of data view and multiple shared representations simultaneously by optimizing multiple groups of decoder deep networks. In addition, it minimizes a redundancy term to simultaneously %uses Hilbert-Schmidt Independence Criterion (HSIC) to control the diversity among these representations and among parameters of different networks. Next, it generates an individual clustering from each of these shared representations. Experiments on benchmark datasets confirm that DiMVMC outperforms the state-of-the-art competitors in generating multiple clusterings with high diversity and quality.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-02T08:01:24Z</published>\n",
      "    <arxiv:comment>10 pages, ICDM2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Shaowei Wei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jun Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Guoxian Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Carlotta Domeniconi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiangliang Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02038v1</id>\n",
      "    <title>A Simple Framework for Uncertainty in Contrastive Learning</title>\n",
      "    <updated>2020-10-05T14:17:42Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02038v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02038v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Contrastive approaches to representation learning have recently shown great promise. In contrast to generative approaches, these contrastive models learn a deterministic encoder with no notion of uncertainty or confidence. In this paper, we introduce a simple approach based on \"contrasting distributions\" that learns to assign uncertainty for pretrained contrastive representations. In particular, we train a deep network from a representation to a distribution in representation space, whose variance can be used as a measure of confidence. In our experiments, we show that this deep uncertainty model can be used (1) to visually interpret model behavior, (2) to detect new noise in the input to deployed models, (3) to detect anomalies, where we outperform 10 baseline methods across 11 tasks with improvements of up to 14% absolute, and (4) to classify out-of-distribution examples where our fully unsupervised model is competitive with supervised methods.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T14:17:42Z</published>\n",
      "    <arxiv:comment>8 pages main text</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Mike Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Noah Goodman</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02167v1</id>\n",
      "    <title>Latent neural source recovery via transcoding of simultaneous EEG-fMRI</title>\n",
      "    <updated>2020-10-05T17:17:29Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02167v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02167v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Simultaneous EEG-fMRI is a multi-modal neuroimaging technique that provides complementary spatial and temporal resolution for inferring a latent source space of neural activity. In this paper we address this inference problem within the framework of transcoding -- mapping from a specific encoding (modality) to a decoding (the latent source space) and then encoding the latent source space to the other modality. Specifically, we develop a symmetric method consisting of a cyclic convolutional transcoder that transcodes EEG to fMRI and vice versa. Without any prior knowledge of either the hemodynamic response function or lead field matrix, the method exploits the temporal and spatial relationships between the modalities and latent source spaces to learn these mappings. We show, for real EEG-fMRI data, how well the modalities can be transcoded from one to another as well as the source spaces that are recovered, all on unseen data. In addition to enabling a new way to symmetrically infer a latent source space, the method can also be seen as low-cost computational neuroimaging -- i.e. generating an 'expensive' fMRI BOLD image from 'low cost' EEG data.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T17:17:29Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xueqing Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Linbi Hong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paul Sajda</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02183v2</id>\n",
      "    <title>Estimating conditional density of missing values using deep Gaussian mixture model</title>\n",
      "    <updated>2020-10-06T08:18:29Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02183v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02183v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We consider the problem of estimating the conditional probability distribution of missing values given the observed ones. We propose an approach, which combines the flexibility of deep neural networks with the simplicity of Gaussian mixture models (GMMs). Given an incomplete data point, our neural network returns the parameters of Gaussian distribution (in the form of Factor Analyzers model) representing the corresponding conditional density. We experimentally verify that our model provides better log-likelihood than conditional GMM trained in a typical way. Moreover, imputation obtained by replacing missing values using the mean vector of our model looks visually plausible.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T17:39:25Z</published>\n",
      "    <arxiv:comment>A preliminary version of this paper appeared as an extended abstract at the ICML 2020 Workshop on The Art of Learning with Missing Values</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Marcin Przewięźlikowski</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marek Śmieja</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Łukasz Struski</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1007/978-3-030-63836-8_19</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1007/978-3-030-63836-8_19\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06266v1</id>\n",
      "    <title>Model-Based Reinforcement Learning for Type 1Diabetes Blood Glucose Control</title>\n",
      "    <updated>2020-10-13T10:17:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06266v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06266v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper we investigate the use of model-based reinforcement learning to assist people with Type 1 Diabetes with insulin dose decisions. The proposed architecture consists of multiple Echo State Networks to predict blood glucose levels combined with Model Predictive Controller for planning. Echo State Network is a version of recurrent neural networks which allows us to learn long term dependencies in the input of time series data in an online manner. Additionally, we address the quantification of uncertainty for a more robust control. Here, we used ensembles of Echo State Networks to capture model (epistemic) uncertainty. We evaluated the approach with the FDA-approved UVa/Padova Type 1 Diabetes simulator and compared the results against baseline algorithms such as Basal-Bolus controller and Deep Q-learning. The results suggest that the model-based reinforcement learning algorithm can perform equally or better than the baseline algorithms for the majority of virtual Type 1 Diabetes person profiles tested.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T10:17:30Z</published>\n",
      "    <arxiv:comment>Presented at ECAI 2020 SP4HC Workshop</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Taku Yamagata</name>\n",
      "      <arxiv:affiliation>University of Bristol</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aisling O'Kane</name>\n",
      "      <arxiv:affiliation>University of Bristol</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amid Ayobi</name>\n",
      "      <arxiv:affiliation>University of Bristol</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dmitri Katz</name>\n",
      "      <arxiv:affiliation>The Open University</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Katarzyna Stawarz</name>\n",
      "      <arxiv:affiliation>Cardiff University</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paul Marshall</name>\n",
      "      <arxiv:affiliation>University of Bristol</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peter Flach</name>\n",
      "      <arxiv:affiliation>University of Bristol</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Raúl Santos-Rodríguez</name>\n",
      "      <arxiv:affiliation>University of Bristol</arxiv:affiliation>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06307v1</id>\n",
      "    <title>How important are faces for person re-identification?</title>\n",
      "    <updated>2020-10-13T11:47:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06307v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06307v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper investigates the dependence of existing state-of-the-art person re-identification models on the presence and visibility of human faces. We apply a face detection and blurring algorithm to create anonymized versions of several popular person re-identification datasets including Market1501, DukeMTMC-reID, CUHK03, Viper, and Airport. Using a cross-section of existing state-of-the-art models that range in accuracy and computational efficiency, we evaluate the effect of this anonymization on re-identification performance using standard metrics. Perhaps surprisingly, the effect on mAP is very small, and accuracy is recovered by simply training on the anonymized versions of the data rather than the original data. These findings are consistent across multiple models and datasets. These results indicate that datasets can be safely anonymized by blurring faces without significantly impacting the performance of person reidentification systems, and may allow for the release of new richer re-identification datasets where previously there were privacy or data protection concerns.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T11:47:16Z</published>\n",
      "    <arxiv:comment>25th International Conference on Pattern Recognition (ICPR2020), Milan, Italy, 10-15 January 2021</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Julia Dietlmeier</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joseph Antony</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kevin McGuinness</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Noel E. O'Connor</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06325v2</id>\n",
      "    <title>Modeling the Music Genre Perception across Language-Bound Cultures</title>\n",
      "    <updated>2020-11-16T11:43:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06325v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06325v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The music genre perception expressed through human annotations of artists or albums varies significantly across language-bound cultures. These variations cannot be modeled as mere translations since we also need to account for cultural differences in the music genre perception. In this work, we study the feasibility of obtaining relevant cross-lingual, culture-specific music genre annotations based only on language-specific semantic representations, namely distributed concept embeddings and ontologies. Our study, focused on six languages, shows that unsupervised cross-lingual music genre annotation is feasible with high accuracy, especially when combining both types of representations. This approach of studying music genres is the most extensive to date and has many implications in musicology and music information retrieval. Besides, we introduce a new, domain-dependent cross-lingual corpus to benchmark state of the art multilingual pre-trained embedding models.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T12:20:32Z</published>\n",
      "    <arxiv:comment>2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Elena V. Epure</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Guillaume Salha</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Manuel Moussallam</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Romain Hennequin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06045v1</id>\n",
      "    <title>Spectral Synthesis for Satellite-to-Satellite Translation</title>\n",
      "    <updated>2020-10-12T21:36:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06045v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06045v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Earth observing satellites carrying multi-spectral sensors are widely used to monitor the physical and biological states of the atmosphere, land, and oceans. These satellites have different vantage points above the earth and different spectral imaging bands resulting in inconsistent imagery from one to another. This presents challenges in building downstream applications. What if we could generate synthetic bands for existing satellites from the union of all domains? We tackle the problem of generating synthetic spectral imagery for multispectral sensors as an unsupervised image-to-image translation problem with partial labels and introduce a novel shared spectral reconstruction loss. Simulated experiments performed by dropping one or more spectral bands show that cross-domain reconstruction outperforms measurements obtained from a second vantage point. On a downstream cloud detection task, we show that generating synthetic bands with our model improves segmentation performance beyond our baseline. Our proposed approach enables synchronization of multispectral data and provides a basis for more homogeneous remote sensing datasets.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-12T21:36:39Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Thomas Vandal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniel McDuff</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Weile Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrew Michaelis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ramakrishna Nemani</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06096v2</id>\n",
      "    <title>Attn-HybridNet: Improving Discriminability of Hybrid Features with Attention Fusion</title>\n",
      "    <updated>2020-10-14T12:44:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06096v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06096v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The principal component analysis network (PCANet) is an unsupervised parsimonious deep network, utilizing principal components as filters in its convolution layers. Albeit powerful, the PCANet consists of basic operations such as principal components and spatial pooling, which suffers from two fundamental problems. First, the principal components obtain information by transforming it to column vectors (which we call the amalgamated view), which incurs the loss of the spatial information in the data. Second, the generalized spatial pooling utilized in the PCANet induces feature redundancy and also fails to accommodate spatial statistics of natural images. In this research, we first propose a tensor-factorization based deep network called the Tensor Factorization Network (TFNet). The TFNet extracts features from the spatial structure of the data (which we call the minutiae view). We then show that the information obtained by the PCANet and the TFNet are distinctive and non-trivial but individually insufficient. This phenomenon necessitates the development of proposed HybridNet, which integrates the information discovery with the two views of the data. To enhance the discriminability of hybrid features, we propose Attn-HybridNet, which alleviates the feature redundancy by performing attention-based feature fusion. The significance of our proposed Attn-HybridNet is demonstrated on multiple real-world datasets where the features obtained with Attn-HybridNet achieves better classification performance over other popular baseline methods, demonstrating the effectiveness of the proposed technique.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T00:52:57Z</published>\n",
      "    <arxiv:comment>Under minor review at IEEE Transactions on Cybernetics</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Sunny Verma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chen Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liming Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wei Liu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06142v1</id>\n",
      "    <title>Hindsight Experience Replay with Kronecker Product Approximate Curvature</title>\n",
      "    <updated>2020-10-09T20:25:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06142v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06142v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Hindsight Experience Replay (HER) is one of the efficient algorithm to solve Reinforcement Learning tasks related to sparse rewarded environments.But due to its reduced sample efficiency and slower convergence HER fails to perform effectively. Natural gradients solves these challenges by converging the model parameters better. It avoids taking bad actions that collapse the training performance. However updating parameters in neural networks requires expensive computation and thus increase in training time. Our proposed method solves the above mentioned challenges with better sample efficiency and faster convergence with increased success rate. A common failure mode for DDPG is that the learned Q-function begins to dramatically overestimate Q-values, which then leads to the policy breaking, because it exploits the errors in the Q-function. We solve this issue by including Twin Delayed Deep Deterministic Policy Gradients(TD3) in HER. TD3 learns two Q-functions instead of one and it adds noise tothe target action, to make it harder for the policy to exploit Q-function errors. The experiments are done with the help of OpenAis Mujoco environments. Results on these environments show that our algorithm (TDHER+KFAC) performs better inmost of the scenarios</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-09T20:25:14Z</published>\n",
      "    <arxiv:comment>arXiv admin note: text overlap with arXiv:1708.05144 by other authors</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Dhuruva Priyan G M</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abhik Singla</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shalabh Bhatnagar</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06177v1</id>\n",
      "    <title>COVID-19 Imaging Data Privacy by Federated Learning Design: A Theoretical Framework</title>\n",
      "    <updated>2020-10-13T04:34:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06177v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06177v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>To address COVID-19 healthcare challenges, we need frequent sharing of health data, knowledge and resources at a global scale. However, in this digital age, data privacy is a big concern that requires the secure embedding of privacy assurance into the design of all technological solutions that use health data. In this paper, we introduce differential privacy by design (dPbD) framework and discuss its embedding into the federated machine learning system. To limit the scope of our paper, we focus on the problem scenario of COVID-19 imaging data privacy for disease diagnosis by computer vision and deep learning approaches. We discuss the evaluation of the proposed design of federated machine learning systems and discuss how differential privacy by design (dPbD) framework can enhance data privacy in federated learning systems with scalability and robustness. We argue that scalable differentially private federated learning design is a promising solution for building a secure, private and collaborative machine learning model such as required to combat COVID19 challenge.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T04:34:30Z</published>\n",
      "    <arxiv:comment>2 images, 0 Table,8 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Anwaar Ulhaq</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Oliver Burmeister</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06212v1</id>\n",
      "    <title>S3ML: A Secure Serving System for Machine Learning Inference</title>\n",
      "    <updated>2020-10-13T07:41:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06212v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06212v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present S3ML, a secure serving system for machine learning inference in this paper. S3ML runs machine learning models in Intel SGX enclaves to protect users' privacy. S3ML designs a secure key management service to construct flexible privacy-preserving server clusters and proposes novel SGX-aware load balancing and scaling methods to satisfy users' Service-Level Objectives. We have implemented S3ML based on Kubernetes as a low-overhead, high-available, and scalable system. We demonstrate the system performance and effectiveness of S3ML through extensive experiments on a series of widely-used models.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T07:41:13Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Junming Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chaofan Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aihui Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bingzhe Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xibin Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xingyu Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiangqun Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lei Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Donggang Cao</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06233v1</id>\n",
      "    <title>Artist-driven layering and user's behaviour impact on recommendations in a playlist continuation scenario</title>\n",
      "    <updated>2020-10-13T08:47:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06233v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06233v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper we provide an overview of the approach we used as team Creamy Fireflies for the ACM RecSys Challenge 2018. The competition, organized by Spotify, focuses on the problem of playlist continuation, that is suggesting which tracks the user may add to an existing playlist. The challenge addresses this issue in many use cases, from playlist cold start to playlists already composed by up to a hundred tracks. Our team proposes a solution based on a few well known models both content based and collaborative, whose predictions are aggregated via an ensembling step. Moreover by analyzing the underlying structure of the data, we propose a series of boosts to be applied on top of the final predictions and improve the recommendation quality. The proposed approach leverages well-known algorithms and is able to offer a high recommendation quality while requiring a limited amount of computational resources.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T08:47:08Z</published>\n",
      "    <arxiv:comment>Source code available here: https://github.com/MaurizioFD/spotify-recsys-challenge</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <arxiv:journal_ref>Proceedings of the ACM Recommender Systems Challenge 2018 (RecSys Challenge '18)</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Sebastiano Antenucci</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Simone Boglio</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Emanuele Chioso</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ervin Dervishaj</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shuwen Kang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tommaso Scarlatti</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maurizio Ferrari Dacrema</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3267471.3267475</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3267471.3267475\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04862v2</id>\n",
      "    <title>Remarks on Optimal Scores for Speaker Recognition</title>\n",
      "    <updated>2020-10-30T03:33:49Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04862v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04862v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this article, we first establish the theory of optimal scores for speaker recognition. Our analysis shows that the minimum Bayes risk (MBR) decisions for both the speaker identification and speaker verification tasks can be based on a normalized likelihood (NL). When the underlying generative model is a linear Gaussian, the NL score is mathematically equivalent to the PLDA likelihood ratio, and the empirical scores based on cosine distance and Euclidean distance can be seen as approximations of this linear Gaussian NL score under some conditions. We discuss a number of properties of the NL score and perform a simple simulation experiment to demonstrate the properties of the NL score.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-10T01:28:24Z</published>\n",
      "    <arxiv:comment>17 pages, 8 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Dong Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.04947v1</id>\n",
      "    <title>Double Forward Propagation for Memorized Batch Normalization</title>\n",
      "    <updated>2020-10-10T08:48:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.04947v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.04947v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Batch Normalization (BN) has been a standard component in designing deep neural networks (DNNs). Although the standard BN can significantly accelerate the training of DNNs and improve the generalization performance, it has several underlying limitations which may hamper the performance in both training and inference. In the training stage, BN relies on estimating the mean and variance of data using a single minibatch. Consequently, BN can be unstable when the batch size is very small or the data is poorly sampled. In the inference stage, BN often uses the so called moving mean and moving variance instead of batch statistics, i.e., the training and inference rules in BN are not consistent. Regarding these issues, we propose a memorized batch normalization (MBN), which considers multiple recent batches to obtain more accurate and robust statistics. Note that after the SGD update for each batch, the model parameters will change, and the features will change accordingly, leading to the Distribution Shift before and after the update for the considered batch. To alleviate this issue, we present a simple Double-Forward scheme in MBN which can further improve the performance. Compared to related methods, the proposed MBN exhibits consistent behaviors in both training and inference. Empirical results show that the MBN based models trained with the Double-Forward scheme greatly reduce the sensitivity of data and significantly improve the generalization performance.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-10T08:48:41Z</published>\n",
      "    <arxiv:comment>AAAI2018, 8 pages, 3 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yong Guo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qingyao Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chaorui Deng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jian Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mingkui Tan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06457v1</id>\n",
      "    <title>CrypTFlow2: Practical 2-Party Secure Inference</title>\n",
      "    <updated>2020-10-13T15:12:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06457v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06457v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present CrypTFlow2, a cryptographic framework for secure inference over realistic Deep Neural Networks (DNNs) using secure 2-party computation. CrypTFlow2 protocols are both correct -- i.e., their outputs are bitwise equivalent to the cleartext execution -- and efficient -- they outperform the state-of-the-art protocols in both latency and scale. At the core of CrypTFlow2, we have new 2PC protocols for secure comparison and division, designed carefully to balance round and communication complexity for secure inference tasks. Using CrypTFlow2, we present the first secure inference over ImageNet-scale DNNs like ResNet50 and DenseNet121. These DNNs are at least an order of magnitude larger than those considered in the prior work of 2-party DNN inference. Even on the benchmarks considered by prior work, CrypTFlow2 requires an order of magnitude less communication and 20x-30x less time than the state-of-the-art.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T15:12:28Z</published>\n",
      "    <arxiv:comment>To appear at ACM CCS 2020. Code available at: https://github.com/mpc-msri/EzPC</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Deevashwer Rathee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mayank Rathee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nishant Kumar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nishanth Chandran</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Divya Gupta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aseem Rastogi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rahul Sharma</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3372297.3417274</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3372297.3417274\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06487v2</id>\n",
      "    <title>Simultaneously forecasting global geomagnetic activity using Recurrent Networks</title>\n",
      "    <updated>2020-11-20T19:36:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06487v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06487v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Many systems used by society are extremely vulnerable to space weather events such as solar flares and geomagnetic storms which could potentially cause catastrophic damage. In recent years, many works have emerged to provide early warning to such systems by forecasting these events through some proxy, but these approaches have largely focused on a specific phenomenon. We present a sequence-to-sequence learning approach to the problem of forecasting global space weather conditions at an hourly resolution. This approach improves upon other work in this field by simultaneously forecasting several key proxies for geomagnetic activity up to 6 hours in advance. We demonstrate an improvement over the best currently known predictor of geomagnetic storms, and an improvement over a persistence baseline several hours in advance.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T15:48:20Z</published>\n",
      "    <arxiv:comment>7 pages, 2 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Charles Topliff</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Morris Cohen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>William Bristow</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06491v1</id>\n",
      "    <title>Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning</title>\n",
      "    <updated>2020-10-13T15:51:24Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06491v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06491v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Long-horizon planning in realistic environments requires the ability to reason over sequential tasks in high-dimensional state spaces with complex dynamics. Classical motion planning algorithms, such as rapidly-exploring random trees, are capable of efficiently exploring large state spaces and computing long-horizon, sequential plans. However, these algorithms are generally challenged with complex, stochastic, and high-dimensional state spaces as well as in the presence of narrow passages, which naturally emerge in tasks that interact with the environment. Machine learning offers a promising solution for its ability to learn general policies that can handle complex interactions and high-dimensional observations. However, these policies are generally limited in horizon length. Our approach, Broadly-Exploring, Local-policy Trees (BELT), merges these two approaches to leverage the strengths of both through a task-conditioned, model-based tree search. BELT uses an RRT-inspired tree search to efficiently explore the state space. Locally, the exploration is guided by a task-conditioned, learned policy capable of performing general short-horizon tasks. This task space can be quite general and abstract; its only requirements are to be sampleable and to well-cover the space of useful tasks. This search is aided by a task-conditioned model that temporally extends dynamics propagation to allow long-horizon search and sequential reasoning over tasks. BELT is demonstrated experimentally to be able to plan long-horizon, sequential trajectories with a goal conditioned policy and generate plans that are robust.</summary>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T15:51:24Z</published>\n",
      "    <arxiv:primary_category term=\"cs.RO\"/>\n",
      "    <author>\n",
      "      <name>Brian Ichter</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pierre Sermanet</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Corey Lynch</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06579v1</id>\n",
      "    <title>Fantastic Features and Where to Find Them: Detecting Cognitive Impairment with a Subsequence Classification Guided Approach</title>\n",
      "    <updated>2020-10-13T17:57:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06579v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06579v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Despite the widely reported success of embedding-based machine learning methods on natural language processing tasks, the use of more easily interpreted engineered features remains common in fields such as cognitive impairment (CI) detection. Manually engineering features from noisy text is time and resource consuming, and can potentially result in features that do not enhance model performance. To combat this, we describe a new approach to feature engineering that leverages sequential machine learning models and domain knowledge to predict which features help enhance performance. We provide a concrete example of this method on a standard data set of CI speech and demonstrate that CI classification accuracy improves by 2.3% over a strong baseline when using features produced by this method. This demonstration provides an ex-ample of how this method can be used to assist classification in fields where interpretability is important, such as health care.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T17:57:18Z</published>\n",
      "    <arxiv:comment>EMNLP Workshop on Noisy User-generated Text (W-NUT 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Benjamin Eyre</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aparna Balagopalan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jekaterina Novikova</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.05696v1</id>\n",
      "    <title>Deep Adversarial Domain Adaptation Based on Multi-layer Joint Kernelized Distance</title>\n",
      "    <updated>2020-10-09T02:32:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.05696v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.05696v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Domain adaptation refers to the learning scenario that a model learned from the source data is applied on the target data which have the same categories but different distribution. While it has been widely applied, the distribution discrepancy between source data and target data can substantially affect the adaptation performance. The problem has been recently addressed by employing adversarial learning and distinctive adaptation performance has been reported. In this paper, a deep adversarial domain adaptation model based on a multi-layer joint kernelized distance metric is proposed. By utilizing the abstract features extracted from deep networks, the multi-layer joint kernelized distance (MJKD) between the $j$th target data predicted as the $m$th category and all the source data of the $m'$th category is computed. Base on MJKD, a class-balanced selection strategy is utilized in each category to select target data that are most likely to be classified correctly and treat them as labeled data using their pseudo labels. Then an adversarial architecture is used to draw the newly generated labeled training data and the remaining target data close to each other. In this way, the target data itself provide valuable information to enhance the domain adaptation. An analysis of the proposed method is also given and the experimental results demonstrate that the proposed method can achieve a better performance than a number of state-of-the-art methods.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-09T02:32:48Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Sitong Mao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiaxin Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiao Shen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fu-lai Chung</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.05738v1</id>\n",
      "    <title>Using Type Information to Improve Entity Coreference Resolution</title>\n",
      "    <updated>2020-10-12T14:32:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.05738v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.05738v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Coreference resolution (CR) is an essential part of discourse analysis. Most recently, neural approaches have been proposed to improve over SOTA models from earlier paradigms. So far none of the published neural models leverage external semantic knowledge such as type information. This paper offers the first such model and evaluation, demonstrating modest gains in accuracy by introducing either gold standard or predicted types. In the proposed approach, type information serves both to (1) improve mention representation and (2) create a soft type consistency check between coreference candidate mentions. Our evaluation covers two different grain sizes of types over four different benchmark corpora.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-12T14:32:39Z</published>\n",
      "    <arxiv:comment>Accepted as Long Paper at CODI workshop EMNLP 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Sopan Khosla</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Carolyn Rose</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.05744v1</id>\n",
      "    <title>On Feature Selection Using Anisotropic General Regression Neural Network</title>\n",
      "    <updated>2020-10-12T14:35:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.05744v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.05744v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The presence of irrelevant features in the input dataset tends to reduce the interpretability and predictive quality of machine learning models. Therefore, the development of feature selection methods to recognize irrelevant features is a crucial topic in machine learning. Here we show how the General Regression Neural Network used with an anisotropic Gaussian Kernel can be used to perform feature selection. A number of numerical experiments are conducted using simulated data to study the robustness of the proposed methodology and its sensitivity to sample size. Finally, a comparison with four other feature selection methods is performed on several real world datasets.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-12T14:35:40Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <arxiv:journal_ref>ESANN 2020 proceedings, European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning. Online event, 2-4 October 2020, i6doc.com publ., ISBN 978-2-87587-074-2. Available from http://www.i6doc.com/en/</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Federico Amato</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fabian Guignard</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Philippe Jacquet</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mikhail Kanevski</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.05771v1</id>\n",
      "    <title>Modeling Electrical Motor Dynamics using Encoder-Decoder with Recurrent Skip Connection</title>\n",
      "    <updated>2020-10-08T15:10:04Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.05771v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.05771v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Electrical motors are the most important source of mechanical energy in the industrial world. Their modeling traditionally relies on a physics-based approach, which aims at taking their complex internal dynamics into account. In this paper, we explore the feasibility of modeling the dynamics of an electrical motor by following a data-driven approach, which uses only its inputs and outputs and does not make any assumption on its internal behaviour. We propose a novel encoder-decoder architecture which benefits from recurrent skip connections. We also propose a novel loss function that takes into account the complexity of electrical motor quantities and helps in avoiding model bias. We show that the proposed architecture can achieve a good learning performance on our high-frequency high-variance datasets. Two datasets are considered: the first one is generated using a simulator based on the physics of an induction motor and the second one is recorded from an industrial electrical motor. We benchmark our solution using variants of traditional neural networks like feedforward, convolutional, and recurrent networks. We evaluate various design choices of our architecture and compare it to the baselines. We show the domain adaptation capability of our model to learn dynamics just from simulated data by testing it on the raw sensor data. We finally show the effect of signal complexity on the proposed method ability to model temporal dynamics.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-08T15:10:04Z</published>\n",
      "    <arxiv:comment>8 pages, AAAI2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Sagar Verma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nicolas Henwood</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marc Castella</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Francois Malrait</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jean-Christophe Pesquet</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.05772v1</id>\n",
      "    <title>AttendLight: Universal Attention-Based Reinforcement Learning Model for Traffic Signal Control</title>\n",
      "    <updated>2020-10-12T15:07:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.05772v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.05772v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose AttendLight, an end-to-end Reinforcement Learning (RL) algorithm for the problem of traffic signal control. Previous approaches for this problem have the shortcoming that they require training for each new intersection with a different structure or traffic flow distribution. AttendLight solves this issue by training a single, universal model for intersections with any number of roads, lanes, phases (possible signals), and traffic flow. To this end, we propose a deep RL model which incorporates two attention models. The first attention model is introduced to handle different numbers of roads-lanes; and the second attention model is intended for enabling decision-making with any number of phases in an intersection. As a result, our proposed model works for any intersection configuration, as long as a similar configuration is represented in the training set. Experiments were conducted with both synthetic and real-world standard benchmark data-sets. The results we show cover intersections with three or four approaching roads; one-directional/bi-directional roads with one, two, and three lanes; different number of phases; and different traffic flows. We consider two regimes: (i) single-environment training, single-deployment, and (ii) multi-environment training, multi-deployment. AttendLight outperforms both classical and other RL-based approaches on all cases in both regimes.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-12T15:07:57Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Afshin Oroojlooy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mohammadreza Nazari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Davood Hajinezhad</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jorge Silva</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.05818v1</id>\n",
      "    <title>Control Barrier Functions for Unknown Nonlinear Systems using Gaussian Processes</title>\n",
      "    <updated>2020-10-12T16:12:52Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.05818v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.05818v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper focuses on the controller synthesis for unknown, nonlinear systems while ensuring safety constraints. Our approach consists of two steps, a learning step that uses Gaussian processes and a controller synthesis step that is based on control barrier functions. In the learning step, we use a data-driven approach utilizing Gaussian processes to learn the unknown control affine nonlinear dynamics together with a statistical bound on the accuracy of the learned model. In the second controller synthesis steps, we develop a systematic approach to compute control barrier functions that explicitly take into consideration the uncertainty of the learned model. The control barrier function not only results in a safe controller by construction but also provides a rigorous lower bound on the probability of satisfaction of the safety specification. Finally, we illustrate the effectiveness of the proposed results by synthesizing a safety controller for a jet engine example.</summary>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-12T16:12:52Z</published>\n",
      "    <arxiv:comment>6 pages, 3 figures, accepted at 59th IEEE Conference on Decision and Control (CDC) 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.SY\"/>\n",
      "    <author>\n",
      "      <name>Pushpak Jagtap</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>George J. Pappas</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Majid Zamani</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.05080v2</id>\n",
      "    <title>Noise in Classification</title>\n",
      "    <updated>2020-11-13T15:42:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.05080v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.05080v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This chapter considers the computational and statistical aspects of learning linear thresholds in presence of noise. When there is no noise, several algorithms exist that efficiently learn near-optimal linear thresholds using a small amount of data. However, even a small amount of adversarial noise makes this problem notoriously hard in the worst-case. We discuss approaches for dealing with these negative results by exploiting natural assumptions on the data-generating process.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-10T19:52:26Z</published>\n",
      "    <arxiv:comment>Chapter 16 of the book Beyond the Worst-Case Analysis of Algorithms</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Maria-Florina Balcan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nika Haghtalab</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.05177v1</id>\n",
      "    <title>MammoGANesis: Controlled Generation of High-Resolution Mammograms for Radiology Education</title>\n",
      "    <updated>2020-10-11T06:47:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.05177v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.05177v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>During their formative years, radiology trainees are required to interpret hundreds of mammograms per month, with the objective of becoming apt at discerning the subtle patterns differentiating benign from malignant lesions. Unfortunately, medico-legal and technical hurdles make it difficult to access and query medical images for training.\n",
      "  In this paper we train a generative adversarial network (GAN) to synthesize 512 x 512 high-resolution mammograms. The resulting model leads to the unsupervised separation of high-level features (e.g. the standard mammography views and the nature of the breast lesions), with stochastic variation in the generated images (e.g. breast adipose tissue, calcification), enabling user-controlled global and local attribute-editing of the synthesized images.\n",
      "  We demonstrate the model's ability to generate anatomically and medically relevant mammograms by achieving an average AUC of 0.54 in a double-blind study on four expert mammography radiologists to distinguish between generated and real images, ascribing to the high visual quality of the synthesized and edited mammograms, and to their potential use in advancing and facilitating medical education.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-11T06:47:56Z</published>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Cyril Zakka</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ghida Saheb</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Elie Najem</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ghina Berjawi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.05571v1</id>\n",
      "    <title>Enhancement Of Coded Speech Using a Mask-Based Post-Filter</title>\n",
      "    <updated>2020-10-12T09:48:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.05571v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.05571v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The quality of speech codecs deteriorates at low bitrates due to high quantization noise. A post-filter is generally employed to enhance the quality of the coded speech. In this paper, a data-driven post-filter relying on masking in the time-frequency domain is proposed. A fully connected neural network (FCNN), a convolutional encoder-decoder (CED) network and a long short-term memory (LSTM) network are implemeted to estimate a real-valued mask per time-frequency bin. The proposed models were tested on the five lowest operating modes (6.65 kbps-15.85 kbps) of the Adaptive Multi-Rate Wideband codec (AMR-WB). Both objective and subjective evaluations confirm the enhancement of the coded speech and also show the superiority of the mask-based neural network system over a conventional heuristic post-filter used in the standard like ITU-T G.718.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-12T09:48:09Z</published>\n",
      "    <arxiv:comment>ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Srikanth Korse</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kishan Gupta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Guillaume Fuchs</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/ICASSP40776.2020.9053283</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/ICASSP40776.2020.9053283\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.05591v2</id>\n",
      "    <title>Physically constrained causal noise models for high-contrast imaging of exoplanets</title>\n",
      "    <updated>2020-12-09T17:04:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.05591v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.05591v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The detection of exoplanets in high-contrast imaging (HCI) data hinges on post-processing methods to remove spurious light from the host star. So far, existing methods for this task hardly utilize any of the available domain knowledge about the problem explicitly. We propose a new approach to HCI post-processing based on a modified half-sibling regression scheme, and show how we use this framework to combine machine learning with existing scientific domain knowledge. On three real data sets, we demonstrate that the resulting system performs clearly better (both visually and in terms of the SNR) than one of the currently leading algorithms. If further studies can confirm these results, our method could have the potential to allow significant discoveries of exoplanets both in new and archival data.</summary>\n",
      "    <category term=\"astro-ph.IM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-12T10:35:03Z</published>\n",
      "    <arxiv:comment>Accepted at the \"Machine Learning and the Physical Sciences\" workshop at NeurIPS 2020. 9 pages, 6 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"astro-ph.IM\"/>\n",
      "    <author>\n",
      "      <name>Timothy D. Gebhard</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Markus J. Bonse</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sascha P. Quanz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bernhard Schölkopf</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.05639v1</id>\n",
      "    <title>Predicting Clinical Trial Results by Implicit Evidence Integration</title>\n",
      "    <updated>2020-10-12T12:25:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.05639v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.05639v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Clinical trials provide essential guidance for practicing Evidence-Based Medicine, though often accompanying with unendurable costs and risks. To optimize the design of clinical trials, we introduce a novel Clinical Trial Result Prediction (CTRP) task. In the CTRP framework, a model takes a PICO-formatted clinical trial proposal with its background as input and predicts the result, i.e. how the Intervention group compares with the Comparison group in terms of the measured Outcome in the studied Population. While structured clinical evidence is prohibitively expensive for manual collection, we exploit large-scale unstructured sentences from medical literature that implicitly contain PICOs and results as evidence. Specifically, we pre-train a model to predict the disentangled results from such implicit evidence and fine-tune the model with limited data on the downstream datasets. Experiments on the benchmark Evidence Integration dataset show that the proposed model outperforms the baselines by large margins, e.g., with a 10.7% relative gain over BioBERT in macro-F1. Moreover, the performance improvement is also validated on another dataset composed of clinical trials related to COVID-19.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-12T12:25:41Z</published>\n",
      "    <arxiv:comment>EMNLP 2020 long paper</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Qiao Jin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chuanqi Tan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mosha Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaozhong Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Songfang Huang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.05250v2</id>\n",
      "    <title>Domain Agnostic Learning for Unbiased Authentication</title>\n",
      "    <updated>2020-11-23T09:13:33Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.05250v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.05250v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Authentication is the task of confirming the matching relationship between a data instance and a given identity. Typical examples of authentication problems include face recognition and person re-identification. Data-driven authentication could be affected by undesired biases, i.e., the models are often trained in one domain (e.g., for people wearing spring outfits) while applied in other domains (e.g., they change the clothes to summer outfits). Previous works have made efforts to eliminate domain-difference. They typically assume domain annotations are provided, and all the domains share classes. However, for authentication, there could be a large number of domains shared by different identities/classes, and it is impossible to annotate these domains exhaustively. It could make domain-difference challenging to model and eliminate. In this paper, we propose a domain-agnostic method that eliminates domain-difference without domain labels. We alternately perform latent domain discovery and domain-difference elimination until our model no longer detects domain-difference. In our approach, the latent domains are discovered by learning the heterogeneous predictive relationships between inputs and outputs. Then domain-difference is eliminated in both class-dependent and class-independent spaces to improve robustness of elimination. We further extend our method to a meta-learning framework to pursue more thorough domain-difference elimination. Comprehensive empirical evaluation results are provided to demonstrate the effectiveness and superiority of our proposed method.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-11T14:05:16Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Jian Liang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuren Cao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shuang Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bing Bai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hao Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fei Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kun Bai</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.05315v1</id>\n",
      "    <title>SMYRF: Efficient Attention using Asymmetric Clustering</title>\n",
      "    <updated>2020-10-11T18:49:17Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.05315v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.05315v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose a novel type of balanced clustering algorithm to approximate attention. Attention complexity is reduced from $O(N^2)$ to $O(N \\log N)$, where $N$ is the sequence length. Our algorithm, SMYRF, uses Locality Sensitive Hashing (LSH) in a novel way by defining new Asymmetric transformations and an adaptive scheme that produces balanced clusters. The biggest advantage of SMYRF is that it can be used as a drop-in replacement for dense attention layers without any retraining. On the contrary, prior fast attention methods impose constraints (e.g. queries and keys share the same vector representations) and require re-training from scratch. We apply our method to pre-trained state-of-the-art Natural Language Processing and Computer Vision models and we report significant memory and speed benefits. Notably, SMYRF-BERT outperforms (slightly) BERT on GLUE, while using $50\\%$ less memory. We also show that SMYRF can be used interchangeably with dense attention before and after training. Finally, we use SMYRF to train GANs with attention in high resolutions. Using a single TPU, we were able to scale attention to 128x128=16k and 256x256=65k tokens on BigGAN on CelebA-HQ.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-11T18:49:17Z</published>\n",
      "    <arxiv:comment>30 pages, 10 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Giannis Daras</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nikita Kitaev</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Augustus Odena</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexandros G. Dimakis</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.16418v1</id>\n",
      "    <title>Handling Missing Data with Graph Representation Learning</title>\n",
      "    <updated>2020-10-30T17:59:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.16418v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.16418v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Machine learning with missing data has been approached in two different ways, including feature imputation where missing feature values are estimated based on observed values, and label prediction where downstream labels are learned directly from incomplete data. However, existing imputation models tend to have strong prior assumptions and cannot learn from downstream tasks, while models targeting label prediction often involve heuristics and can encounter scalability issues. Here we propose GRAPE, a graph-based framework for feature imputation as well as label prediction. GRAPE tackles the missing data problem using a graph representation, where the observations and features are viewed as two types of nodes in a bipartite graph, and the observed feature values as edges. Under the GRAPE framework, the feature imputation is formulated as an edge-level prediction task and the label prediction as a node-level prediction task. These tasks are then solved with Graph Neural Networks. Experimental results on nine benchmark datasets show that GRAPE yields 20% lower mean absolute error for imputation tasks and 10% lower for label prediction tasks, compared with existing state-of-the-art methods.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-30T17:59:13Z</published>\n",
      "    <arxiv:comment>NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jiaxuan You</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaobai Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daisy Yi Ding</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mykel Kochenderfer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jure Leskovec</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00046v1</id>\n",
      "    <title>Measure Inducing Classification and Regression Trees for Functional Data</title>\n",
      "    <updated>2020-10-30T18:49:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00046v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00046v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose a tree-based algorithm for classification and regression problems in the context of functional data analysis, which allows to leverage representation learning and multiple splitting rules at the node level, reducing generalization error while retaining the interpretability of a tree. This is achieved by learning a weighted functional $L^{2}$ space by means of constrained convex optimization, which is then used to extract multiple weighted integral features from the input functions, in order to determine the binary split for each internal node of the tree. The approach is designed to manage multiple functional inputs and/or outputs, by defining suitable splitting rules and loss functions that can depend on the specific problem and can also be combined with scalar and categorical data, as the tree is grown with the original greedy CART algorithm. We focus on the case of scalar-valued functional inputs defined on unidimensional domains and illustrate the effectiveness of our method in both classification and regression tasks, through a simulation study and four real world applications.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-30T18:49:53Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Edoardo Belli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Simone Vantini</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00080v1</id>\n",
      "    <title>Dynamic Data Selection for Curriculum Learning via Ability Estimation</title>\n",
      "    <updated>2020-10-30T20:01:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00080v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00080v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Curriculum learning methods typically rely on heuristics to estimate the difficulty of training examples or the ability of the model. In this work, we propose replacing difficulty heuristics with learned difficulty parameters. We also propose Dynamic Data selection for Curriculum Learning via Ability Estimation (DDaCLAE), a strategy that probes model ability at each training epoch to select the best training examples at that point. We show that models using learned difficulty and/or ability outperform heuristic-based curriculum learning models on the GLUE classification tasks.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-30T20:01:56Z</published>\n",
      "    <arxiv:comment>Findings of EMNLP 2020, presented at CoNLL 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>John P. Lalor</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hong Yu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00120v1</id>\n",
      "    <title>Optimizing Mixed Autonomy Traffic Flow With Decentralized Autonomous Vehicles and Multi-Agent RL</title>\n",
      "    <updated>2020-10-30T22:06:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00120v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00120v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study the ability of autonomous vehicles to improve the throughput of a bottleneck using a fully decentralized control scheme in a mixed autonomy setting. We consider the problem of improving the throughput of a scaled model of the San Francisco-Oakland Bay Bridge: a two-stage bottleneck where four lanes reduce to two and then reduce to one. Although there is extensive work examining variants of bottleneck control in a centralized setting, there is less study of the challenging multi-agent setting where the large number of interacting AVs leads to significant optimization difficulties for reinforcement learning methods. We apply multi-agent reinforcement algorithms to this problem and demonstrate that significant improvements in bottleneck throughput, from 20\\% at a 5\\% penetration rate to 33\\% at a 40\\% penetration rate, can be achieved. We compare our results to a hand-designed feedback controller and demonstrate that our results sharply outperform the feedback controller despite extensive tuning. Additionally, we demonstrate that the RL-based controllers adopt a robust strategy that works across penetration rates whereas the feedback controllers degrade immediately upon penetration rate variation. We investigate the feasibility of both action and observation decentralization and demonstrate that effective strategies are possible using purely local sensing. Finally, we open-source our code at  https://github.com/eugenevinitsky/decentralized_bottlenecks.</summary>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-30T22:06:05Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SY\"/>\n",
      "    <author>\n",
      "      <name>Eugene Vinitsky</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nathan Lichtle</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kanaad Parvate</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexandre Bayen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00160v1</id>\n",
      "    <title>Automatic Chronic Degenerative Diseases Identification Using Enteric Nervous System Images</title>\n",
      "    <updated>2020-10-31T01:04:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00160v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00160v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Studies recently accomplished on the Enteric Nervous System have shown that chronic degenerative diseases affect the Enteric Glial Cells (EGC) and, thus, the development of recognition methods able to identify whether or not the EGC are affected by these type of diseases may be helpful in its diagnoses. In this work, we propose the use of pattern recognition and machine learning techniques to evaluate if a given animal EGC image was obtained from a healthy individual or one affect by a chronic degenerative disease. In the proposed approach, we have performed the classification task with handcrafted features and deep learning based techniques, also known as non-handcrafted features. The handcrafted features were obtained from the textural content of the ECG images using texture descriptors, such as the Local Binary Pattern (LBP). Moreover, the representation learning techniques employed in the approach are based on different Convolutional Neural Network (CNN) architectures, such as AlexNet and VGG16, with and without transfer learning. The complementarity between the handcrafted and non-handcrafted features was also evaluated with late fusion techniques. The datasets of EGC images used in the experiments, which are also contributions of this paper, are composed of three different chronic degenerative diseases: Cancer, Diabetes Mellitus, and Rheumatoid Arthritis. The experimental results, supported by statistical analysis, shown that the proposed approach can distinguish healthy cells from the sick ones with a recognition rate of 89.30% (Rheumatoid Arthritis), 98.45% (Cancer), and 95.13% (Diabetes Mellitus), being achieved by combining classifiers obtained both feature scenarios.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-31T01:04:46Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Gustavo Z. Felipe</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jacqueline N. Zanoni</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Camila C. Sehaber-Sierakowski</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gleison D. P. Bossolani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sara R. G. Souza</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Franklin C. Flores</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luiz E. S. Oliveira</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rodolfo M. Pereira</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yandre M. G. Costa</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2011.00164v1</id>\n",
      "    <title>Differentially Private ADMM Algorithms for Machine Learning</title>\n",
      "    <updated>2020-10-31T01:37:24Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2011.00164v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2011.00164v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we study efficient differentially private alternating direction methods of multipliers (ADMM) via gradient perturbation for many machine learning problems. For smooth convex loss functions with (non)-smooth regularization, we propose the first differentially private ADMM (DP-ADMM) algorithm with performance guarantee of $(ε,δ)$-differential privacy ($(ε,δ)$-DP). From the viewpoint of theoretical analysis, we use the Gaussian mechanism and the conversion relationship between Rényi Differential Privacy (RDP) and DP to perform a comprehensive privacy analysis for our algorithm. Then we establish a new criterion to prove the convergence of the proposed algorithms including DP-ADMM. We also give the utility analysis of our DP-ADMM. Moreover, we propose an accelerated DP-ADMM (DP-AccADMM) with the Nesterov's acceleration technique. Finally, we conduct numerical experiments on many real-world datasets to show the privacy-utility tradeoff of the two proposed algorithms, and all the comparative analysis shows that DP-AccADMM converges faster and has a better utility than DP-ADMM, when the privacy budget $ε$ is larger than a threshold.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-31T01:37:24Z</published>\n",
      "    <arxiv:comment>11 pages, 2 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Tao Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fanhua Shang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuanyuan Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hongying Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Longjie Shen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maoguo Gong</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.16249v1</id>\n",
      "    <title>SLM: Learning a Discourse Language Representation with Sentence Unshuffling</title>\n",
      "    <updated>2020-10-30T13:33:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.16249v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.16249v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We introduce Sentence-level Language Modeling, a new pre-training objective for learning a discourse language representation in a fully self-supervised manner. Recent pre-training methods in NLP focus on learning either bottom or top-level language representations: contextualized word representations derived from language model objectives at one extreme and a whole sequence representation learned by order classification of two given textual segments at the other. However, these models are not directly encouraged to capture representations of intermediate-size structures that exist in natural languages such as sentences and the relationships among them. To that end, we propose a new approach to encourage learning of a contextualized sentence-level representation by shuffling the sequence of input sentences and training a hierarchical transformer model to reconstruct the original ordering. Through experiments on downstream tasks such as GLUE, SQuAD, and DiscoEval, we show that this feature of our model improves the performance of the original BERT by large margins.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-30T13:33:41Z</published>\n",
      "    <arxiv:comment>EMNLP 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Haejun Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Drew A. Hudson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kangwook Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christopher D. Manning</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.16260v1</id>\n",
      "    <title>Exploring the potential of transfer learning for metamodels of heterogeneous material deformation</title>\n",
      "    <updated>2020-10-28T12:43:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.16260v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.16260v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>From the nano-scale to the macro-scale, biological tissue is spatially heterogeneous. Even when tissue behavior is well understood, the exact subject specific spatial distribution of material properties is often unknown. And, when developing computational models of biological tissue, it is usually prohibitively computationally expensive to simulate every plausible spatial distribution of material properties for each problem of interest. Therefore, one of the major challenges in developing accurate computational models of biological tissue is capturing the potential effects of this spatial heterogeneity. Recently, machine learning based metamodels have gained popularity as a computationally tractable way to overcome this problem because they can make predictions based on a limited number of direct simulation runs. These metamodels are promising, but they often still require a high number of direct simulations to achieve an acceptable performance. Here we show that transfer learning, a strategy where knowledge gained while solving one problem is transferred to solving a different but related problem, can help overcome this limitation. Critically, transfer learning can be used to leverage both low-fidelity simulation data and simulation data that is the outcome of solving a different but related mechanical problem. In this paper, we extend Mechanical MNIST, our open source benchmark dataset of heterogeneous material undergoing large deformation, to include a selection of low-fidelity simulation results that require 2-4 orders of magnitude less CPU time to run. Then, we show that transferring the knowledge stored in metamodels trained on these low-fidelity simulation results can vastly improve the performance of metamodels used to predict the results of high-fidelity simulations.</summary>\n",
      "    <category term=\"q-bio.TO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"physics.data-an\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-28T12:43:46Z</published>\n",
      "    <arxiv:comment>9 figures, 35 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"q-bio.TO\"/>\n",
      "    <author>\n",
      "      <name>Emma Lejeune</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bill Zhao</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.16262v2</id>\n",
      "    <title>Experimental design for MRI by greedy policy search</title>\n",
      "    <updated>2020-12-15T11:12:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.16262v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.16262v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In today's clinical practice, magnetic resonance imaging (MRI) is routinely accelerated through subsampling of the associated Fourier domain. Currently, the construction of these subsampling strategies - known as experimental design - relies primarily on heuristics. We propose to learn experimental design strategies for accelerated MRI with policy gradient methods. Unexpectedly, our experiments show that a simple greedy approximation of the objective leads to solutions nearly on-par with the more general non-greedy approach. We offer a partial explanation for this phenomenon rooted in greater variance in the non-greedy objective's gradient estimates, and experimentally verify that this variance hampers non-greedy models in adapting their policies to individual MR images. We empirically show that this adaptivity is key to improving subsampling designs.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-30T13:38:09Z</published>\n",
      "    <arxiv:comment>Accepted to NeurIPS 2020 (spotlight), 15-12-2020: Fixed typos, Figure 9, and pseudocode</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Tim Bakker</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Herke van Hoof</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Max Welling</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.16324v1</id>\n",
      "    <title>Topic-Preserving Synthetic News Generation: An Adversarial Deep Reinforcement Learning Approach</title>\n",
      "    <updated>2020-10-30T15:29:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.16324v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.16324v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Nowadays, there exist powerful language models such as OpenAI's GPT-2 that can generate readable text and can be fine-tuned to generate text for a specific domain. Considering GPT-2, it cannot directly generate synthetic news with respect to a given topic and the output of the language model cannot be explicitly controlled. In this paper, we study the novel problem of topic-preserving synthetic news generation. We propose a novel deep reinforcement learning-based method to control the output of GPT-2 with respect to a given news topic. When generating text using GPT-2, by default, the most probable word is selected from the vocabulary. Instead of selecting the best word each time from GPT-2's output, an RL agent tries to select words that optimize the matching of a given topic. In addition, using a fake news detector as an adversary, we investigate generating realistic news using our proposed method. In this paper, we consider realistic news as news that cannot be easily detected by a fake news classifier. Experimental results demonstrate the effectiveness of the proposed framework on generating topic-preserving news content than state-of-the-art baselines.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-30T15:29:16Z</published>\n",
      "    <arxiv:comment>10 pages, under review</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Ahmadreza Mosallanezhad</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kai Shu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Huan Liu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.16357v1</id>\n",
      "    <title>A Cross-lingual Natural Language Processing Framework for Infodemic Management</title>\n",
      "    <updated>2020-10-30T16:26:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.16357v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.16357v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The COVID-19 pandemic has put immense pressure on health systems which are further strained due to the misinformation surrounding it. Under such a situation, providing the right information at the right time is crucial. There is a growing demand for the management of information spread using Artificial Intelligence. Hence, we have exploited the potential of Natural Language Processing for identifying relevant information that needs to be disseminated amongst the masses. In this work, we present a novel Cross-lingual Natural Language Processing framework to provide relevant information by matching daily news with trusted guidelines from the World Health Organization. The proposed pipeline deploys various techniques of NLP such as summarizers, word embeddings, and similarity metrics to provide users with news articles along with a corresponding healthcare guideline. A total of 36 models were evaluated and a combination of LexRank based summarizer on Word2Vec embedding with Word Mover distance metric outperformed all other models. This novel open-source approach can be used as a template for proactive dissemination of relevant healthcare information in the midst of misinformation spread associated with epidemics.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-30T16:26:35Z</published>\n",
      "    <arxiv:comment>8 Pages, 2 Figures, 3 Tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Ridam Pal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rohan Pandey</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vaibhav Gautam</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kanav Bhagat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tavpritesh Sethi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15999v1</id>\n",
      "    <title>Unsupervised One-shot Learning of Both Specific Instances and Generalised Classes with a Hippocampal Architecture</title>\n",
      "    <updated>2020-10-30T00:10:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15999v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15999v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Established experimental procedures for one-shot machine learning do not test the ability to learn or remember specific instances of classes, a key feature of animal intelligence. Distinguishing specific instances is necessary for many real-world tasks, such as remembering which cup belongs to you. Generalisation within classes conflicts with the ability to separate instances of classes, making it difficult to achieve both capabilities within a single architecture. We propose an extension to the standard Omniglot classification-generalisation framework that additionally tests the ability to distinguish specific instances after one exposure and introduces noise and occlusion corruption. Learning is defined as an ability to classify as well as recall training samples. Complementary Learning Systems (CLS) is a popular model of mammalian brain regions believed to play a crucial role in learning from a single exposure to a stimulus. We created an artificial neural network implementation of CLS and applied it to the extended Omniglot benchmark. Our unsupervised model demonstrates comparable performance to existing supervised ANNs on the Omniglot classification task (requiring generalisation), without the need for domain-specific inductive biases. On the extended Omniglot instance-recognition task, the same model also demonstrates significantly better performance than a baseline nearest-neighbour approach, given partial occlusion and noise.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-30T00:10:23Z</published>\n",
      "    <arxiv:comment>To appear at AI 2020 (Australasian Joint Conference on Artificial Intelligence - http://www.ajcai2020.net/)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Gideon Kowadlo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abdelrahman Ahmed</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Rawlinson</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.16043v1</id>\n",
      "    <title>CT-CAPS: Feature Extraction-based Automated Framework for COVID-19 Disease Identification from Chest CT Scans using Capsule Networks</title>\n",
      "    <updated>2020-10-30T03:35:29Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.16043v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.16043v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The global outbreak of the novel corona virus (COVID-19) disease has drastically impacted the world and led to one of the most challenging crisis across the globe since World War II. The early diagnosis and isolation of COVID-19 positive cases are considered as crucial steps towards preventing the spread of the disease and flattening the epidemic curve. Chest Computed Tomography (CT) scan is a highly sensitive, rapid, and accurate diagnostic technique that can complement Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. Recently, deep learning-based models, mostly based on Convolutional Neural Networks (CNN), have shown promising diagnostic results. CNNs, however, are incapable of capturing spatial relations between image instances and require large datasets. Capsule Networks, on the other hand, can capture spatial relations, require smaller datasets, and have considerably fewer parameters. In this paper, a Capsule network framework, referred to as the \"CT-CAPS\", is presented to automatically extract distinctive features of chest CT scans. These features, which are extracted from the layer before the final capsule layer, are then leveraged to differentiate COVID-19 from Non-COVID cases. The experiments on our in-house dataset of 307 patients show the state-of-the-art performance with the accuracy of 90.8%, sensitivity of 94.5%, and specificity of 86.0%.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-30T03:35:29Z</published>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Shahin Heidarian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Parnian Afshar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arash Mohammadi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Moezedin Javad Rafiee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anastasia Oikonomou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Konstantinos N. Plataniotis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Farnoosh Naderkhani</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.16052v2</id>\n",
      "    <title>HHAR-net: Hierarchical Human Activity Recognition using Neural Networks</title>\n",
      "    <updated>2020-11-10T22:52:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.16052v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.16052v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Activity recognition using built-in sensors in smart and wearable devices provides great opportunities to understand and detect human behavior in the wild and gives a more holistic view of individuals' health and well being. Numerous computational methods have been applied to sensor streams to recognize different daily activities. However, most methods are unable to capture different layers of activities concealed in human behavior. Also, the performance of the models starts to decrease with increasing the number of activities. This research aims at building a hierarchical classification with Neural Networks to recognize human activities based on different levels of abstraction. We evaluate our model on the Extrasensory dataset; a dataset collected in the wild and containing data from smartphones and smartwatches. We use a two-level hierarchy with a total of six mutually exclusive labels namely, \"lying down\", \"sitting\", \"standing in place\", \"walking\", \"running\", and \"bicycling\" divided into \"stationary\" and \"non-stationary\". The results show that our model can recognize low-level activities (stationary/non-stationary) with 95.8% accuracy and overall accuracy of 92.8% over six labels. This is 3% above our best performing baseline.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-28T17:06:42Z</published>\n",
      "    <arxiv:comment>Accepted in IHCI2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Mehrdad Fazli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kamran Kowsari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Erfaneh Gharavi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Laura Barnes</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Afsaneh Doryab</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15040v2</id>\n",
      "    <title>Training Generative Adversarial Networks by Solving Ordinary Differential Equations</title>\n",
      "    <updated>2020-11-28T16:07:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15040v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15040v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The instability of Generative Adversarial Network (GAN) training has frequently been attributed to gradient descent. Consequently, recent methods have aimed to tailor the models and training procedures to stabilise the discrete updates. In contrast, we study the continuous-time dynamics induced by GAN training. Both theory and toy experiments suggest that these dynamics are in fact surprisingly stable. From this perspective, we hypothesise that instabilities in training GANs arise from the integration error in discretising the continuous dynamics. We experimentally verify that well-known ODE solvers (such as Runge-Kutta) can stabilise training - when combined with a regulariser that controls the integration error. Our approach represents a radical departure from previous methods which typically use adaptive optimisation and stabilisation techniques that constrain the functional space (e.g. Spectral Normalisation). Evaluation on CIFAR-10 and ImageNet shows that our method outperforms several strong baselines, demonstrating its efficacy.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-28T15:23:49Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Chongli Qin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yan Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jost Tobias Springenberg</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrew Brock</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jeff Donahue</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Timothy P. Lillicrap</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pushmeet Kohli</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15054v1</id>\n",
      "    <title>Attribution Preservation in Network Compression for Reliable Network Interpretation</title>\n",
      "    <updated>2020-10-28T16:02:31Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15054v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15054v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Neural networks embedded in safety-sensitive applications such as self-driving cars and wearable health monitors rely on two important techniques: input attribution for hindsight analysis and network compression to reduce its size for edge-computing. In this paper, we show that these seemingly unrelated techniques conflict with each other as network compression deforms the produced attributions, which could lead to dire consequences for mission-critical applications. This phenomenon arises due to the fact that conventional network compression methods only preserve the predictions of the network while ignoring the quality of the attributions. To combat the attribution inconsistency problem, we present a framework that can preserve the attributions while compressing a network. By employing the Weighted Collapsed Attribution Matching regularizer, we match the attribution maps of the network being compressed to its pre-compression former self. We demonstrate the effectiveness of our algorithm both quantitatively and qualitatively on diverse compression methods.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-28T16:02:31Z</published>\n",
      "    <arxiv:comment>NeurIPS 2020. Code: https://github.com/GeondoPark/attribute-preserve</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Geondo Park</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>June Yong Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sung Ju Hwang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eunho Yang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15056v1</id>\n",
      "    <title>Self-awareness in Intelligent Vehicles: Experience Based Abnormality Detection</title>\n",
      "    <updated>2020-10-28T16:08:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15056v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15056v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The evolution of Intelligent Transportation System in recent times necessitates the development of self-driving agents: the self-awareness consciousness. This paper aims to introduce a novel method to detect abnormalities based on internal cross-correlation parameters of the vehicle. Before the implementation of Machine Learning, the detection of abnormalities were manually programmed by checking every variable and creating huge nested conditions that are very difficult to track. Nowadays, it is possible to train a Dynamic Bayesian Network (DBN) model to automatically evaluate and detect when the vehicle is potentially misbehaving. In this paper, different scenarios have been set in order to train and test a switching DBN for Perimeter Monitoring Task using a semantic segmentation for the DBN model and Hellinger Distance metric for abnormality measurements.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-28T16:08:54Z</published>\n",
      "    <arxiv:comment>Robot 2019: Fourth Iberian Robotics Conference</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Divya Kanapram</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pablo Marin-Plaza</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lucio Marcenaro</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Martin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arturo de la Escalera</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Carlo Regazzoni</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1007/978-3-030-35990-4_18</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1007/978-3-030-35990-4_18\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15058v2</id>\n",
      "    <title>Measuring non-trivial compositionality in emergent communication</title>\n",
      "    <updated>2020-10-29T16:22:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15058v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15058v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Compositionality is an important explanatory target in emergent communication and language evolution. The vast majority of computational models of communication account for the emergence of only a very basic form of compositionality: trivial compositionality. A compositional protocol is trivially compositional if the meaning of a complex signal (e.g. blue circle) boils down to the intersection of meanings of its constituents (e.g. the intersection of the set of blue objects and the set of circles). A protocol is non-trivially compositional (NTC) if the meaning of a complex signal (e.g. biggest apple) is a more complex function of the meanings of their constituents. In this paper, we review several metrics of compositionality used in emergent communication and experimentally show that most of them fail to detect NTC - i.e. they treat non-trivial compositionality as a failure of compositionality. The one exception is tree reconstruction error, a metric motivated by formal accounts of compositionality. These results emphasise important limitations of emergent communication research that could hamper progress on modelling the emergence of NTC.</summary>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-28T16:11:07Z</published>\n",
      "    <arxiv:comment>4th Workshop on Emergent Communication, NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.NE\"/>\n",
      "    <author>\n",
      "      <name>Tomasz Korbak</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Julian Zubek</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joanna Rączaszek-Leonardi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15116v2</id>\n",
      "    <title>On Graph Neural Networks versus Graph-Augmented MLPs</title>\n",
      "    <updated>2020-12-02T16:46:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15116v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15116v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>From the perspective of expressive power, this work compares multi-layer Graph Neural Networks (GNNs) with a simplified alternative that we call Graph-Augmented Multi-Layer Perceptrons (GA-MLPs), which first augments node features with certain multi-hop operators on the graph and then applies an MLP in a node-wise fashion. From the perspective of graph isomorphism testing, we show both theoretically and numerically that GA-MLPs with suitable operators can distinguish almost all non-isomorphic graphs, just like the Weifeiler-Lehman (WL) test. However, by viewing them as node-level functions and examining the equivalence classes they induce on rooted graphs, we prove a separation in expressive power between GA-MLPs and GNNs that grows exponentially in depth. In particular, unlike GNNs, GA-MLPs are unable to count the number of attributed walks. We also demonstrate via community detection experiments that GA-MLPs can be limited by their choice of operator family, as compared to GNNs with higher flexibility in learning.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-28T17:59:59Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Lei Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhengdao Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joan Bruna</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15187v1</id>\n",
      "    <title>A Study on Efficiency in Continual Learning Inspired by Human Learning</title>\n",
      "    <updated>2020-10-28T19:11:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15187v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15187v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Humans are efficient continual learning systems; we continually learn new skills from birth with finite cells and resources. Our learning is highly optimized both in terms of capacity and time while not suffering from catastrophic forgetting. In this work we study the efficiency of continual learning systems, taking inspiration from human learning. In particular, inspired by the mechanisms of sleep, we evaluate popular pruning-based continual learning algorithms, using PackNet as a case study. First, we identify that weight freezing, which is used in continual learning without biological justification, can result in over $2\\times$ as many weights being used for a given level of performance. Secondly, we note the similarity in human day and night time behaviors to the training and pruning phases respectively of PackNet. We study a setting where the pruning phase is given a time budget, and identify connections between iterative pruning and multiple sleep cycles in humans. We show there exists an optimal choice of iteration v.s. epochs given different tasks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-28T19:11:01Z</published>\n",
      "    <arxiv:comment>Accepted at NeurIPS 2020 BabyMind Workshop</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Philip J. Ball</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yingzhen Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Angus Lamb</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cheng Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15597v1</id>\n",
      "    <title>Enhancing reinforcement learning by a finite reward response filter with a case study in intelligent structural control</title>\n",
      "    <updated>2020-10-25T19:28:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15597v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15597v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In many reinforcement learning (RL) problems, it takes some time until a taken action by the agent reaches its maximum effect on the environment and consequently the agent receives the reward corresponding to that action by a delay called action-effect delay. Such delays reduce the performance of the learning algorithm and increase the computational costs, as the reinforcement learning agent values the immediate rewards more than the future reward that is more related to the taken action. This paper addresses this issue by introducing an applicable enhanced Q-learning method in which at the beginning of the learning phase, the agent takes a single action and builds a function that reflects the environments response to that action, called the reflexive $γ$ - function. During the training phase, the agent utilizes the created reflexive $γ$- function to update the Q-values. We have applied the developed method to a structural control problem in which the goal of the agent is to reduce the vibrations of a building subjected to earthquake excitations with a specified delay. Seismic control problems are considered as a complex task in structural engineering because of the stochastic and unpredictable nature of earthquakes and the complex behavior of the structure. Three scenarios are presented to study the effects of zero, medium, and long action-effect delays and the performance of the Enhanced method is compared to the standard Q-learning method. Both RL methods use neural network to learn to estimate the state-action value function that is used to control the structure. The results show that the enhanced method significantly outperforms the performance of the original method in all cases, and also improves the stability of the algorithm in dealing with action-effect delays.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-25T19:28:35Z</published>\n",
      "    <arxiv:comment>16 pages, 16 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Hamid Radmard Rahmani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Carsten Koenke</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marco A. Wiering</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15599v1</id>\n",
      "    <title>Expert Selection in High-Dimensional Markov Decision Processes</title>\n",
      "    <updated>2020-10-26T03:57:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15599v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15599v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this work we present a multi-armed bandit framework for online expert selection in Markov decision processes and demonstrate its use in high-dimensional settings. Our method takes a set of candidate expert policies and switches between them to rapidly identify the best performing expert using a variant of the classical upper confidence bound algorithm, thus ensuring low regret in the overall performance of the system. This is useful in applications where several expert policies may be available, and one needs to be selected at run-time for the underlying environment.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-26T03:57:25Z</published>\n",
      "    <arxiv:comment>In proceedings of the 59th IEEE Conference on Decision and Control 2020. arXiv admin note: text overlap with arXiv:1707.05714</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Vicenc Rubies-Royo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eric Mazumdar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Roy Dong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Claire Tomlin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>S. Shankar Sastry</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15740v1</id>\n",
      "    <title>Recurrent Neural Networks for video object detection</title>\n",
      "    <updated>2020-10-29T16:40:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15740v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15740v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>There is lots of scientific work about object detection in images. For many applications like for example autonomous driving the actual data on which classification has to be done are videos. This work compares different methods, especially those which use Recurrent Neural Networks to detect objects in videos. We differ between feature-based methods, which feed feature maps of different frames into the recurrent units, box-level methods, which feed bounding boxes with class probabilities into the recurrent units and methods which use flow networks. This study indicates common outcomes of the compared methods like the benefit of including the temporal context into object detection and states conclusions and guidelines for video object detection networks.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-29T16:40:10Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Ahmad B Qasim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arnd Pettirsch</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15760v1</id>\n",
      "    <title>Identifying Transition States of Chemical Kinetic Systems using Network Embedding Techniques</title>\n",
      "    <updated>2020-10-29T17:07:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15760v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15760v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Using random walk sampling methods for feature learning on networks, we develop a method for generating low-dimensional node embeddings for directed graphs and identifying transition states of stochastic chemical reacting systems. We modified objective functions adopted in existing random walk based network embedding methods to handle directed graphs and neighbors of different degrees. Through optimization via gradient ascent, we embed the weighted graph vertices into a low-dimensional vector space Rd while preserving the neighborhood of each node. We then demonstrate the effectiveness of the method on dimension reduction through several examples regarding identification of transition states of chemical reactions, especially for entropic systems.</summary>\n",
      "    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-29T17:07:25Z</published>\n",
      "    <arxiv:primary_category term=\"math.NA\"/>\n",
      "    <author>\n",
      "      <name>Paula Mercurio</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Di Liu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15805v2</id>\n",
      "    <title>A Local Search Framework for Experimental Design</title>\n",
      "    <updated>2020-12-18T05:00:04Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15805v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15805v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present a local search framework to design and analyze both combinatorial algorithms and rounding algorithms for experimental design problems. This framework provides a unifying approach to match and improve all known results in D/A/E-design and to obtain new results in previously unknown settings.\n",
      "  For combinatorial algorithms, we provide a new analysis of the classical Fedorov's exchange method. We prove that this simple local search algorithm works well as long as there exists an almost optimal solution with good condition number. Moreover, we design a new combinatorial local search algorithm for E-design using the regret minimization framework.\n",
      "  For rounding algorithms, we provide a unified randomized exchange algorithm to match and improve previous results for D/A/E-design. Furthermore, the algorithm works in the more general setting to approximately satisfy multiple knapsack constraints, which can be used for weighted experimental design and for incorporating fairness constraints into experimental design.</summary>\n",
      "    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-29T17:43:06Z</published>\n",
      "    <arxiv:comment>Improved probability bound in Theorem 1.4. A preliminary version accepted by SODA 2021</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.DS\"/>\n",
      "    <author>\n",
      "      <name>Lap Chi Lau</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hong Zhou</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15899v1</id>\n",
      "    <title>Transfer Learning improves MI BCI models classification accuracy in Parkinson's disease patients</title>\n",
      "    <updated>2020-10-29T19:28:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15899v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15899v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Motor-Imagery based BCI (MI-BCI) neurorehabilitation can improve locomotor ability and reduce the deficit symptoms in Parkinson's Disease patients. Advanced Motor-Imagery BCI methods are needed to overcome the accuracy and time-related MI BCI calibration challenges in such patients. In this study, we proposed a Multi-session FBCSP (msFBCSP) based on inter-session transfer learning and we investigated its performance compared to the single-session based FBSCP. The main result of this study is the significantly improved accuracy obtained by proposed msFBCSP compared to single-session FBCSP in PD patients (median 81.3%, range 41.2-100.0% vs median 61.1%, range 25.0-100.0%, respectively; p&lt;0.001). In conclusion, this study proposes a transfer learning-based multi-session based FBCSP approach which allowed to significantly improve calibration accuracy in MI BCI performed on PD patients.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-29T19:28:00Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Aleksandar Miladinović</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Miloš Ajčević</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pierpaolo Busan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joanna Jarmolowska</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Giulia Silveri</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Susanna Mezzarobba</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Piero Paolo Battaglini</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Agostino Accardo</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15937v1</id>\n",
      "    <title>Detecting small polyps using a Dynamic SSD-GAN</title>\n",
      "    <updated>2020-10-29T20:54:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15937v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15937v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Endoscopic examinations are used to inspect the throat, stomach and bowel for polyps which could develop into cancer. Machine learning systems can be trained to process colonoscopy images and detect polyps. However, these systems tend to perform poorly on objects which appear visually small in the images. It is shown here that combining the single-shot detector as a region proposal network with an adversarially-trained generator to upsample small region proposals can significantly improve the detection of visually-small polyps. The Dynamic SSD-GAN pipeline introduced in this paper achieved a 12% increase in sensitivity on visually-small polyps compared to a conventional FCN baseline.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-29T20:54:34Z</published>\n",
      "    <arxiv:comment>Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended Abstract</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Daniel C. Ohrenstein</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Patrick Brandao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniel Toth</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Laurence Lovat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Danail Stoyanov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peter Mountney</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15985v1</id>\n",
      "    <title>Differential Privacy and Natural Language Processing to Generate Contextually Similar Decoy Messages in Honey Encryption Scheme</title>\n",
      "    <updated>2020-10-29T23:02:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15985v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15985v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Honey Encryption is an approach to encrypt the messages using low min-entropy keys, such as weak passwords, OTPs, PINs, credit card numbers. The ciphertext is produces, when decrypted with any number of incorrect keys, produces plausible-looking but bogus plaintext called \"honey messages\". But the current techniques used in producing the decoy plaintexts do not model human language entirely. A gibberish, random assortment of words is not enough to fool an attacker; that will not be acceptable and convincing, whether or not the attacker knows some information of the genuine source.\n",
      "  In this paper, I focus on the plaintexts which are some non-numeric informative messages. In order to fool the attacker into believing that the decoy message can actually be from a certain source, we need to capture the empirical and contextual properties of the language. That is, there should be no linguistic difference between real and fake message, without revealing the structure of the real message. I employ natural language processing and generalized differential privacy to solve this problem. Mainly I focus on machine learning methods like keyword extraction, context classification, bags-of-words, word embeddings, transformers for text processing to model privacy for text documents. Then I prove the security of this approach with e-differential privacy.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-29T23:02:32Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Kunjal Panchal</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15425v2</id>\n",
      "    <title>Detection of asteroid trails in Hubble Space Telescope images using Deep Learning</title>\n",
      "    <updated>2020-10-30T12:48:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15425v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15425v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present an application of Deep Learning for the image recognition of asteroid trails in single-exposure photos taken by the Hubble Space Telescope. Using algorithms based on multi-layered deep Convolutional Neural Networks, we report accuracies of above 80% on the validation set. Our project was motivated by the Hubble Asteroid Hunter project on Zooniverse, which focused on identifying these objects in order to localize and better characterize them. We aim to demonstrate that Machine Learning techniques can be very useful in trying to solve problems that are closely related to Astronomy and Astrophysics, but that they are still not developed enough for very specific tasks.</summary>\n",
      "    <category term=\"astro-ph.IM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"astro-ph.EP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-29T09:03:18Z</published>\n",
      "    <arxiv:comment>12 pages, 8 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"astro-ph.IM\"/>\n",
      "    <author>\n",
      "      <name>Andrei A. Parfeni</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Laurentiu I. Caramete</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andreea M. Dobre</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nguyen Tran Bach</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15440v1</id>\n",
      "    <title>FlatNet: Towards Photorealistic Scene Reconstruction from Lensless Measurements</title>\n",
      "    <updated>2020-10-29T09:20:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15440v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15440v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Lensless imaging has emerged as a potential solution towards realizing ultra-miniature cameras by eschewing the bulky lens in a traditional camera. Without a focusing lens, the lensless cameras rely on computational algorithms to recover the scenes from multiplexed measurements. However, the current iterative-optimization-based reconstruction algorithms produce noisier and perceptually poorer images. In this work, we propose a non-iterative deep learning based reconstruction approach that results in orders of magnitude improvement in image quality for lensless reconstructions. Our approach, called $\\textit{FlatNet}$, lays down a framework for reconstructing high-quality photorealistic images from mask-based lensless cameras, where the camera's forward model formulation is known. FlatNet consists of two stages: (1) an inversion stage that maps the measurement into a space of intermediate reconstruction by learning parameters within the forward model formulation, and (2) a perceptual enhancement stage that improves the perceptual quality of this intermediate reconstruction. These stages are trained together in an end-to-end manner. We show high-quality reconstructions by performing extensive experiments on real and challenging scenes using two different types of lensless prototypes: one which uses a separable forward model and another, which uses a more general non-separable cropped-convolution model. Our end-to-end approach is fast, produces photorealistic reconstructions, and is easy to adopt for other mask-based lensless cameras.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-29T09:20:22Z</published>\n",
      "    <arxiv:comment>Accepted to IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2020. Supplementary material attached. For project website, see https://siddiquesalman.github.io/flatnet/</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Salman S. Khan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Varun Sundar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vivek Boominathan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ashok Veeraraghavan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kaushik Mitra</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/TPAMI.2020.3033882</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/TPAMI.2020.3033882\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15511v1</id>\n",
      "    <title>An Exact Solution Path Algorithm for SLOPE and Quasi-Spherical OSCAR</title>\n",
      "    <updated>2020-10-29T12:03:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15511v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15511v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Sorted $L_1$ penalization estimator (SLOPE) is a regularization technique for sorted absolute coefficients in high-dimensional regression. By arbitrarily setting its regularization weights $λ$ under the monotonicity constraint, SLOPE can have various feature selection and clustering properties. On weight tuning, the selected features and their clusters are very sensitive to the tuning parameters. Moreover, the exhaustive tracking of their changes is difficult using grid search methods. This study presents a solution path algorithm that provides the complete and exact path of solutions for SLOPE in fine-tuning regularization weights. A simple optimality condition for SLOPE is derived and used to specify the next splitting point of the solution path. This study also proposes a new design of a regularization sequence $λ$ for feature clustering, which is called the quasi-spherical and octagonal shrinkage and clustering algorithm for regression (QS-OSCAR). QS-OSCAR is designed with a contour surface of the regularization terms most similar to a sphere. Among several regularization sequence designs, sparsity and clustering performance are compared through simulation studies. The numerical observations show that QS-OSCAR performs feature clustering more efficiently than other designs.</summary>\n",
      "    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-29T12:03:22Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ME\"/>\n",
      "    <author>\n",
      "      <name>Shunichi Nomura</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15533v1</id>\n",
      "    <title>How do Offline Measures for Exploration in Reinforcement Learning behave?</title>\n",
      "    <updated>2020-10-29T12:58:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15533v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15533v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Sufficient exploration is paramount for the success of a reinforcement learning agent. Yet, exploration is rarely assessed in an algorithm-independent way. We compare the behavior of three data-based, offline exploration metrics described in the literature on intuitive simple distributions and highlight problems to be aware of when using them. We propose a fourth metric,uniform relative entropy, and implement it using either a k-nearest-neighbor or a nearest-neighbor-ratio estimator, highlighting that the implementation choices have a profound impact on these measures.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-29T12:58:30Z</published>\n",
      "    <arxiv:comment>KBRL Workshop at IJCAI-PRICAI 2020, Yokohama, Japan</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jakob J. Hollenstein</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sayantan Auddy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matteo Saveriano</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Erwan Renaudo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Justus Piater</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.15581v1</id>\n",
      "    <title>The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research</title>\n",
      "    <updated>2020-10-22T15:11:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.15581v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.15581v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Increasingly, modern Artificial Intelligence (AI) research has become more computationally intensive. However, a growing concern is that due to unequal access to computing power, only certain firms and elite universities have advantages in modern AI research. Using a novel dataset of 171394 papers from 57 prestigious computer science conferences, we document that firms, in particular, large technology firms and elite universities have increased participation in major AI conferences since deep learning's unanticipated rise in 2012. The effect is concentrated among elite universities, which are ranked 1-50 in the QS World University Rankings. Further, we find two strategies through which firms increased their presence in AI research: first, they have increased firm-only publications; and second, firms are collaborating primarily with elite universities. Consequently, this increased presence of firms and elite universities in AI research has crowded out mid-tier (QS ranked 201-300) and lower-tier (QS ranked 301-500) universities. To provide causal evidence that deep learning's unanticipated rise resulted in this divergence, we leverage the generalized synthetic control method, a data-driven counterfactual estimator. Using machine learning based text analysis methods, we provide additional evidence that the divergence between these two groups - large firms and non-elite universities - is driven by access to computing power or compute, which we term as the \"compute divide\". This compute divide between large firms and non-elite universities increases concerns around bias and fairness within AI technology, and presents an obstacle towards \"democratizing\" AI. These results suggest that a lack of access to specialized equipment such as compute can de-democratize knowledge production.</summary>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-22T15:11:14Z</published>\n",
      "    <arxiv:comment>52 pages,13 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CY\"/>\n",
      "    <author>\n",
      "      <name>Nur Ahmed</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Muntasir Wahed</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14907v1</id>\n",
      "    <title>Online feature selection for rapid, low-overhead learning in networked systems</title>\n",
      "    <updated>2020-10-28T12:00:42Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14907v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14907v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Data-driven functions for operation and management often require measurements collected through monitoring for model training and prediction. The number of data sources can be very large, which requires a significant communication and computing overhead to continuously extract and collect this data, as well as to train and update the machine-learning models. We present an online algorithm, called OSFS, that selects a small feature set from a large number of available data sources, which allows for rapid, low-overhead, and effective learning and prediction. OSFS is instantiated with a feature ranking algorithm and applies the concept of a stable feature set, which we introduce in the paper. We perform extensive, experimental evaluation of our method on data from an in-house testbed. We find that OSFS requires several hundreds measurements to reduce the number of data sources by two orders of magnitude, from which models are trained with acceptable prediction accuracy. While our method is heuristic and can be improved in many ways, the results clearly suggests that many learning tasks do not require a lengthy monitoring phase and expensive offline training.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-28T12:00:42Z</published>\n",
      "    <arxiv:comment>A short version of this paper has been published at IFIP/IEEE 16th International Conference on Network and Service Management, 2-6 November 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xiaoxuan Wang</name>\n",
      "      <arxiv:affiliation>KTH Royal Institute of Technology, Sweden</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Forough Shahab Samani</name>\n",
      "      <arxiv:affiliation>KTH Royal Institute of Technology, Sweden</arxiv:affiliation>\n",
      "      <arxiv:affiliation>RISE Research Institutes of Sweden</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rolf Stadler</name>\n",
      "      <arxiv:affiliation>KTH Royal Institute of Technology, Sweden</arxiv:affiliation>\n",
      "      <arxiv:affiliation>RISE Research Institutes of Sweden</arxiv:affiliation>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.00064v1</id>\n",
      "    <title>Linear-Sample Learning of Low-Rank Distributions</title>\n",
      "    <updated>2020-09-30T19:10:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.00064v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.00064v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Many latent-variable applications, including community detection, collaborative filtering, genomic analysis, and NLP, model data as generated by low-rank matrices. Yet despite considerable research, except for very special cases, the number of samples required to efficiently recover the underlying matrices has not been known. We determine the onset of learning in several common latent-variable settings. For all of them, we show that learning $k\\times k$, rank-$r$, matrices to normalized $L_{1}$ distance $ε$ requires $Ω(\\frac{kr}{ε^2})$ samples, and propose an algorithm that uses ${\\cal O}(\\frac{kr}{ε^2}\\log^2\\frac rε)$ samples, a number linear in the high dimension, and nearly linear in the, typically low, rank. The algorithm improves on existing spectral techniques and runs in polynomial time. The proofs establish new results on the rapid convergence of the spectral distance between the model and observation matrices, and may be of independent interest.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-30T19:10:32Z</published>\n",
      "    <arxiv:comment>Accepted for Neurips 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Ayush Jain</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alon Orlitsky</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.00072v2</id>\n",
      "    <title>Using Machine Learning to Augment Coarse-Grid Computational Fluid Dynamics Simulations</title>\n",
      "    <updated>2020-10-03T19:22:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.00072v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.00072v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Simulation of turbulent flows at high Reynolds number is a computationally challenging task relevant to a large number of engineering and scientific applications in diverse fields such as climate science, aerodynamics, and combustion. Turbulent flows are typically modeled by the Navier-Stokes equations. Direct Numerical Simulation (DNS) of the Navier-Stokes equations with sufficient numerical resolution to capture all the relevant scales of the turbulent motions can be prohibitively expensive. Simulation at lower-resolution on a coarse-grid introduces significant errors. We introduce a machine learning (ML) technique based on a deep neural network architecture that corrects the numerical errors induced by a coarse-grid simulation of turbulent flows at high-Reynolds numbers, while simultaneously recovering an estimate of the high-resolution fields. Our proposed simulation strategy is a hybrid ML-PDE solver that is capable of obtaining a meaningful high-resolution solution trajectory while solving the system PDE at a lower resolution. The approach has the potential to dramatically reduce the expense of turbulent flow simulations. As a proof-of-concept, we demonstrate our ML-PDE strategy on a two-dimensional turbulent (Rayleigh Number $Ra=10^9$) Rayleigh-Bénard Convection (RBC) problem.</summary>\n",
      "    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"physics.geo-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-30T19:29:21Z</published>\n",
      "    <arxiv:comment>Corrected typographical errors in the previous version related to the incorrectly formatted accented character \"é\" appearing in various places in the manuscript</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"physics.comp-ph\"/>\n",
      "    <author>\n",
      "      <name>Jaideep Pathak</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mustafa Mustafa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Karthik Kashinath</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Emmanuel Motheau</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Thorsten Kurth</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marcus Day</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.00073v1</id>\n",
      "    <title>Adaptive Online Estimation of Piecewise Polynomial Trends</title>\n",
      "    <updated>2020-09-30T19:30:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.00073v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.00073v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We consider the framework of non-stationary stochastic optimization [Besbes et al, 2015] with squared error losses and noisy gradient feedback where the dynamic regret of an online learner against a time varying comparator sequence is studied. Motivated from the theory of non-parametric regression, we introduce a new variational constraint that enforces the comparator sequence to belong to a discrete $k^{th}$ order Total Variation ball of radius $C_n$. This variational constraint models comparators that have piece-wise polynomial structure which has many relevant practical applications [Tibshirani, 2014]. By establishing connections to the theory of wavelet based non-parametric regression, we design a polynomial time algorithm that achieves the nearly optimal dynamic regret of $\\tilde{O}(n^{\\frac{1}{2k+3}}C_n^{\\frac{2}{2k+3}})$. The proposed policy is adaptive to the unknown radius $C_n$. Further, we show that the same policy is minimax optimal for several other non-parametric families of interest.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-30T19:30:28Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Dheeraj Baby</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu-Xiang Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.00121v1</id>\n",
      "    <title>Interactive Re-Fitting as a Technique for Improving Word Embeddings</title>\n",
      "    <updated>2020-09-30T21:54:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.00121v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.00121v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Word embeddings are a fixed, distributional representation of the context of words in a corpus learned from word co-occurrences. While word embeddings have proven to have many practical uses in natural language processing tasks, they reflect the attributes of the corpus upon which they are trained. Recent work has demonstrated that post-processing of word embeddings to apply information found in lexical dictionaries can improve their quality. We build on this post-processing technique by making it interactive. Our approach makes it possible for humans to adjust portions of a word embedding space by moving sets of words closer to one another. One motivating use case for this capability is to enable users to identify and reduce the presence of bias in word embeddings. Our approach allows users to trigger selective post-processing as they interact with and assess potential bias in word embeddings.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-30T21:54:22Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>James Powell</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kari Sentz</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.07494v2</id>\n",
      "    <title>Knowledge Transfer in Multi-Task Deep Reinforcement Learning for Continuous Control</title>\n",
      "    <updated>2020-10-16T14:34:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.07494v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.07494v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>While Deep Reinforcement Learning (DRL) has emerged as a promising approach to many complex tasks, it remains challenging to train a single DRL agent that is capable of undertaking multiple different continuous control tasks. In this paper, we present a Knowledge Transfer based Multi-task Deep Reinforcement Learning framework (KTM-DRL) for continuous control, which enables a single DRL agent to achieve expert-level performance in multiple different tasks by learning from task-specific teachers. In KTM-DRL, the multi-task agent first leverages an offline knowledge transfer algorithm designed particularly for the actor-critic architecture to quickly learn a control policy from the experience of task-specific teachers, and then it employs an online learning algorithm to further improve itself by learning from new online transition samples under the guidance of those teachers. We perform a comprehensive empirical study with two commonly-used benchmarks in the MuJoCo continuous control task suite. The experimental results well justify the effectiveness of KTM-DRL and its knowledge transfer and online learning algorithms, as well as its superiority over the state-of-the-art by a large margin.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-15T03:26:47Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Zhiyuan Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kun Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhengping Che</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jian Tang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jieping Ye</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.07587v1</id>\n",
      "    <title>Depth-Width Trade-offs for Neural Networks via Topological Entropy</title>\n",
      "    <updated>2020-10-15T08:14:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.07587v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.07587v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>One of the central problems in the study of deep learning theory is to understand how the structure properties, such as depth, width and the number of nodes, affect the expressivity of deep neural networks. In this work, we show a new connection between the expressivity of deep neural networks and topological entropy from dynamical system, which can be used to characterize depth-width trade-offs of neural networks. We provide an upper bound on the topological entropy of neural networks with continuous semi-algebraic units by the structure parameters. Specifically, the topological entropy of ReLU network with $l$ layers and $m$ nodes per layer is upper bounded by $O(l\\log m)$. Besides, if the neural network is a good approximation of some function $f$, then the size of the neural network has an exponential lower bound with respect to the topological entropy of $f$. Moreover, we discuss the relationship between topological entropy, the number of oscillations, periods and Lipschitz constant.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-15T08:14:44Z</published>\n",
      "    <arxiv:comment>17 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Kaifeng Bu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yaobo Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qingxian Luo</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.07334v1</id>\n",
      "    <title>Towards Accurate Quantization and Pruning via Data-free Knowledge Transfer</title>\n",
      "    <updated>2020-10-14T18:02:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.07334v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.07334v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>When large scale training data is available, one can obtain compact and accurate networks to be deployed in resource-constrained environments effectively through quantization and pruning. However, training data are often protected due to privacy concerns and it is challenging to obtain compact networks without data. We study data-free quantization and pruning by transferring knowledge from trained large networks to compact networks. Auxiliary generators are simultaneously and adversarially trained with the targeted compact networks to generate synthetic inputs that maximize the discrepancy between the given large network and its quantized or pruned version. We show theoretically that the alternating optimization for the underlying minimax problem converges under mild conditions for pruning and quantization. Our data-free compact networks achieve competitive accuracy to networks trained and fine-tuned with training data. Our quantized and pruned networks achieve good performance while being more compact and lightweight. Further, we demonstrate that the compact structure and corresponding initialization from the Lottery Ticket Hypothesis can also help in data-free training.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-14T18:02:55Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Chen Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zheng Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ali Shafahi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Manli Shu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amin Ghiasi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tom Goldstein</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.07373v1</id>\n",
      "    <title>Graph Deep Factors for Forecasting</title>\n",
      "    <updated>2020-10-14T19:25:26Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.07373v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.07373v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep probabilistic forecasting techniques have recently been proposed for modeling large collections of time-series. However, these techniques explicitly assume either complete independence (local model) or complete dependence (global model) between time-series in the collection. This corresponds to the two extreme cases where every time-series is disconnected from every other time-series in the collection or likewise, that every time-series is related to every other time-series resulting in a completely connected graph. In this work, we propose a deep hybrid probabilistic graph-based forecasting framework called Graph Deep Factors (GraphDF) that goes beyond these two extremes by allowing nodes and their time-series to be connected to others in an arbitrary fashion. GraphDF is a hybrid forecasting framework that consists of a relational global and relational local model. In particular, we propose a relational global model that learns complex non-linear time-series patterns globally using the structure of the graph to improve both forecasting accuracy and computational efficiency. Similarly, instead of modeling every time-series independently, we learn a relational local model that not only considers its individual time-series but also the time-series of nodes that are connected in the graph. The experiments demonstrate the effectiveness of the proposed deep hybrid graph-based forecasting model compared to the state-of-the-art methods in terms of its forecasting accuracy, runtime, and scalability. Our case study reveals that GraphDF can successfully generate cloud usage forecasts and opportunistically schedule workloads to increase cloud cluster utilization by 47.5% on average.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-14T19:25:26Z</published>\n",
      "    <arxiv:comment>18 pages, 7 figures, submitted to MLSys 2021</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Hongjie Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ryan A. Rossi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kanak Mahadik</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sungchul Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hoda Eldardiry</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.07388v1</id>\n",
      "    <title>Interpretable Machine Learning with an Ensemble of Gradient Boosting Machines</title>\n",
      "    <updated>2020-10-14T20:18:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.07388v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.07388v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A method for the local and global interpretation of a black-box model on the basis of the well-known generalized additive models is proposed. It can be viewed as an extension or a modification of the algorithm using the neural additive model. The method is based on using an ensemble of gradient boosting machines (GBMs) such that each GBM is learned on a single feature and produces a shape function of the feature. The ensemble is composed as a weighted sum of separate GBMs resulting a weighted sum of shape functions which form the generalized additive model. GBMs are built in parallel using randomized decision trees of depth 1, which provide a very simple architecture. Weights of GBMs as well as features are computed in each iteration of boosting by using the Lasso method and then updated by means of a specific smoothing procedure. In contrast to the neural additive model, the method provides weights of features in the explicit form, and it is simply trained. A lot of numerical experiments with an algorithm implementing the proposed method on synthetic and real datasets demonstrate its efficiency and properties for local and global interpretation.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-14T20:18:40Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Andrei V. Konstantinov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lev V. Utkin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.07856v2</id>\n",
      "    <title>Bi-level Score Matching for Learning Energy-based Latent Variable Models</title>\n",
      "    <updated>2020-10-16T07:33:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.07856v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.07856v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Score matching (SM) provides a compelling approach to learn energy-based models (EBMs) by avoiding the calculation of partition function. However, it remains largely open to learn energy-based latent variable models (EBLVMs), except some special cases. This paper presents a bi-level score matching (BiSM) method to learn EBLVMs with general structures by reformulating SM as a bi-level optimization problem. The higher level introduces a variational posterior of the latent variables and optimizes a modified SM objective, and the lower level optimizes the variational posterior to fit the true posterior. To solve BiSM efficiently, we develop a stochastic optimization algorithm with gradient unrolling. Theoretically, we analyze the consistency of BiSM and the convergence of the stochastic algorithm. Empirically, we show the promise of BiSM in Gaussian restricted Boltzmann machines and highly nonstructural EBLVMs parameterized by deep convolutional neural networks. BiSM is comparable to the widely adopted contrastive divergence and SM methods when they are applicable; and can learn complex EBLVMs with intractable posteriors to generate natural images.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-15T16:24:04Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Fan Bao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chongxuan Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kun Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hang Su</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jun Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bo Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.07866v2</id>\n",
      "    <title>Double Robust Representation Learning for Counterfactual Prediction</title>\n",
      "    <updated>2020-10-16T21:32:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.07866v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.07866v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Causal inference, or counterfactual prediction, is central to decision making in healthcare, policy and social sciences. To de-bias causal estimators with high-dimensional data in observational studies, recent advances suggest the importance of combining machine learning models for both the propensity score and the outcome function. We propose a novel scalable method to learn double-robust representations for counterfactual predictions, leading to consistent causal estimation if the model for either the propensity score or the outcome, but not necessarily both, is correctly specified. Specifically, we use the entropy balancing method to learn the weights that minimize the Jensen-Shannon divergence of the representation between the treated and control groups, based on which we make robust and efficient counterfactual predictions for both individual and average treatment effects. We provide theoretical justifications for the proposed method. The algorithm shows competitive performance with the state-of-the-art on real world and synthetic data.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-15T16:39:26Z</published>\n",
      "    <arxiv:comment>18 pages, 5 figures, 2 Tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Shuxi Zeng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Serge Assaad</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chenyang Tao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shounak Datta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lawrence Carin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fan Li</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.07891v2</id>\n",
      "    <title>Improving Natural Language Processing Tasks with Human Gaze-Guided Neural Attention</title>\n",
      "    <updated>2020-10-27T16:16:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.07891v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.07891v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A lack of corpora has so far limited advances in integrating human gaze data as a supervisory signal in neural attention mechanisms for natural language processing(NLP). We propose a novel hybrid text saliency model(TSM) that, for the first time, combines a cognitive model of reading with explicit human gaze supervision in a single machine learning framework. On four different corpora we demonstrate that our hybrid TSM duration predictions are highly correlated with human gaze ground truth. We further propose a novel joint modeling approach to integrate TSM predictions into the attention layer of a network designed for a specific upstream NLP task without the need for any task-specific human gaze data. We demonstrate that our joint model outperforms the state of the art in paraphrase generation on the Quora Question Pairs corpus by more than 10% in BLEU-4 and achieves state of the art performance for sentence compression on the challenging Google Sentence Compression corpus. As such, our work introduces a practical approach for bridging between data-driven and cognitive models and demonstrates a new way to integrate human gaze-guided neural attention into NLP tasks.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-15T17:14:09Z</published>\n",
      "    <arxiv:comment>NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Ekta Sood</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Simon Tannert</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Philipp Mueller</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andreas Bulling</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.07910v1</id>\n",
      "    <title>Marginal Contribution Feature Importance -- an Axiomatic Approach for The Natural Case</title>\n",
      "    <updated>2020-10-15T17:41:42Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.07910v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.07910v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>When training a predictive model over medical data, the goal is sometimes to gain insights about a certain disease. In such cases, it is common to use feature importance as a tool to highlight significant factors contributing to that disease. As there are many existing methods for computing feature importance scores, understanding their relative merits is not trivial. Further, the diversity of scenarios in which they are used lead to different expectations from the feature importance scores. While it is common to make the distinction between local scores that focus on individual predictions and global scores that look at the contribution of a feature to the model, another important division distinguishes model scenarios, in which the goal is to understand predictions of a given model from natural scenarios, in which the goal is to understand a phenomenon such as a disease. We develop a set of axioms that represent the properties expected from a feature importance function in the natural scenario and prove that there exists only one function that satisfies all of them, the Marginal Contribution Feature Importance (MCI). We analyze this function for its theoretical and empirical properties and compare it to other feature importance scores. While our focus is the natural scenario, we suggest that our axiomatic approach could be carried out in other scenarios too.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-15T17:41:42Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Amnon Catav</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Boyang Fu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jason Ernst</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sriram Sankararaman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ran Gilad-Bachrach</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.07935v1</id>\n",
      "    <title>Multi-Agent Motion Planning using Deep Learning for Space Applications</title>\n",
      "    <updated>2020-10-15T06:42:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.07935v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.07935v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>State-of-the-art motion planners cannot scale to a large number of systems. Motion planning for multiple agents is an NP (non-deterministic polynomial-time) hard problem, so the computation time increases exponentially with each addition of agents. This computational demand is a major stumbling block to the motion planner's application to future NASA missions involving the swarm of space vehicles. We applied a deep neural network to transform computationally demanding mathematical motion planning problems into deep learning-based numerical problems. We showed optimal motion trajectories can be accurately replicated using deep learning-based numerical models in several 2D and 3D systems with multiple agents. The deep learning-based numerical model demonstrates superior computational efficiency with plans generated 1000 times faster than the mathematical model counterpart.</summary>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-15T06:42:47Z</published>\n",
      "    <arxiv:comment>2020 AIAA ASCEND</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.RO\"/>\n",
      "    <author>\n",
      "      <name>Kyongsik Yun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Changrak Choi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ryan Alimo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anthony Davis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Linda Forster</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amir Rahmani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Muhammad Adil</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ramtin Madani</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.08034v1</id>\n",
      "    <title>Overfitting or Underfitting? Understand Robustness Drop in Adversarial Training</title>\n",
      "    <updated>2020-10-15T21:43:07Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.08034v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.08034v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Our goal is to understand why the robustness drops after conducting adversarial training for too long. Although this phenomenon is commonly explained as overfitting, our analysis suggest that its primary cause is perturbation underfitting. We observe that after training for too long, FGSM-generated perturbations deteriorate into random noise. Intuitively, since no parameter updates are made to strengthen the perturbation generator, once this process collapses, it could be trapped in such local optima. Also, sophisticating this process could mostly avoid the robustness drop, which supports that this phenomenon is caused by underfitting instead of overfitting. In the light of our analyses, we propose APART, an adaptive adversarial training framework, which parameterizes perturbation generation and progressively strengthens them. Shielding perturbations from underfitting unleashes the potential of our framework. In our experiments, APART provides comparable or even better robustness than PGD-10, with only about 1/4 of its computational cost.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-15T21:43:07Z</published>\n",
      "    <arxiv:comment>Work in Progress</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Zichao Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liyuan Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chengyu Dong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jingbo Shang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06651v1</id>\n",
      "    <title>Higher-Order Certification for Randomized Smoothing</title>\n",
      "    <updated>2020-10-13T19:35:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06651v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06651v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Randomized smoothing is a recently proposed defense against adversarial attacks that has achieved SOTA provable robustness against $\\ell_2$ perturbations. A number of publications have extended the guarantees to other metrics, such as $\\ell_1$ or $\\ell_\\infty$, by using different smoothing measures. Although the current framework has been shown to yield near-optimal $\\ell_p$ radii, the total safety region certified by the current framework can be arbitrarily small compared to the optimal. In this work, we propose a framework to improve the certified safety region for these smoothed classifiers without changing the underlying smoothing scheme. The theoretical contributions are as follows: 1) We generalize the certification for randomized smoothing by reformulating certified radius calculation as a nested optimization problem over a class of functions. 2) We provide a method to calculate the certified safety region using $0^{th}$-order and $1^{st}$-order information for Gaussian-smoothed classifiers. We also provide a framework that generalizes the calculation for certification using higher-order information. 3) We design efficient, high-confidence estimators for the relevant statistics of the first-order information. Combining the theoretical contribution 2) and 3) allows us to certify safety region that are significantly larger than the ones provided by the current methods. On CIFAR10 and Imagenet datasets, the new regions certified by our approach achieve significant improvements on general $\\ell_1$ certified radii and on the $\\ell_2$ certified radii for color-space attacks ($\\ell_2$ restricted to 1 channel) while also achieving smaller improvements on the general $\\ell_2$ certified radii. Our framework can also provide a way to circumvent the current impossibility results on achieving higher magnitude of certified radii without requiring the use of data-dependent smoothing techniques.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T19:35:48Z</published>\n",
      "    <arxiv:comment>Accepted to NeurIPS2020(spotlight)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jeet Mohapatra</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ching-Yun Ko</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tsui-Wei Weng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pin-Yu Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sijia Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luca Daniel</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06663v1</id>\n",
      "    <title>Intrapersonal Parameter Optimization for Offline Handwritten Signature Augmentation</title>\n",
      "    <updated>2020-10-13T19:54:02Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06663v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06663v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Usually, in a real-world scenario, few signature samples are available to train an automatic signature verification system (ASVS). However, such systems do indeed need a lot of signatures to achieve an acceptable performance. Neuromotor signature duplication methods and feature space augmentation methods may be used to meet the need for an increase in the number of samples. Such techniques manually or empirically define a set of parameters to introduce a degree of writer variability. Therefore, in the present study, a method to automatically model the most common writer variability traits is proposed. The method is used to generate offline signatures in the image and the feature space and train an ASVS. We also introduce an alternative approach to evaluate the quality of samples considering their feature vectors. We evaluated the performance of an ASVS with the generated samples using three well-known offline signature datasets: GPDS, MCYT-75, and CEDAR. In GPDS-300, when the SVM classifier was trained using one genuine signature per writer and the duplicates generated in the image space, the Equal Error Rate (EER) decreased from 5.71% to 1.08%. Under the same conditions, the EER decreased to 1.04% using the feature space augmentation technique. We also verified that the model that generates duplicates in the image space reproduces the most common writer variability traits in the three different datasets.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T19:54:02Z</published>\n",
      "    <arxiv:comment>16 pages, 11 figures, To appear in the IEEE Transactions on Information Forensics &amp; Security</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Teruo M. Maruyama</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luiz S. Oliveira</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alceu S. Britto</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Robert Sabourin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06684v2</id>\n",
      "    <title>Motif Learning in Knowledge Graphs Using Trajectories Of Differential Equations</title>\n",
      "    <updated>2020-10-18T18:31:11Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06684v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06684v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Knowledge Graph Embeddings (KGEs) have shown promising performance on link prediction tasks by mapping the entities and relations from a knowledge graph into a geometric space (usually a vector space). Ultimately, the plausibility of the predicted links is measured by using a scoring function over the learned embeddings (vectors). Therefore, the capability in preserving graph characteristics including structural aspects and semantics highly depends on the design of the KGE, as well as the inherited abilities from the underlying geometry. Many KGEs use the flat geometry which renders them incapable of preserving complex structures and consequently causes wrong inferences by the models. To address this problem, we propose a neuro differential KGE that embeds nodes of a KG on the trajectories of Ordinary Differential Equations (ODEs). To this end, we represent each relation (edge) in a KG as a vector field on a smooth Riemannian manifold. We specifically parameterize ODEs by a neural network to represent various complex shape manifolds and more importantly complex shape vector fields on the manifold. Therefore, the underlying embedding space is capable of getting various geometric forms to encode complexity in subgraph structures with different motifs. Experiments on synthetic and benchmark dataset as well as social network KGs justify the ODE trajectories as a means to structure preservation and consequently avoiding wrong inferences over state-of-the-art KGE models.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T20:53:17Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Mojtaba Nayyeri</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chengjin Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jens Lehmann</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sahar Vahdati</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06705v1</id>\n",
      "    <title>Weakly-Supervised Aspect-Based Sentiment Analysis via Joint Aspect-Sentiment Topic Embedding</title>\n",
      "    <updated>2020-10-13T21:33:24Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06705v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06705v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner. It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspect-based reviews by sentiment polarity. In this paper, we propose a weakly-supervised approach for aspect-based sentiment analysis, which uses only a few keywords describing each aspect/sentiment without using any labeled examples. Existing methods are either designed only for one of the sub-tasks, neglecting the benefit of coupling both, or are based on topic models that may contain overlapping concepts. We propose to first learn &lt;sentiment, aspect&gt; joint topic embeddings in the word embedding space by imposing regularizations to encourage topic distinctiveness, and then use neural models to generalize the word-level discriminative information by pre-training the classifiers with embedding-based predictions and self-training them on unlabeled data. Our comprehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4% and 5.1% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets. Our code and data are available at https://github.com/teapot123/JASen.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T21:33:24Z</published>\n",
      "    <arxiv:comment>accepted to EMNLP 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Jiaxin Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu Meng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fang Guo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Heng Ji</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiawei Han</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.07754v1</id>\n",
      "    <title>EnCoD: Distinguishing Compressed and Encrypted File Fragments</title>\n",
      "    <updated>2020-10-15T13:55:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.07754v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.07754v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Reliable identification of encrypted file fragments is a requirement for several security applications, including ransomware detection, digital forensics, and traffic analysis. A popular approach consists of estimating high entropy as a proxy for randomness. However, many modern content types (e.g. office documents, media files, etc.) are highly compressed for storage and transmission efficiency. Compression algorithms also output high-entropy data, thus reducing the accuracy of entropy-based encryption detectors. Over the years, a variety of approaches have been proposed to distinguish encrypted file fragments from high-entropy compressed fragments. However, these approaches are typically only evaluated over a few, select data types and fragment sizes, which makes a fair assessment of their practical applicability impossible. This paper aims to close this gap by comparing existing statistical tests on a large, standardized dataset. Our results show that current approaches cannot reliably tell apart encryption and compression, even for large fragment sizes. To address this issue, we design EnCoD, a learning-based classifier which can reliably distinguish compressed and encrypted data, starting with fragments as small as 512 bytes. We evaluate EnCoD against current approaches over a large dataset of different data types, showing that it outperforms current state-of-the-art for most considered fragment sizes and data types.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-15T13:55:55Z</published>\n",
      "    <arxiv:comment>19 pages, 6 images, 2 tables. Accepted for publication at the 14th International Conference on Network and System Security (NSS2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Fabio De Gaspari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dorjan Hitaj</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Giulio Pagnotta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lorenzo De Carli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luigi V. Mancini</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.07808v1</id>\n",
      "    <title>Federated Learning in Adversarial Settings</title>\n",
      "    <updated>2020-10-15T14:57:02Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.07808v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.07808v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Federated Learning enables entities to collaboratively learn a shared prediction model while keeping their training data locally. It prevents data collection and aggregation and, therefore, mitigates the associated privacy risks. However, it still remains vulnerable to various security attacks where malicious participants aim at degrading the generated model, inserting backdoors, or inferring other participants' training data. This paper presents a new federated learning scheme that provides different trade-offs between robustness, privacy, bandwidth efficiency, and model accuracy. Our scheme uses biased quantization of model updates and hence is bandwidth efficient. It is also robust against state-of-the-art backdoor as well as model degradation attacks even when a large proportion of the participant nodes are malicious. We propose a practical differentially private extension of this scheme which protects the whole dataset of participating entities. We show that this extension performs as efficiently as the non-private but robust scheme, even with stringent privacy requirements but are less robust against model degradation and backdoor attacks. This suggests a possible fundamental trade-off between Differential Privacy and robustness.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-15T14:57:02Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Raouf Kerkouche</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gergely Ács</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Claude Castelluccia</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.07054v1</id>\n",
      "    <title>Representativity Fairness in Clustering</title>\n",
      "    <updated>2020-10-11T21:50:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.07054v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.07054v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Incorporating fairness constructs into machine learning algorithms is a topic of much societal importance and recent interest. Clustering, a fundamental task in unsupervised learning that manifests across a number of web data scenarios, has also been subject of attention within fair ML research. In this paper, we develop a novel notion of fairness in clustering, called representativity fairness. Representativity fairness is motivated by the need to alleviate disparity across objects' proximity to their assigned cluster representatives, to aid fairer decision making. We illustrate the importance of representativity fairness in real-world decision making scenarios involving clustering and provide ways of quantifying objects' representativity and fairness over it. We develop a new clustering formulation, RFKM, that targets to optimize for representativity fairness along with clustering quality. Inspired by the $K$-Means framework, RFKM incorporates novel loss terms to formulate an objective function. The RFKM objective and optimization approach guides it towards clustering configurations that yield higher representativity fairness. Through an empirical evaluation over a variety of public datasets, we establish the effectiveness of our method. We illustrate that we are able to significantly improve representativity fairness at only marginal impact to clustering quality.</summary>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-11T21:50:06Z</published>\n",
      "    <arxiv:comment>In 12th ACM Web Science Conference (WebSci 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CY\"/>\n",
      "    <author>\n",
      "      <name>Deepak P</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Savitha Sam Abraham</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3394231.3397910</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3394231.3397910\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.06890v1</id>\n",
      "    <title>Identifying Wrongly Predicted Samples: A Method for Active Learning</title>\n",
      "    <updated>2020-10-14T09:00:42Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.06890v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.06890v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>State-of-the-art machine learning models require access to significant amount of annotated data in order to achieve the desired level of performance. While unlabelled data can be largely available and even abundant, annotation process can be quite expensive and limiting. Under the assumption that some samples are more important for a given task than others, active learning targets the problem of identifying the most informative samples that one should acquire annotations for. Instead of the conventional reliance on model uncertainty as a proxy to leverage new unknown labels, in this work we propose a simple sample selection criterion that moves beyond uncertainty. By first accepting the model prediction and then judging its effect on the generalization error, we can better identify wrongly predicted samples. We further present an approximation to our criterion that is very efficient and provides a similarity based interpretation. In addition to evaluating our method on the standard benchmarks of active learning, we consider the challenging yet realistic scenario of imbalanced data where categories are not equally represented. We show state-of-the-art results and better rates at identifying wrongly predicted samples. Our method is simple, model agnostic and relies on the current model status without the need for re-training from scratch.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-14T09:00:42Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Rahaf Aljundi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nikolay Chumerin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniel Olmeda Reino</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.07024v1</id>\n",
      "    <title>STP-UDGAT: Spatial-Temporal-Preference User Dimensional Graph Attention Network for Next POI Recommendation</title>\n",
      "    <updated>2020-10-06T04:03:42Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.07024v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.07024v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Next Point-of-Interest (POI) recommendation is a longstanding problem across the domains of Location-Based Social Networks (LBSN) and transportation. Recent Recurrent Neural Network (RNN) based approaches learn POI-POI relationships in a local view based on independent user visit sequences. This limits the model's ability to directly connect and learn across users in a global view to recommend semantically trained POIs. In this work, we propose a Spatial-Temporal-Preference User Dimensional Graph Attention Network (STP-UDGAT), a novel explore-exploit model that concurrently exploits personalized user preferences and explores new POIs in global spatial-temporal-preference (STP) neighbourhoods, while allowing users to selectively learn from other users. In addition, we propose random walks as a masked self-attention option to leverage the STP graphs' structures and find new higher-order POI neighbours during exploration. Experimental results on six real-world datasets show that our model significantly outperforms baseline and state-of-the-art methods.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T04:03:42Z</published>\n",
      "    <arxiv:comment>To appear in Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM), 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Nicholas Lim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bryan Hooi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>See-Kiong Ng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xueou Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yong Liang Goh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Renrong Weng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jagannadan Varadarajan</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1145/3340531.3411876</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1145/3340531.3411876\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.11926v1</id>\n",
      "    <title>Neural-Symbolic Integration: A Compositional Perspective</title>\n",
      "    <updated>2020-10-22T17:55:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.11926v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.11926v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Despite significant progress in the development of neural-symbolic frameworks, the question of how to integrate a neural and a symbolic system in a \\emph{compositional} manner remains open. Our work seeks to fill this gap by treating these two systems as black boxes to be integrated as modules into a single architecture, without making assumptions on their internal structure and semantics. Instead, we expect only that each module exposes certain methods for accessing the functions that the module implements: the symbolic module exposes a deduction method for computing the function's output on a given input, and an abduction method for computing the function's inputs for a given output; the neural module exposes a deduction method for computing the function's output on a given input, and an induction method for updating the function given input-output training instances. We are, then, able to show that a symbolic module -- with any choice for syntax and semantics, as long as the deduction and abduction methods are exposed -- can be cleanly integrated with a neural module, and facilitate the latter's efficient training, achieving empirical performance that exceeds that of previous work.</summary>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-22T17:55:44Z</published>\n",
      "    <arxiv:primary_category term=\"cs.AI\"/>\n",
      "    <author>\n",
      "      <name>Efthymia Tsamoura</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Loizos Michael</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.11940v1</id>\n",
      "    <title>Motion Planner Augmented Reinforcement Learning for Robot Manipulation in Obstructed Environments</title>\n",
      "    <updated>2020-10-22T17:59:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.11940v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.11940v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep reinforcement learning (RL) agents are able to learn contact-rich manipulation tasks by maximizing a reward signal, but require large amounts of experience, especially in environments with many obstacles that complicate exploration. In contrast, motion planners use explicit models of the agent and environment to plan collision-free paths to faraway goals, but suffer from inaccurate models in tasks that require contacts with the environment. To combine the benefits of both approaches, we propose motion planner augmented RL (MoPA-RL) which augments the action space of an RL agent with the long-horizon planning capabilities of motion planners. Based on the magnitude of the action, our approach smoothly transitions between directly executing the action and invoking a motion planner. We evaluate our approach on various simulated manipulation tasks and compare it to alternative action spaces in terms of learning efficiency and safety. The experiments demonstrate that MoPA-RL increases learning efficiency, leads to a faster exploration, and results in safer policies that avoid collisions with the environment. Videos and code are available at https://clvrai.com/mopa-rl .</summary>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-22T17:59:09Z</published>\n",
      "    <arxiv:comment>Published at the Conference on Robot Learning (CoRL) 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.RO\"/>\n",
      "    <author>\n",
      "      <name>Jun Yamada</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Youngwoon Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gautam Salhotra</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Karl Pertsch</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Max Pflueger</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gaurav S. Sukhatme</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joseph J. Lim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peter Englert</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.11980v1</id>\n",
      "    <title>A Joint Learning Approach based on Self-Distillation for Keyphrase Extraction from Scientific Documents</title>\n",
      "    <updated>2020-10-22T18:36:31Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.11980v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.11980v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Keyphrase extraction is the task of extracting a small set of phrases that best describe a document. Most existing benchmark datasets for the task typically have limited numbers of annotated documents, making it challenging to train increasingly complex neural networks. In contrast, digital libraries store millions of scientific articles online, covering a wide range of topics. While a significant portion of these articles contain keyphrases provided by their authors, most other articles lack such kind of annotations. Therefore, to effectively utilize these large amounts of unlabeled articles, we propose a simple and efficient joint learning approach based on the idea of self-distillation. Experimental results show that our approach consistently improves the performance of baseline models for keyphrase extraction. Furthermore, our best models outperform previous methods for the task, achieving new state-of-the-art results on two public benchmarks: Inspec and SemEval-2017.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-22T18:36:31Z</published>\n",
      "    <arxiv:comment>Accepted to COLING 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Tuan Manh Lai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Trung Bui</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Doo Soon Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Quan Hung Tran</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12023v1</id>\n",
      "    <title>Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection</title>\n",
      "    <updated>2020-10-22T20:13:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12023v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12023v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Weakly Supervised Object Detection (WSOD) has emerged as an effective tool to train object detectors using only the image-level category labels. However, without object-level labels, WSOD detectors are prone to detect bounding boxes on salient objects, clustered objects and discriminative object parts. Moreover, the image-level category labels do not enforce consistent object detection across different transformations of the same images. To address the above issues, we propose a Comprehensive Attention Self-Distillation (CASD) training approach for WSOD. To balance feature learning among all object instances, CASD computes the comprehensive attention aggregated from multiple transformations and feature layers of the same images. To enforce consistent spatial supervision on objects, CASD conducts self-distillation on the WSOD networks, such that the comprehensive attention is approximated simultaneously by multiple transformations and feature layers of the same images. CASD produces new state-of-the-art WSOD results on standard benchmarks such as PASCAL VOC 2007/2012 and MS-COCO.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-22T20:13:32Z</published>\n",
      "    <arxiv:comment>Neural Information Processing Systems (NeurIPS 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Zeyi Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yang Zou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vijayakumar Bhagavatula</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dong Huang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12065v1</id>\n",
      "    <title>A generalized deep learning model for multi-disease Chest X-Ray diagnostics</title>\n",
      "    <updated>2020-10-17T18:57:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12065v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12065v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We investigate the generalizability of deep convolutional neural network (CNN) on the task of disease classification from chest x-rays collected over multiple sites. We systematically train the model using datasets from three independent sites with different patient populations: National Institute of Health (NIH), Stanford University Medical Centre (CheXpert), and Shifa International Hospital (SIH). We formulate a sequential training approach and demonstrate that the model produces generalized prediction performance using held out test sets from the three sites. Our model generalizes better when trained on multiple datasets, with the CheXpert-Shifa-NET model performing significantly better (p-values &lt; 0.05) than the models trained on individual datasets for 3 out of the 4 distinct disease classes. The code for training the model will be made available open source at: www.github.com/link-to-code at the time of publication.</summary>\n",
      "    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-17T18:57:40Z</published>\n",
      "    <arxiv:primary_category term=\"q-bio.QM\"/>\n",
      "    <author>\n",
      "      <name>Nabit Bajwa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kedar Bajwa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Atif Rana</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>M. Faique Shakeel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kashif Haqqi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Suleiman Ali Khan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12089v1</id>\n",
      "    <title>The Pursuit of Algorithmic Fairness: On \"Correcting\" Algorithmic Unfairness in a Child Welfare Reunification Success Classifier</title>\n",
      "    <updated>2020-10-22T22:07:43Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12089v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12089v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The algorithmic fairness of predictive analytic tools in the public sector has increasingly become a topic of rigorous exploration. While instruments pertaining to criminal recidivism and academic admissions, for example, have garnered much attention, the predictive instruments of Child Welfare jurisdictions have received considerably less attention. This is in part because comparatively few such instruments exist and because even fewer have been scrutinized through the lens of algorithmic fairness. In this work, we seek to address both of these gaps. To this end, a novel classification algorithm for predicting reunification success within Oregon Child Welfare is presented, including all of the relevant details associated with building such an instrument. The purpose of this tool is to maximize the number of stable reunifications and identify potentially unstable reunifications which may require additional resources and scrutiny. Additionally, because the algorithmic fairness of the resulting tool, if left unaltered, is unquestionably lacking, the utilized procedure for mitigating such unfairness is presented, along with the rationale behind each difficult and unavoidable choice. This procedure, though similar to other post-processing group-specific thresholding methods, is novel in its use of a penalized optimizer and contextually requisite subsampling. These novel methodological components yield a rich and informative empirical understanding of the trade-off continuum between fairness and accuracy. As the developed procedure is generalizable across a variety of group-level definitions of algorithmic fairness, as well as across an arbitrary number of protected attribute levels and risk thresholds, the approach is broadly applicable both within and beyond Child Welfare.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-22T22:07:43Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jordan Purdy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Brian Glass</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.11805v1</id>\n",
      "    <title>Urban Sound Classification : striving towards a fair comparison</title>\n",
      "    <updated>2020-10-22T15:37:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.11805v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.11805v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Urban sound classification has been achieving remarkable progress and is still an active research area in audio pattern recognition. In particular, it allows to monitor the noise pollution, which becomes a growing concern for large cities. The contribution of this paper is two-fold. First, we present our DCASE 2020 task 5 winning solution which aims at helping the monitoring of urban noise pollution. It achieves a macro-AUPRC of 0.82 / 0.62 for the coarse / fine classification on validation set. Moreover, it reaches accuracies of 89.7% and 85.41% respectively on ESC-50 and US8k datasets. Second, it is not easy to find a fair comparison and to reproduce the performance of existing models. Sometimes authors copy-pasting the results of the original papers which is not helping reproducibility. As a result, we provide a fair comparison by using the same input representation, metrics and optimizer to assess performances. We preserve data augmentation used by the original papers. We hope this framework could help evaluate new architectures in this field. For better reproducibility, the code is available on our GitHub repository.</summary>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-22T15:37:39Z</published>\n",
      "    <arxiv:comment>7 pages, 1 figure</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.SD\"/>\n",
      "    <author>\n",
      "      <name>Augustin Arnault</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Baptiste Hanssens</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nicolas Riche</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.11828v2</id>\n",
      "    <title>Once-for-All Adversarial Training: In-Situ Tradeoff between Robustness and Accuracy for Free</title>\n",
      "    <updated>2020-11-10T08:18:58Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.11828v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.11828v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Adversarial training and its many variants substantially improve deep network robustness, yet at the cost of compromising standard accuracy. Moreover, the training process is heavy and hence it becomes impractical to thoroughly explore the trade-off between accuracy and robustness. This paper asks this new question: how to quickly calibrate a trained model in-situ, to examine the achievable trade-offs between its standard and robust accuracies, without (re-)training it many times? Our proposed framework, Once-for-all Adversarial Training (OAT), is built on an innovative model-conditional training framework, with a controlling hyper-parameter as the input. The trained model could be adjusted among different standard and robust accuracies \"for free\" at testing time. As an important knob, we exploit dual batch normalization to separate standard and adversarial feature statistics, so that they can be learned in one model without degrading performance. We further extend OAT to a Once-for-all Adversarial Training and Slimming (OATS) framework, that allows for the joint trade-off among accuracy, robustness and runtime efficiency. Experiments show that, without any re-training nor ensembling, OAT/OATS achieve similar or even superior performance compared to dedicatedly trained models at various configurations. Our codes and pretrained models are available at: https://github.com/VITA-Group/Once-for-All-Adversarial-Training.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-22T16:06:34Z</published>\n",
      "    <arxiv:comment>NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Haotao Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tianlong Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shupeng Gui</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ting-Kuei Hu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ji Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhangyang Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.11863v1</id>\n",
      "    <title>Planning with Submodular Objective Functions</title>\n",
      "    <updated>2020-10-22T16:55:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.11863v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.11863v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study planning with submodular objective functions, where instead of maximizing the cumulative reward, the goal is to maximize the objective value induced by a submodular function. Our framework subsumes standard planning and submodular maximization with cardinality constraints as special cases, and thus many practical applications can be naturally formulated within our framework. Based on the notion of multilinear extension, we propose a novel and theoretically principled algorithmic framework for planning with submodular objective functions, which recovers classical algorithms when applied to the two special cases mentioned above. Empirically, our approach significantly outperforms baseline algorithms on synthetic environments and navigation tasks.</summary>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-22T16:55:12Z</published>\n",
      "    <arxiv:primary_category term=\"cs.AI\"/>\n",
      "    <author>\n",
      "      <name>Ruosong Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hanrui Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Devendra Singh Chaplot</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Denis Garagić</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruslan Salakhutdinov</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12470v1</id>\n",
      "    <title>A Practical Guide of Off-Policy Evaluation for Bandit Problems</title>\n",
      "    <updated>2020-10-23T15:11:19Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12470v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12470v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Off-policy evaluation (OPE) is the problem of estimating the value of a target policy from samples obtained via different policies. Recently, applying OPE methods for bandit problems has garnered attention. For the theoretical guarantees of an estimator of the policy value, the OPE methods require various conditions on the target policy and policy used for generating the samples. However, existing studies did not carefully discuss the practical situation where such conditions hold, and the gap between them remains. This paper aims to show new results for bridging the gap. Based on the properties of the evaluation policy, we categorize OPE situations. Then, among practical applications, we mainly discuss the best policy selection. For the situation, we propose a meta-algorithm based on existing OPE estimators. We investigate the proposed concepts using synthetic and open real-world datasets in experiments.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"econ.EM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-23T15:11:19Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Masahiro Kato</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kenshi Abe</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kaito Ariu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shota Yasui</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12493v2</id>\n",
      "    <title>A Review of Deep Learning Methods for Irregularly Sampled Medical Time Series Data</title>\n",
      "    <updated>2020-10-26T04:51:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12493v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12493v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Irregularly sampled time series (ISTS) data has irregular temporal intervals between observations and different sampling rates between sequences. ISTS commonly appears in healthcare, economics, and geoscience. Especially in the medical environment, the widely used Electronic Health Records (EHRs) have abundant typical irregularly sampled medical time series (ISMTS) data. Developing deep learning methods on EHRs data is critical for personalized treatment, precise diagnosis and medical management. However, it is challenging to directly use deep learning models for ISMTS data. On the one hand, ISMTS data has the intra-series and inter-series relations. Both the local and global structures should be considered. On the other hand, methods should consider the trade-off between task accuracy and model complexity and remain generality and interpretability. So far, many existing works have tried to solve the above problems and have achieved good results. In this paper, we review these deep learning methods from the perspectives of technology and task. Under the technology-driven perspective, we summarize them into two categories - missing data-based methods and raw data-based methods. Under the task-driven perspective, we also summarize them into two categories - data imputation-oriented and downstream task-oriented. For each of them, we point out their advantages and disadvantages. Moreover, we implement some representative methods and compare them on four medical datasets with two tasks. Finally, we discuss the challenges and opportunities in this area.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-23T15:51:23Z</published>\n",
      "    <arxiv:comment>19 pages, 7 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Chenxi Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shenda Hong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Moxian Song</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hongyan Li</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12533v1</id>\n",
      "    <title>Legal Document Classification: An Application to Law Area Prediction of Petitions to Public Prosecution Service</title>\n",
      "    <updated>2020-10-13T18:05:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12533v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12533v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In recent years, there has been an increased interest in the application of Natural Language Processing (NLP) to legal documents. The use of convolutional and recurrent neural networks along with word embedding techniques have presented promising results when applied to textual classification problems, such as sentiment analysis and topic segmentation of documents. This paper proposes the use of NLP techniques for textual classification, with the purpose of categorizing the descriptions of the services provided by the Public Prosecutor's Office of the State of Paraná to the population in one of the areas of law covered by the institution. Our main goal is to automate the process of assigning petitions to their respective areas of law, with a consequent reduction in costs and time associated with such process while allowing the allocation of human resources to more complex tasks. In this paper, we compare different approaches to word representations in the aforementioned task: including document-term matrices and a few different word embeddings. With regards to the classification models, we evaluated three different families: linear models, boosted trees and neural networks. The best results were obtained with a combination of Word2Vec trained on a domain-specific corpus and a Recurrent Neural Network (RNN) architecture (more specifically, LSTM), leading to an accuracy of 90\\% and F1-Score of 85\\% in the classification of eighteen categories (law areas).</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-13T18:05:37Z</published>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Mariana Y. Noguti</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eduardo Vellasques</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luiz S. Oliveira</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/IJCNN48605.2020.9207211</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/IJCNN48605.2020.9207211\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12546v1</id>\n",
      "    <title>Quantizing Multiple Sources to a Common Cluster Center: An Asymptotic Analysis</title>\n",
      "    <updated>2020-10-23T17:14:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12546v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12546v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We consider quantizing an $Ld$-dimensional sample, which is obtained by concatenating $L$ vectors from datasets of $d$-dimensional vectors, to a $d$-dimensional cluster center. The distortion measure is the weighted sum of $r$th powers of the distances between the cluster center and the samples. For $L=1$, one recovers the ordinary center based clustering formulation. The general case $L&gt;1$ appears when one wishes to cluster a dataset through $L$ noisy observations of each of its members. We find a formula for the average distortion performance in the asymptotic regime where the number of cluster centers are large. We also provide an algorithm to numerically optimize the cluster centers and verify our analytical results on real and artificial datasets. In terms of faithfulness to the original (noiseless) dataset, our clustering approach outperforms the naive approach that relies on quantizing the $Ld$-dimensional noisy observation vectors to $Ld$-dimensional centers.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-23T17:14:28Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Erdem Koyuncu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12650v1</id>\n",
      "    <title>A Study of Transfer Learning in Music Source Separation</title>\n",
      "    <updated>2020-10-23T20:29:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12650v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12650v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Supervised deep learning methods for performing audio source separation can be very effective in domains where there is a large amount of training data. While some music domains have enough data suitable for training a separation system, such as rock and pop genres, many musical domains do not, such as classical music, choral music, and non-Western music traditions. It is well known that transferring learning from related domains can result in a performance boost for deep learning systems, but it is not always clear how best to do pretraining. In this work we investigate the effectiveness of data augmentation during pretraining, the impact on performance as a result of pretraining and downstream datasets having similar content domains, and also explore how much of a model must be retrained on the final target task, once pretrained.</summary>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-23T20:29:47Z</published>\n",
      "    <arxiv:comment>4 pages + 1 reference page. 3 figures. Submitted to ICASSP</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.SD\"/>\n",
      "    <author>\n",
      "      <name>Andreas Bugler</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bryan Pardo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Prem Seetharaman</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12777v1</id>\n",
      "    <title>Improving Multilingual Models with Language-Clustered Vocabularies</title>\n",
      "    <updated>2020-10-24T04:49:15Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12777v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12777v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applications. In this work, we introduce a novel procedure for multilingual vocabulary generation that combines the separately trained vocabularies of several automatically derived language clusters, thus balancing the trade-off between cross-lingual subword sharing and language-specific vocabularies. Our experiments show improvements across languages on key multilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1\\%), and WikiAnn NER (+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without increasing the size of the model or data.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-24T04:49:15Z</published>\n",
      "    <arxiv:comment>Published in the main conference of EMNLP 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Hyung Won Chung</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dan Garrette</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kiat Chuan Tan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jason Riesa</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.13058v2</id>\n",
      "    <title>Adaptive Federated Learning and Digital Twin for Industrial Internet of Things</title>\n",
      "    <updated>2020-11-01T02:53:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.13058v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.13058v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Industrial Internet of Things (IoT) enables distributed intelligent services varying with the dynamic and realtime industrial devices to achieve Industry 4.0 benefits. In this paper, we consider a new architecture of digital twin empowered Industrial IoT where digital twins capture the characteristics of industrial devices to assist federated learning. Noticing that digital twins may bring estimation deviations from the actual value of device state, a trusted based aggregation is proposed in federated learning to alleviate the effects of such deviation. We adaptively adjust the aggregation frequency of federated learning based on Lyapunov dynamic deficit queue and deep reinforcement learning, to improve the learning performance under the resource constraints. To further adapt to the heterogeneity of Industrial IoT, a clustering-based asynchronous federated learning framework is proposed. Numerical results show that the proposed framework is superior to the benchmark in terms of learning accuracy, convergence, and energy saving.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-25T07:59:17Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Wen Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shiyu Lei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lu Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhiqiang Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yan Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.13059v1</id>\n",
      "    <title>A QP-adaptive Mechanism for CNN-based Filter in Video Coding</title>\n",
      "    <updated>2020-10-25T08:02:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.13059v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.13059v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Convolutional neural network (CNN)-based filters have achieved great success in video coding. However, in most previous works, individual models are needed for each quantization parameter (QP) band. This paper presents a generic method to help an arbitrary CNN-filter handle different quantization noise. We model the quantization noise problem and implement a feasible solution on CNN, which introduces the quantization step (Qstep) into the convolution. When the quantization noise increases, the ability of the CNN-filter to suppress noise improves accordingly. This method can be used directly to replace the (vanilla) convolution layer in any existing CNN-filters. By using only 25% of the parameters, the proposed method achieves better performance than using multiple models with VTM-6.3 anchor. Besides, an additional BD-rate reduction of 0.2% is achieved by our proposed method for chroma components.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-25T08:02:38Z</published>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Chao Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Heming Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiro Katto</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaoyang Zeng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yibo Fan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.13100v1</id>\n",
      "    <title>Tensor Casting: Co-Designing Algorithm-Architecture for Personalized Recommendation Training</title>\n",
      "    <updated>2020-10-25T12:04:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.13100v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.13100v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Personalized recommendations are one of the most widely deployed machine learning (ML) workload serviced from cloud datacenters. As such, architectural solutions for high-performance recommendation inference have recently been the target of several prior literatures. Unfortunately, little have been explored and understood regarding the training side of this emerging ML workload. In this paper, we first perform a detailed workload characterization study on training recommendations, root-causing sparse embedding layer training as one of the most significant performance bottlenecks. We then propose our algorithm-architecture co-design called Tensor Casting, which enables the development of a generic accelerator architecture for tensor gather-scatter that encompasses all the key primitives of training embedding layers. When prototyped on a real CPU-GPU system, Tensor Casting provides 1.9-21x improvements in training throughput compared to state-of-the-art approaches.</summary>\n",
      "    <category term=\"cs.AR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-25T12:04:32Z</published>\n",
      "    <arxiv:primary_category term=\"cs.AR\"/>\n",
      "    <author>\n",
      "      <name>Youngeun Kwon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yunjae Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Minsoo Rhu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.13103v1</id>\n",
      "    <title>LazyBatching: An SLA-aware Batching System for Cloud Machine Learning Inference</title>\n",
      "    <updated>2020-10-25T12:13:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.13103v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.13103v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In cloud ML inference systems, batching is an essential technique to increase throughput which helps optimize total-cost-of-ownership. Prior graph batching combines the individual DNN graphs into a single one, allowing multiple inputs to be concurrently executed in parallel. We observe that the coarse-grained graph batching becomes suboptimal in effectively handling the dynamic inference request traffic, leaving significant performance left on the table. This paper proposes LazyBatching, an SLA-aware batching system that considers both scheduling and batching in the granularity of individual graph nodes, rather than the entire graph for flexible batching. We show that LazyBatching can intelligently determine the set of nodes that can be efficiently batched together, achieving an average 15x, 1.5x, and 5.5x improvement than graph batching in terms of average response time, throughput, and SLA satisfaction, respectively.</summary>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-25T12:13:32Z</published>\n",
      "    <arxiv:primary_category term=\"cs.DC\"/>\n",
      "    <author>\n",
      "      <name>Yujeong Choi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yunseong Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Minsoo Rhu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.13116v1</id>\n",
      "    <title>An empirical study of domain-agnostic semi-supervised learning via energy-based models: joint-training and pre-training</title>\n",
      "    <updated>2020-10-25T13:35:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.13116v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.13116v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A class of recent semi-supervised learning (SSL) methods heavily rely on domain-specific data augmentations. In contrast, generative SSL methods involve unsupervised learning based on generative models by either joint-training or pre-training, and are more appealing from the perspective of being domain-agnostic, since they do not inherently require data augmentations. Joint-training estimates the joint distribution of observations and labels, while pre-training is taken over observations only. Recently, energy-based models (EBMs) have achieved promising results for generative modeling. Joint-training via EBMs for SSL has been explored with encouraging results across different data modalities. In this paper, we make two contributions. First, we explore pre-training via EBMs for SSL and compare it to joint-training. Second, a suite of experiments are conducted over domains of image classification and natural language labeling to give a realistic whole picture of the performances of EBM based SSL methods. It is found that joint-training EBMs outperform pre-training EBMs marginally but nearly consistently.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-25T13:35:23Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Yunfu Song</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Huahuan Zheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhijian Ou</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.13164v1</id>\n",
      "    <title>A Hierarchical Graph Signal Processing Approach to Inference from Spatiotemporal Signals</title>\n",
      "    <updated>2020-10-25T17:08:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.13164v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.13164v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Motivated by the emerging area of graph signal processing (GSP), we introduce a novel method to draw inference from spatiotemporal signals. Data acquisition in different locations over time is common in sensor networks, for diverse applications ranging from object tracking in wireless networks to medical uses such as electroencephalography (EEG) signal processing. In this paper we leverage novel techniques of GSP to develop a hierarchical feature extraction approach by mapping the data onto a series of spatiotemporal graphs. Such a model maps signals onto vertices of a graph and the time-space dependencies among signals are modeled by the edge weights. Signal components acquired from different locations and time often have complicated functional dependencies. Accordingly, their corresponding graph weights are learned from data and used in two ways. First, they are used as a part of the embedding related to the topology of graph, such as density. Second, they provide the connectivities of the base graph for extracting higher level GSP-based features. The latter include the energies of the signal's graph Fourier transform in different frequency bands. We test our approach on the intracranial EEG (iEEG) data set of the Kaggle epileptic seizure detection contest. In comparison to the winning code, the results show a slight net improvement and up to 6 percent improvement in per subject analysis, while the number of features are decreased by 75 percent on average.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-25T17:08:13Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <arxiv:journal_ref>In 2018 29th Biennial Symposium on Communications (BSC) (pp. 1-5). IEEE (2018, June)</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Nafiseh Ghoroghchian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stark C. Draper</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Roman Genov</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/BSC.2018.8494688</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/BSC.2018.8494688\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.11632v1</id>\n",
      "    <title>The Primal-Dual method for Learning Augmented Algorithms</title>\n",
      "    <updated>2020-10-22T11:58:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.11632v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.11632v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The extension of classical online algorithms when provided with predictions is a new and active research area. In this paper, we extend the primal-dual method for online algorithms in order to incorporate predictions that advise the online algorithm about the next action to take. We use this framework to obtain novel algorithms for a variety of online covering problems. We compare our algorithms to the cost of the true and predicted offline optimal solutions and show that these algorithms outperform any online algorithm when the prediction is accurate while maintaining good guarantees when the prediction is misleading.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-22T11:58:47Z</published>\n",
      "    <arxiv:comment>30 pages, 11 figures. To appear in NeurIPS 2020 (oral)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Étienne Bamas</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andreas Maggiori</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ola Svensson</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.11635v2</id>\n",
      "    <title>Continual Learning in Low-rank Orthogonal Subspaces</title>\n",
      "    <updated>2020-12-08T15:23:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.11635v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.11635v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In continual learning (CL), a learner is faced with a sequence of tasks, arriving one after the other, and the goal is to remember all the tasks once the continual learning experience is finished. The prior art in CL uses episodic memory, parameter regularization or extensible network structures to reduce interference among tasks, but in the end, all the approaches learn different tasks in a joint vector space. We believe this invariably leads to interference among different tasks. We propose to learn tasks in different (low-rank) vector subspaces that are kept orthogonal to each other in order to minimize interference. Further, to keep the gradients of different tasks coming from these subspaces orthogonal to each other, we learn isometric mappings by posing network training as an optimization problem over the Stiefel manifold. To the best of our understanding, we report, for the first time, strong results over experience-replay baseline with and without memory on standard classification benchmarks in continual learning. The code is made publicly available.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-22T12:07:43Z</published>\n",
      "    <arxiv:comment>The paper is accepted at NeurIPS'20</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>NeurIPS, 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Arslan Chaudhry</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Naeemullah Khan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Puneet K. Dokania</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Philip H. S. Torr</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.11672v1</id>\n",
      "    <title>CycleGAN-VC3: Examining and Improving CycleGAN-VCs for Mel-spectrogram Conversion</title>\n",
      "    <updated>2020-10-22T13:08:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.11672v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.11672v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Non-parallel voice conversion (VC) is a technique for learning mappings between source and target speeches without using a parallel corpus. Recently, cycle-consistent adversarial network (CycleGAN)-VC and CycleGAN-VC2 have shown promising results regarding this problem and have been widely used as benchmark methods. However, owing to the ambiguity of the effectiveness of CycleGAN-VC/VC2 for mel-spectrogram conversion, they are typically used for mel-cepstrum conversion even when comparative methods employ mel-spectrogram as a conversion target. To address this, we examined the applicability of CycleGAN-VC/VC2 to mel-spectrogram conversion. Through initial experiments, we discovered that their direct applications compromised the time-frequency structure that should be preserved during conversion. To remedy this, we propose CycleGAN-VC3, an improvement of CycleGAN-VC2 that incorporates time-frequency adaptive normalization (TFAN). Using TFAN, we can adjust the scale and bias of the converted features while reflecting the time-frequency structure of the source mel-spectrogram. We evaluated CycleGAN-VC3 on inter-gender and intra-gender non-parallel VC. A subjective evaluation of naturalness and similarity showed that for every VC pair, CycleGAN-VC3 outperforms or is competitive with the two types of CycleGAN-VC2, one of which was applied to mel-cepstrum and the other to mel-spectrogram. Audio samples are available at http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc3/index.html.</summary>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-22T13:08:44Z</published>\n",
      "    <arxiv:comment>Accepted to Interspeech 2020. Project page: http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc3/index.html</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.SD\"/>\n",
      "    <author>\n",
      "      <name>Takuhiro Kaneko</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hirokazu Kameoka</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kou Tanaka</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nobukatsu Hojo</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.11679v1</id>\n",
      "    <title>DPAttack: Diffused Patch Attacks against Universal Object Detection</title>\n",
      "    <updated>2020-10-16T04:48:24Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.11679v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.11679v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recently, deep neural networks (DNNs) have been widely and successfully used in Object Detection, e.g. Faster RCNN, YOLO, CenterNet. However, recent studies have shown that DNNs are vulnerable to adversarial attacks. Adversarial attacks against object detection can be divided into two categories, whole-pixel attacks and patch attacks. While these attacks add perturbations to a large number of pixels in images, we proposed a diffused patch attack (\\textbf{DPAttack}) to successfully fool object detectors by diffused patches of asteroid-shaped or grid-shape, which only change a small number of pixels. Experiments show that our DPAttack can successfully fool most object detectors with diffused patches and we get the second place in the Alibaba Tianchi competition: Alibaba-Tsinghua Adversarial Challenge on Object Detection. Our code can be obtained from https://github.com/Wu-Shudeng/DPAttack.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-16T04:48:24Z</published>\n",
      "    <arxiv:comment>4 pages, 2 figures, CIKM Workshop</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Shudeng Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tao Dai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shu-Tao Xia</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.11682v1</id>\n",
      "    <title>Lung Nodule Classification Using Biomarkers, Volumetric Radiomics and 3D CNNs</title>\n",
      "    <updated>2020-10-19T18:57:26Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.11682v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.11682v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present a hybrid algorithm to estimate lung nodule malignancy that combines imaging biomarkers from Radiologist's annotation with image classification of CT scans. Our algorithm employs a 3D Convolutional Neural Network (CNN) as well as a Random Forest in order to combine CT imagery with biomarker annotation and volumetric radiomic features. We analyze and compare the performance of the algorithm using only imagery, only biomarkers, combined imagery + biomarkers, combined imagery + volumetric radiomic features and finally the combination of imagery + biomarkers + volumetric features in order to classify the suspicion level of nodule malignancy. The National Cancer Institute (NCI) Lung Image Database Consortium (LIDC) IDRI dataset is used to train and evaluate the classification task. We show that the incorporation of semi-supervised learning by means of K-Nearest-Neighbors (KNN) can increase the available training sample size of the LIDC-IDRI thereby further improving the accuracy of malignancy estimation of most of the models tested although there is no significant improvement with the use of KNN semi-supervised learning if image classification with CNNs and volumetric features are combined with descriptive biomarkers. Unexpectedly, we also show that a model using image biomarkers alone is more accurate than one that combines biomarkers with volumetric radiomics, 3D CNNs, and semi-supervised learning. We discuss the possibility that this result may be influenced by cognitive bias in LIDC-IDRI because malignancy estimates were recorded by the same radiologist panel as biomarkers, as well as future work to incorporate pathology information over a subset of study participants.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-19T18:57:26Z</published>\n",
      "    <arxiv:comment>This paper has been submitted to the Journal of Digital Imaging (JDI 2020). The poster of this paper has received the 2nd prize for the Research Poster Award. Link: https://siim.org/page/20m_p_lung_node_malignancy</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Kushal Mehta</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arshita Jain</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jayalakshmi Mangalagiri</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sumeet Menon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Phuong Nguyen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David R. Chapman</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12799v1</id>\n",
      "    <title>Private Outsourced Bayesian Optimization</title>\n",
      "    <updated>2020-10-24T06:30:45Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12799v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12799v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper presents the private-outsourced-Gaussian process-upper confidence bound (PO-GP-UCB) algorithm, which is the first algorithm for privacy-preserving Bayesian optimization (BO) in the outsourced setting with a provable performance guarantee. We consider the outsourced setting where the entity holding the dataset and the entity performing BO are represented by different parties, and the dataset cannot be released non-privately. For example, a hospital holds a dataset of sensitive medical records and outsources the BO task on this dataset to an industrial AI company. The key idea of our approach is to make the BO performance of our algorithm similar to that of non-private GP-UCB run using the original dataset, which is achieved by using a random projection-based transformation that preserves both privacy and the pairwise distances between inputs. Our main theoretical contribution is to show that a regret bound similar to that of the standard GP-UCB algorithm can be established for our PO-GP-UCB algorithm. We empirically evaluate the performance of our PO-GP-UCB algorithm with synthetic and real-world datasets.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-24T06:30:45Z</published>\n",
      "    <arxiv:comment>37th International Conference on Machine Learning (ICML 2020), Extended version with proofs, 27 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Dmitrii Kharkovskii</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhongxiang Dai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bryan Kian Hsiang Low</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12803v1</id>\n",
      "    <title>Attentive Autoencoders for Multifaceted Preference Learning in One-class Collaborative Filtering</title>\n",
      "    <updated>2020-10-24T06:35:44Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12803v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12803v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Most existing One-Class Collaborative Filtering (OC-CF) algorithms estimate a user's preference as a latent vector by encoding their historical interactions. However, users often show diverse interests, which significantly increases the learning difficulty. In order to capture multifaceted user preferences, existing recommender systems either increase the encoding complexity or extend the latent representation dimension. Unfortunately, these changes inevitably lead to increased training difficulty and exacerbate scalability issues. In this paper, we propose a novel and efficient CF framework called Attentive Multi-modal AutoRec (AMA) that explicitly tracks multiple facets of user preferences. Specifically, we extend the Autoencoding-based recommender AutoRec to learn user preferences with multi-modal latent representations, where each mode captures one facet of a user's preferences. By leveraging the attention mechanism, each observed interaction can have different contributions to the preference facets. Through extensive experiments on three real-world datasets, we show that AMA is competitive with state-of-the-art models under the OC-CF setting. Also, we demonstrate how the proposed model improves interpretability by providing explanations using the attention mechanism.</summary>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-24T06:35:44Z</published>\n",
      "    <arxiv:comment>Accepted at ICDMW 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.IR\"/>\n",
      "    <author>\n",
      "      <name>Zheda Mai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ga Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kai Luo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Scott Sanner</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12865v1</id>\n",
      "    <title>Fast Epigraphical Projection-based Incremental Algorithms for Wasserstein Distributionally Robust Support Vector Machine</title>\n",
      "    <updated>2020-10-24T10:42:27Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12865v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12865v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Wasserstein \\textbf{D}istributionally \\textbf{R}obust \\textbf{O}ptimization (DRO) is concerned with finding decisions that perform well on data that are drawn from the worst-case probability distribution within a Wasserstein ball centered at a certain nominal distribution. In recent years, it has been shown that various DRO formulations of learning models admit tractable convex reformulations. However, most existing works propose to solve these convex reformulations by general-purpose solvers, which are not well-suited for tackling large-scale problems. In this paper, we focus on a family of Wasserstein distributionally robust support vector machine (DRSVM) problems and propose two novel epigraphical projection-based incremental algorithms to solve them. The updates in each iteration of these algorithms can be computed in a highly efficient manner. Moreover, we show that the DRSVM problems considered in this paper satisfy a Hölderian growth condition with explicitly determined growth exponents. Consequently, we are able to establish the convergence rates of the proposed incremental algorithms. Our numerical results indicate that the proposed methods are orders of magnitude faster than the state-of-the-art, and the performance gap grows considerably as the problem size increases.</summary>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-24T10:42:27Z</published>\n",
      "    <arxiv:comment>Accepted by NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"math.OC\"/>\n",
      "    <author>\n",
      "      <name>Jiajin Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Caihua Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anthony Man-Cho So</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12989v1</id>\n",
      "    <title>Are Adversarial Examples Created Equal? A Learnable Weighted Minimax Risk for Robustness under Non-uniform Attacks</title>\n",
      "    <updated>2020-10-24T21:20:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12989v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12989v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Adversarial Training is proved to be an efficient method to defend against adversarial examples, being one of the few defenses that withstand strong attacks. However, traditional defense mechanisms assume a uniform attack over the examples according to the underlying data distribution, which is apparently unrealistic as the attacker could choose to focus on more vulnerable examples. We present a weighted minimax risk optimization that defends against non-uniform attacks, achieving robustness against adversarial examples under perturbed test data distributions. Our modified risk considers importance weights of different adversarial examples and focuses adaptively on harder examples that are wrongly classified or at higher risk of being classified incorrectly. The designed risk allows the training process to learn a strong defense through optimizing the importance weights. The experiments show that our model significantly improves state-of-the-art adversarial accuracy under non-uniform attacks without a significant drop under uniform attacks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-24T21:20:35Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Huimin Zeng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chen Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tom Goldstein</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Furong Huang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12191v2</id>\n",
      "    <title>Escape saddle points faster on manifolds via perturbed Riemannian stochastic recursive gradient</title>\n",
      "    <updated>2020-10-28T23:41:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12191v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12191v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we propose a variant of Riemannian stochastic recursive gradient method that can achieve second-order convergence guarantee and escape saddle points using simple perturbation. The idea is to perturb the iterates when gradient is small and carry out stochastic recursive gradient updates over tangent space. This avoids the complication of exploiting Riemannian geometry. We show that under finite-sum setting, our algorithm requires $\\widetilde{\\mathcal{O}}\\big( \\frac{ \\sqrt{n}}{ε^2} + \\frac{\\sqrt{n} }{δ^4} + \\frac{n}{δ^3}\\big)$ stochastic gradient queries to find a $(ε, δ)$-second-order critical point. This strictly improves the complexity of perturbed Riemannian gradient descent and is superior to perturbed Riemannian accelerated gradient descent under large-sample settings. We also provide a complexity of $\\widetilde{\\mathcal{O}} \\big( \\frac{1}{ε^3} + \\frac{1}{δ^3 ε^2} + \\frac{1}{δ^4 ε} \\big)$ for online optimization, which is novel on Riemannian manifold in terms of second-order convergence using only first-order information.</summary>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-23T06:41:56Z</published>\n",
      "    <arxiv:comment>Minor typos corrected</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"math.OC\"/>\n",
      "    <author>\n",
      "      <name>Andi Han</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Junbin Gao</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12214v2</id>\n",
      "    <title>Learning to Optimise General TSP Instances</title>\n",
      "    <updated>2020-11-03T11:51:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12214v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12214v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The Travelling Salesman Problem (TSP) is a classical combinatorial optimisation problem. Deep learning has been successfully extended to meta-learning, where previous solving efforts assist in learning how to optimise future optimisation instances. In recent years, learning to optimise approaches have shown success in solving TSP problems. However, they focus on one type of TSP problem, namely ones where the points are uniformly distributed in Euclidean spaces and have issues in generalising to other embedding spaces, e.g., spherical distance spaces, and to TSP instances where the points are distributed in a non-uniform manner. An aim of learning to optimise is to train once and solve across a broad spectrum of (TSP) problems. Although supervised learning approaches have shown to achieve more optimal solutions than unsupervised approaches, they do require the generation of training data and running a solver to obtain solutions to learn from, which can be time-consuming and difficult to find reasonable solutions for harder TSP instances. Hence this paper introduces a new learning-based approach to solve a variety of different and common TSP problems that are trained on easier instances which are faster to train and are easier to obtain better solutions. We name this approach the non-Euclidean TSP network (NETSP-Net). The approach is evaluated on various TSP instances using the benchmark TSPLIB dataset and popular instance generator used in the literature. We performed extensive experiments that indicate our approach generalises across many types of instances and scales to instances that are larger than what was used during training.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-23T07:37:16Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Nasrin Sultana</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jeffrey Chan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>A. K. Qin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tabinda Sarwar</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12142v1</id>\n",
      "    <title>Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning</title>\n",
      "    <updated>2020-10-23T03:22:01Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12142v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12142v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Sample efficiency has been one of the major challenges for deep reinforcement learning. Recently, model-based reinforcement learning has been proposed to address this challenge by performing planning on imaginary trajectories with a learned world model. However, world model learning may suffer from overfitting to training trajectories, and thus model-based value estimation and policy search will be pone to be sucked in an inferior local policy. In this paper, we propose a novel model-based reinforcement learning algorithm, called BrIdging Reality and Dream (BIRD). It maximizes the mutual information between imaginary and real trajectories so that the policy improvement learned from imaginary trajectories can be easily generalized to real trajectories. We demonstrate that our approach improves sample efficiency of model-based planning, and achieves state-of-the-art performance on challenging visual control benchmarks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-23T03:22:01Z</published>\n",
      "    <arxiv:comment>Published on 34th Conference on Neural Information Processing Systems (NeurIPS 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Guangxiang Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Minghao Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Honglak Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chongjie Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12171v1</id>\n",
      "    <title>DualNet: Locate Then Detect Effective Payload with Deep Attention Network</title>\n",
      "    <updated>2020-10-23T05:32:21Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12171v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12171v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Network intrusion detection (NID) is an essential defense strategy that is used to discover the trace of suspicious user behaviour in large-scale cyberspace, and machine learning (ML), due to its capability of automation and intelligence, has been gradually adopted as a mainstream hunting method in recent years. However, traditional ML based network intrusion detection systems (NIDSs) are not effective to recognize unknown threats and their high detection rate often comes with the cost of high false alarms, which leads to the problem of alarm fatigue. To address the above problems, in this paper, we propose a novel neural network based detection system, DualNet, which is constructed with a general feature extraction stage and a crucial feature learning stage. DualNet can rapidly reuse the spatial-temporal features in accordance with their importance to facilitate the entire learning process and simultaneously mitigate several optimization problems occurred in deep learning (DL). We evaluate the DualNet on two benchmark cyber attack datasets, NSL-KDD and UNSW-NB15. Our experiment shows that DualNet outperforms classical ML based NIDSs and is more effective than existing DL methods for NID in terms of accuracy, detection rate and false alarm rate.</summary>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-23T05:32:21Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CR\"/>\n",
      "    <author>\n",
      "      <name>Shiyi Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peilun Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hui Guo</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.12174v1</id>\n",
      "    <title>KINNEWS and KIRNEWS: Benchmarking Cross-Lingual Text Classification for Kinyarwanda and Kirundi</title>\n",
      "    <updated>2020-10-23T05:37:42Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.12174v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.12174v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent progress in text classification has been focused on high-resource languages such as English and Chinese. For low-resource languages, amongst them most African languages, the lack of well-annotated data and effective preprocessing, is hindering the progress and the transfer of successful methods. In this paper, we introduce two news datasets (KINNEWS and KIRNEWS) for multi-class classification of news articles in Kinyarwanda and Kirundi, two low-resource African languages. The two languages are mutually intelligible, but while Kinyarwanda has been studied in Natural Language Processing (NLP) to some extent, this work constitutes the first study on Kirundi. Along with the datasets, we provide statistics, guidelines for preprocessing, and monolingual and cross-lingual baseline models. Our experiments show that training embeddings on the relatively higher-resourced Kinyarwanda yields successful cross-lingual transfer to Kirundi. In addition, the design of the created datasets allows for a wider use in NLP beyond text classification in future studies, such as representation learning, cross-lingual learning with more distant languages, or as base for new annotations for tasks such as parsing, POS tagging, and NER. The datasets, stopwords, and pre-trained embeddings are publicly available at https://github.com/Andrews2017/KINNEWS-and-KIRNEWS-Corpus .</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-23T05:37:42Z</published>\n",
      "    <arxiv:comment>COLING 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Rubungo Andre Niyongabo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hong Qu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Julia Kreutzer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Li Huang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02800v1</id>\n",
      "    <title>Improving Reconstructive Surgery Design using Gaussian Process Surrogates to Capture Material Behavior Uncertainty</title>\n",
      "    <updated>2020-10-05T11:44:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02800v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02800v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Excessive loads near wounds produce pathological scarring and other complications. Presently, stress cannot easily be measured by surgeons in the operating room. Instead, surgeons rely on intuition and experience. Predictive computational tools are ideal candidates for surgery planning. Finite element (FE) simulations have shown promise in predicting stress fields on large skin patches and complex cases, helping to identify potential regions of complication. Unfortunately, these simulations are computationally expensive and deterministic. However, running a few, well-selected FE simulations allows us to create Gaussian process (GP) surrogate models of local cutaneous flaps that are computationally efficient and able to predict stress and strain for arbitrary material parameters. Here, we create GP surrogates for the advancement, rotation, and transposition flaps. We then use the predictive capability of these surrogates to perform a global sensitivity analysis, ultimately showing that fiber direction has the most significant impact on strain field variations. We then perform an optimization to determine the optimal fiber direction for each flap for three different objectives driven by clinical guidelines. While material properties are not controlled by the surgeon and are actually a source of uncertainty, the surgeon can in fact control the orientation of the flap. Therefore, fiber direction is the only material parameter that can be optimized clinically. The optimization task relies on the efficiency of the GP surrogates to calculate the expected cost of different strategies when the uncertainty of other material parameters is included. We propose optimal flap orientations for the three cost functions and that can help in reducing stress resulting from the surgery and ultimately reduce complications associated with excessive mechanical loading near wounds.</summary>\n",
      "    <category term=\"physics.med-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T11:44:09Z</published>\n",
      "    <arxiv:comment>17 pages, 9 figures. Submitted for publication</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"physics.med-ph\"/>\n",
      "    <author>\n",
      "      <name>Casey Stowers</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Taeksang Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ilias Bilionis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arun Gosain</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Adrian Buganza Tepole</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02806v2</id>\n",
      "    <title>Textual Supervision for Visually Grounded Spoken Language Understanding</title>\n",
      "    <updated>2020-10-07T07:48:12Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02806v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02806v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Visually-grounded models of spoken language understanding extract semantic information directly from speech, without relying on transcriptions. This is useful for low-resource languages, where transcriptions can be expensive or impossible to obtain. Recent work showed that these models can be improved if transcriptions are available at training time. However, it is not clear how an end-to-end approach compares to a traditional pipeline-based approach when one has access to transcriptions. Comparing different strategies, we find that the pipeline approach works better when enough text is available. With low-resource languages in mind, we also show that translations can be effectively used in place of transcriptions but more data is needed to obtain similar results.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T15:16:23Z</published>\n",
      "    <arxiv:comment>Findings of EMNLP 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Bertrand Higy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Desmond Elliott</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Grzegorz Chrupała</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02807v3</id>\n",
      "    <title>Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks</title>\n",
      "    <updated>2020-11-17T02:31:30Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02807v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02807v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T15:16:31Z</published>\n",
      "    <arxiv:comment>Post EMNLP 2020 camera ready updates</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Shubham Toshniwal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sam Wiseman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Allyson Ettinger</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Karen Livescu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kevin Gimpel</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02811v1</id>\n",
      "    <title>Fast Mesh Data Augmentation via Chebyshev Polynomial of Spectral filtering</title>\n",
      "    <updated>2020-10-06T15:18:26Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02811v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02811v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Deep neural networks have recently been recognized as one of the powerful learning techniques in computer vision and medical image analysis. Trained deep neural networks need to be generalizable to new data that was not seen before. In practice, there is often insufficient training data available and augmentation is used to expand the dataset. Even though graph convolutional neural network (graph-CNN) has been widely used in deep learning, there is a lack of augmentation methods to generate data on graphs or surfaces. This study proposes two unbiased augmentation methods, Laplace-Beltrami eigenfunction Data Augmentation (LB-eigDA) and Chebyshev polynomial Data Augmentation (C-pDA), to generate new data on surfaces, whose mean is the same as that of real data. LB-eigDA augments data via the resampling of the LB coefficients. In parallel with LB-eigDA, we introduce a fast augmentation approach, C-pDA, that employs a polynomial approximation of LB spectral filters on surfaces. We design LB spectral bandpass filters by Chebyshev polynomial approximation and resample signals filtered via these filters to generate new data on surfaces. We first validate LB-eigDA and C-pDA via simulated data and demonstrate their use for improving classification accuracy. We then employ the brain images of Alzheimer's Disease Neuroimaging Initiative (ADNI) and extract cortical thickness that is represented on the cortical surface to illustrate the use of the two augmentation methods. We demonstrate that augmented cortical thickness has a similar pattern to real data. Second, we show that C-pDA is much faster than LB-eigDA. Last, we show that C-pDA can improve the AD classification accuracy of graph-CNN.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T15:18:26Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Shih-Gu Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Moo K. Chung</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anqi Qiu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alzheimer's Disease Neuroimaging Initiative</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02847v2</id>\n",
      "    <title>Robustness and Reliability of Gender Bias Assessment in Word Embeddings: The Role of Base Pairs</title>\n",
      "    <updated>2020-10-27T21:24:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02847v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02847v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>It has been shown that word embeddings can exhibit gender bias, and various methods have been proposed to quantify this. However, the extent to which the methods are capturing social stereotypes inherited from the data has been debated. Bias is a complex concept and there exist multiple ways to define it. Previous work has leveraged gender word pairs to measure bias and extract biased analogies. We show that the reliance on these gendered pairs has strong limitations: bias measures based off of them are not robust and cannot identify common types of real-world bias, whilst analogies utilising them are unsuitable indicators of bias. In particular, the well-known analogy \"man is to computer-programmer as woman is to homemaker\" is due to word similarity rather than societal bias. This has important implications for work on measuring bias in embeddings and related work debiasing embeddings.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T16:09:05Z</published>\n",
      "    <arxiv:comment>Accepted at AACL-IJCNLP 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Haiyang Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alison Sneyd</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mark Stevenson</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02916v1</id>\n",
      "    <title>Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate</title>\n",
      "    <updated>2020-10-06T17:58:29Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02916v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02916v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent works (e.g., (Li and Arora, 2020)) suggest that the use of popular normalization schemes (including Batch Normalization) in today's deep learning can move it far from a traditional optimization viewpoint, e.g., use of exponentially increasing learning rates. The current paper highlights other ways in which behavior of normalized nets departs from traditional viewpoints, and then initiates a formal framework for studying their mathematics via suitable adaptation of the conventional framework namely, modeling SGD-induced training trajectory via a suitable stochastic differential equation (SDE) with a noise term that captures gradient noise. This yields: (a) A new ' intrinsic learning rate' parameter that is the product of the normal learning rate and weight decay factor. Analysis of the SDE shows how the effective speed of learning varies and equilibrates over time under the control of intrinsic LR. (b) A challenge -- via theory and experiments -- to popular belief that good generalization requires large learning rates at the start of training. (c) New experiments, backed by mathematical intuition, suggesting the number of steps to equilibrium (in function space) scales as the inverse of the intrinsic learning rate, as opposed to the exponential time convergence bound implied by SDE analysis. We name it the Fast Equilibrium Conjecture and suggest it holds the key to why Batch Normalization is effective.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T17:58:29Z</published>\n",
      "    <arxiv:comment>25 pages, 12 figures. Accepted By 34th Conference on Neural Information Processing Systems (NeurIPS 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Zhiyuan Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kaifeng Lyu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sanjeev Arora</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.00950v1</id>\n",
      "    <title>Regularized K-means through hard-thresholding</title>\n",
      "    <updated>2020-10-02T12:29:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.00950v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.00950v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study a framework of regularized $K$-means methods based on direct penalization of the size of the cluster centers. Different penalization strategies are considered and compared through simulation and theoretical analysis. Based on the results, we propose HT $K$-means, which uses an $\\ell_0$ penalty to induce sparsity in the variables. Different techniques for selecting the tuning parameter are discussed and compared. The proposed method stacks up favorably with the most popular regularized $K$-means methods in an extensive simulation study. Finally, HT $K$-means is applied to several real data examples. Graphical displays are presented and used in these examples to gain more insight into the datasets.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-02T12:29:32Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Jakob Raymaekers</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruben H. Zamar</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.00985v2</id>\n",
      "    <title>Kalman Filtering Attention for User Behavior Modeling in CTR Prediction</title>\n",
      "    <updated>2020-10-20T06:48:43Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.00985v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.00985v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Click-through rate (CTR) prediction is one of the fundamental tasks for e-commerce search engines. As search becomes more personalized, it is necessary to capture the user interest from rich behavior data. Existing user behavior modeling algorithms develop different attention mechanisms to emphasize query-relevant behaviors and suppress irrelevant ones. Despite being extensively studied, these attentions still suffer from two limitations. First, conventional attentions mostly limit the attention field only to a single user's behaviors, which is not suitable in e-commerce where users often hunt for new demands that are irrelevant to any historical behaviors. Second, these attentions are usually biased towards frequent behaviors, which is unreasonable since high frequency does not necessarily indicate great importance. To tackle the two limitations, we propose a novel attention mechanism, termed Kalman Filtering Attention (KFAtt), that considers the weighted pooling in attention as a maximum a posteriori (MAP) estimation. By incorporating a priori, KFAtt resorts to global statistics when few user behaviors are relevant. Moreover, a frequency capping mechanism is incorporated to correct the bias towards frequent behaviors. Offline experiments on both benchmark and a 10 billion scale real production dataset, together with an Online A/B test, show that KFAtt outperforms all compared state-of-the-arts. KFAtt has been deployed in the ranking system of a leading e commerce website, serving the main traffic of hundreds of millions of active users everyday.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-02T13:30:26Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Hu Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jing Lu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiwei Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sulong Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hao Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yutong Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zehua Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jian Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Junsheng Jin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yongjun Bao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Weipeng Yan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.00991v1</id>\n",
      "    <title>RDCNet: Instance segmentation with a minimalist recurrent residual network</title>\n",
      "    <updated>2020-10-02T13:36:45Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.00991v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.00991v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Instance segmentation is a key step for quantitative microscopy. While several machine learning based methods have been proposed for this problem, most of them rely on computationally complex models that are trained on surrogate tasks. Building on recent developments towards end-to-end trainable instance segmentation, we propose a minimalist recurrent network called recurrent dilated convolutional network (RDCNet), consisting of a shared stacked dilated convolution (sSDC) layer that iteratively refines its output and thereby generates interpretable intermediate predictions. It is light-weight and has few critical hyperparameters, which can be related to physical aspects such as object size or density.We perform a sensitivity analysis of its main parameters and we demonstrate its versatility on 3 tasks with different imaging modalities: nuclear segmentation of H&amp;E slides, of 3D anisotropic stacks from light-sheet fluorescence microscopy and leaf segmentation of top-view images of plants. It achieves state-of-the-art on 2 of the 3 datasets.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-02T13:36:45Z</published>\n",
      "    <arxiv:comment>Accepted at MICCAI-MLMI 2020 workshop</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Raphael Ortiz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gustavo de Medeiros</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Antoine H. F. M. Peters</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Prisca Liberali</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Markus Rempfler</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1007/978-3-030-59861-7</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1007/978-3-030-59861-7\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.00994v1</id>\n",
      "    <title>A local geometry of hyperedges in hypergraphs, and its applications to social networks</title>\n",
      "    <updated>2020-09-29T21:20:36Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.00994v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.00994v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In many real world datasets arising from social networks, there are hidden higher order relations among data points which cannot be captured using graph modeling. It is natural to use a more general notion of hypergraphs to model such social networks. In this paper, we introduce a new local geometry of hyperdges in hypergraphs which allows to capture higher order relations among data points. Furthermore based on this new geometry, we also introduce new methodology--the nearest neighbors method in hypergraphs--for analyzing datasets arising from sociology.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-29T21:20:36Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Dong Quan Ngoc Nguyen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lin Xing</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01003v1</id>\n",
      "    <title>Differentiable Weighted Finite-State Transducers</title>\n",
      "    <updated>2020-10-02T13:52:24Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01003v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01003v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We introduce a framework for automatic differentiation with weighted finite-state transducers (WFSTs) allowing them to be used dynamically at training time. Through the separation of graphs from operations on graphs, this framework enables the exploration of new structured loss functions which in turn eases the encoding of prior knowledge into learning algorithms. We show how the framework can combine pruning and back-off in transition models with various sequence-level loss functions. We also show how to learn over the latent decomposition of phrases into word pieces. Finally, to demonstrate that WFSTs can be used in the interior of a deep neural network, we propose a convolutional WFST layer which maps lower-level representations to higher-level representations and can be used as a drop-in replacement for a traditional convolution. We validate these algorithms with experiments in handwriting recognition and speech recognition.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-02T13:52:24Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Awni Hannun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vineel Pratap</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jacob Kahn</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wei-Ning Hsu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01011v1</id>\n",
      "    <title>Deep Convolutional Transform Learning -- Extended version</title>\n",
      "    <updated>2020-10-02T14:03:19Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01011v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01011v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This work introduces a new unsupervised representation learning technique called Deep Convolutional Transform Learning (DCTL). By stacking convolutional transforms, our approach is able to learn a set of independent kernels at different layers. The features extracted in an unsupervised manner can then be used to perform machine learning tasks, such as classification and clustering. The learning technique relies on a well-sounded alternating proximal minimization scheme with established convergence guarantees. Our experimental results show that the proposed DCTL technique outperforms its shallow version CTL, on several benchmark datasets.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-02T14:03:19Z</published>\n",
      "    <arxiv:comment>10 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jyoti Maggu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Angshul Majumdar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Emilie Chouzenoux</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Giovanni Chierchia</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01040v1</id>\n",
      "    <title>Attention-Based Clustering: Learning a Kernel from Context</title>\n",
      "    <updated>2020-10-02T15:06:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01040v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01040v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In machine learning, no data point stands alone. We believe that context is an underappreciated concept in many machine learning methods. We propose Attention-Based Clustering (ABC), a neural architecture based on the attention mechanism, which is designed to learn latent representations that adapt to context within an input set, and which is inherently agnostic to input sizes and number of clusters. By learning a similarity kernel, our method directly combines with any out-of-the-box kernel-based clustering approach. We present competitive results for clustering Omniglot characters and include analytical evidence of the effectiveness of an attention-based approach for clustering.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-02T15:06:06Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Samuel Coward</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Erik Visse-Martindale</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chithrupa Ramesh</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01057v1</id>\n",
      "    <title>LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention</title>\n",
      "    <updated>2020-10-02T15:38:03Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01057v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01057v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-02T15:38:03Z</published>\n",
      "    <arxiv:comment>EMNLP 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Ikuya Yamada</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Akari Asai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hiroyuki Shindo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hideaki Takeda</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuji Matsumoto</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.14588v1</id>\n",
      "    <title>EWS-GCN: Edge Weight-Shared Graph Convolutional Network for Transactional Banking Data</title>\n",
      "    <updated>2020-09-30T12:09:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.14588v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.14588v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we discuss how modern deep learning approaches can be applied to the credit scoring of bank clients. We show that information about connections between clients based on money transfers between them allows us to significantly improve the quality of credit scoring compared to the approaches using information about the target client solely. As a final solution, we develop a new graph neural network model EWS-GCN that combines ideas of graph convolutional and recurrent neural networks via attention mechanism. The resulting model allows for robust training and efficient processing of large-scale data. We also demonstrate that our model outperforms the state-of-the-art graph neural networks achieving excellent results</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-30T12:09:28Z</published>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Ivan Sukharev</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Valentina Shumovskaia</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kirill Fedyanin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maxim Panov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dmitry Berestnev</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.14596v1</id>\n",
      "    <title>Machine Learning and Computational Mathematics</title>\n",
      "    <updated>2020-09-23T23:16:46Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.14596v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.14596v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Neural network-based machine learning is capable of approximating functions in very high dimension with unprecedented efficiency and accuracy. This has opened up many exciting new possibilities, not just in traditional areas of artificial intelligence, but also in scientific computing and computational science. At the same time, machine learning has also acquired the reputation of being a set of \"black box\" type of tricks, without fundamental principles. This has been a real obstacle for making further progress in machine learning. In this article, we try to address the following two very important questions: (1) How machine learning has already impacted and will further impact computational mathematics, scientific computing and computational science? (2) How computational mathematics, particularly numerical analysis, {can} impact machine learning? We describe some of the most important progress that has been made on these issues. Our hope is to put things into a perspective that will help to integrate machine learning with computational mathematics.</summary>\n",
      "    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-23T23:16:46Z</published>\n",
      "    <arxiv:primary_category term=\"math.NA\"/>\n",
      "    <author>\n",
      "      <name>Weinan E</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.4208/cicp.OA-2020-0185</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.4208/cicp.OA-2020-0185\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.14660v2</id>\n",
      "    <title>Driver Anomaly Detection: A Dataset and Contrastive Learning Approach</title>\n",
      "    <updated>2020-11-30T15:00:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.14660v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.14660v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Distracted drivers are more likely to fail to anticipate hazards, which result in car accidents. Therefore, detecting anomalies in drivers' actions (i.e., any action deviating from normal driving) contains the utmost importance to reduce driver-related accidents. However, there are unbounded many anomalous actions that a driver can do while driving, which leads to an 'open set recognition' problem. Accordingly, instead of recognizing a set of anomalous actions that are commonly defined by previous dataset providers, in this work, we propose a contrastive learning approach to learn a metric to differentiate normal driving from anomalous driving. For this task, we introduce a new video-based benchmark, the Driver Anomaly Detection (DAD) dataset, which contains normal driving videos together with a set of anomalous actions in its training set. In the test set of the DAD dataset, there are unseen anomalous actions that still need to be winnowed out from normal driving. Our method reaches 0.9673 AUC on the test set, demonstrating the effectiveness of the contrastive learning approach on the anomaly detection task. Our dataset, codes and pre-trained models are publicly available.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-30T13:23:21Z</published>\n",
      "    <arxiv:comment>Accepted to IEEE Winter Conference on Applications of Computer Vision (WACV 2021)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Okan Köpüklü</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiapeng Zheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hang Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gerhard Rigoll</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.14720v2</id>\n",
      "    <title>DVERGE: Diversifying Vulnerabilities for Enhanced Robust Generation of Ensembles</title>\n",
      "    <updated>2020-10-18T16:35:25Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.14720v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.14720v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent research finds CNN models for image classification demonstrate overlapped adversarial vulnerabilities: adversarial attacks can mislead CNN models with small perturbations, which can effectively transfer between different models trained on the same dataset. Adversarial training, as a general robustness improvement technique, eliminates the vulnerability in a single model by forcing it to learn robust features. The process is hard, often requires models with large capacity, and suffers from significant loss on clean data accuracy. Alternatively, ensemble methods are proposed to induce sub-models with diverse outputs against a transfer adversarial example, making the ensemble robust against transfer attacks even if each sub-model is individually non-robust. Only small clean accuracy drop is observed in the process. However, previous ensemble training methods are not efficacious in inducing such diversity and thus ineffective on reaching robust ensemble. We propose DVERGE, which isolates the adversarial vulnerability in each sub-model by distilling non-robust features, and diversifies the adversarial vulnerability to induce diverse outputs against a transfer attack. The novel diversity metric and training procedure enables DVERGE to achieve higher robustness against transfer attacks comparing to previous ensemble methods, and enables the improved robustness when more sub-models are added to the ensemble. The code of this work is available at https://github.com/zjysteven/DVERGE</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-30T14:57:35Z</published>\n",
      "    <arxiv:comment>To be appeared in NeurIPS 2020 conference (Oral)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Huanrui Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jingyang Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hongliang Dong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nathan Inkawhich</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrew Gardner</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrew Touchet</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wesley Wilkes</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Heath Berry</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hai Li</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.14737v2</id>\n",
      "    <title>Improving Auto-Augment via Augmentation-Wise Weight Sharing</title>\n",
      "    <updated>2020-10-22T15:12:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.14737v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.14737v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The recent progress on automatically searching augmentation policies has boosted the performance substantially for various tasks. A key component of automatic augmentation search is the evaluation process for a particular augmentation policy, which is utilized to return reward and usually runs thousands of times. A plain evaluation process, which includes full model training and validation, would be time-consuming. To achieve efficiency, many choose to sacrifice evaluation reliability for speed. In this paper, we dive into the dynamics of augmented training of the model. This inspires us to design a powerful and efficient proxy task based on the Augmentation-Wise Weight Sharing (AWS) to form a fast yet accurate evaluation process in an elegant way. Comprehensive analysis verifies the superiority of this approach in terms of effectiveness and efficiency. The augmentation policies found by our method achieve superior accuracies compared with existing auto-augmentation search methods. On CIFAR-10, we achieve a top-1 error rate of 1.24%, which is currently the best performing single model without extra training data. On ImageNet, we get a top-1 error rate of 20.36% for ResNet-50, which leads to 3.34% absolute error rate reduction over the baseline augmentation.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-30T15:23:12Z</published>\n",
      "    <arxiv:comment>Accepted to NeurIPS 2020 (Poster)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Keyu Tian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chen Lin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ming Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Luping Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Junjie Yan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wanli Ouyang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01236v1</id>\n",
      "    <title>Placement of UAV-Mounted Mobile Base Station through User Load-Feature K-means Clustering</title>\n",
      "    <updated>2020-10-03T00:24:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01236v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01236v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Temporary high traffic requests in cellular networks is a challenging problem to address. Recent advances in Unmanned Aerial Vehicles applied to cover these types of traffics. UAV -Mounted Mobile Base Stations placement is a challenging problem to achieve high performance. Different approaches have been proposed; however, user required traffic is not considered in UAV placement. We propose a new feature to apply to K-means clustering to find the optimum clusters. User required traffic is defined as a new feature to assign users to the UAVs. Our simulation results show that UAVs could be placed closer to the high traffic users to achieve higher performance.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-03T00:24:56Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Amir Mirzaeinia</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mehdi Mirzaeinia</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mohammad Shekaramiz</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mostafa Hassanalian</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01285v1</id>\n",
      "    <title>Differentially Private Representation for NLP: Formal Guarantee and An Empirical Study on Privacy and Fairness</title>\n",
      "    <updated>2020-10-03T05:58:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01285v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01285v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>It has been demonstrated that hidden representation learned by a deep model can encode private information of the input, hence can be exploited to recover such information with reasonable accuracy. To address this issue, we propose a novel approach called Differentially Private Neural Representation (DPNR) to preserve the privacy of the extracted representation from text. DPNR utilises Differential Privacy (DP) to provide a formal privacy guarantee. Further, we show that masking words via dropout can further enhance privacy. To maintain utility of the learned representation, we integrate DP-noisy representation into a robust training process to derive a robust target model, which also helps for model fairness over various demographic variables. Experimental results on benchmark datasets under various parameter settings demonstrate that DPNR largely reduces privacy leakage without significantly sacrificing the main task performance.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-03T05:58:32Z</published>\n",
      "    <arxiv:comment>accepted to Findings of EMNLP 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Lingjuan Lyu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xuanli He</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yitong Li</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02406v1</id>\n",
      "    <title>Video Anomaly Detection Using Pre-Trained Deep Convolutional Neural Nets and Context Mining</title>\n",
      "    <updated>2020-10-06T00:26:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02406v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02406v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Anomaly detection is critically important for intelligent surveillance systems to detect in a timely manner any malicious activities. Many video anomaly detection approaches using deep learning methods focus on a single camera video stream with a fixed scenario. These deep learning methods use large-scale training data with large complexity. As a solution, in this paper, we show how to use pre-trained convolutional neural net models to perform feature extraction and context mining, and then use denoising autoencoder with relatively low model complexity to provide efficient and accurate surveillance anomaly detection, which can be useful for the resource-constrained devices such as edge devices of the Internet of Things (IoT). Our anomaly detection model makes decisions based on the high-level features derived from the selected embedded computer vision models such as object classification and object detection. Additionally, we derive contextual properties from the high-level features to further improve the performance of our video anomaly detection method. We use two UCSD datasets to demonstrate that our approach with relatively low model complexity can achieve comparable performance compared to the state-of-the-art approaches.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T00:26:14Z</published>\n",
      "    <arxiv:comment>Accepted by AICCSA 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Chongke Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sicong Shao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cihan Tunc</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Salim Hariri</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02468v1</id>\n",
      "    <title>Visualizing Color-wise Saliency of Black-Box Image Classification Models</title>\n",
      "    <updated>2020-10-06T04:27:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02468v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02468v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Image classification based on machine learning is being commonly used. However, a classification result given by an advanced method, including deep learning, is often hard to interpret. This problem of interpretability is one of the major obstacles in deploying a trained model in safety-critical systems. Several techniques have been proposed to address this problem; one of which is RISE, which explains a classification result by a heatmap, called a saliency map, which explains the significance of each pixel. We propose MC-RISE (Multi-Color RISE), which is an enhancement of RISE to take color information into account in an explanation. Our method not only shows the saliency of each pixel in a given image as the original RISE does, but the significance of color components of each pixel; a saliency map with color information is useful especially in the domain where the color information matters (e.g., traffic-sign recognition). We implemented MC-RISE and evaluate them using two datasets (GTSRB and ImageNet) to demonstrate the effectiveness of our methods in comparison with existing techniques for interpreting image classification results.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T04:27:18Z</published>\n",
      "    <arxiv:comment>To appear in ACCV 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Yuhki Hatakeyama</name>\n",
      "      <arxiv:affiliation>SenseTime Japan</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hiroki Sakuma</name>\n",
      "      <arxiv:affiliation>SenseTime Japan</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yoshinori Konishi</name>\n",
      "      <arxiv:affiliation>SenseTime Japan</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kohei Suenaga</name>\n",
      "      <arxiv:affiliation>Kyoto University</arxiv:affiliation>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02477v1</id>\n",
      "    <title>A Unified Deep Learning Framework for Short-Duration Speaker Verification in Adverse Environments</title>\n",
      "    <updated>2020-10-06T04:51:45Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02477v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02477v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Speaker verification (SV) has recently attracted considerable research interest due to the growing popularity of virtual assistants. At the same time, there is an increasing requirement for an SV system: it should be robust to short speech segments, especially in noisy and reverberant environments. In this paper, we consider one more important requirement for practical applications: the system should be robust to an audio stream containing long non-speech segments, where a voice activity detection (VAD) is not applied. To meet these two requirements, we introduce feature pyramid module (FPM)-based multi-scale aggregation (MSA) and self-adaptive soft VAD (SAS-VAD). We present the FPM-based MSA to deal with short speech segments in noisy and reverberant environments. Also, we use the SAS-VAD to increase the robustness to long non-speech segments. To further improve the robustness to acoustic distortions (i.e., noise and reverberation), we apply a masking-based speech enhancement (SE) method. We combine SV, VAD, and SE models in a unified deep learning framework and jointly train the entire network in an end-to-end manner. To the best of our knowledge, this is the first work combining these three models in a deep learning framework. We conduct experiments on Korean indoor (KID) and VoxCeleb datasets, which are corrupted by noise and reverberation. The results show that the proposed method is effective for SV in the challenging conditions and performs better than the baseline i-vector and deep speaker embedding systems.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T04:51:45Z</published>\n",
      "    <arxiv:comment>19 pages, 10 figures, 13 tables</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <arxiv:journal_ref>in IEEE Access, vol. 8, pp. 175448-175466, 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Youngmoon Jung</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yeunju Choi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hyungjun Lim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hoirin Kim</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/ACCESS.2020.3025941</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/ACCESS.2020.3025941\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02495v2</id>\n",
      "    <title>Joint Turn and Dialogue level User Satisfaction Estimation on Multi-Domain Conversations</title>\n",
      "    <updated>2020-10-08T21:10:47Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02495v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02495v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Dialogue level quality estimation is vital for optimizing data driven dialogue management. Current automated methods to estimate turn and dialogue level user satisfaction employ hand-crafted features and rely on complex annotation schemes, which reduce the generalizability of the trained models. We propose a novel user satisfaction estimation approach which minimizes an adaptive multi-task loss function in order to jointly predict turn-level Response Quality labels provided by experts and explicit dialogue-level ratings provided by end users. The proposed BiLSTM based deep neural net model automatically weighs each turn's contribution towards the estimated dialogue-level rating, implicitly encodes temporal dependencies, and removes the need to hand-craft features.\n",
      "  On dialogues sampled from 28 Alexa domains, two dialogue systems and three user groups, the joint dialogue-level satisfaction estimation model achieved up to an absolute 27% (0.43-&gt;0.70) and 7% (0.63-&gt;0.70) improvement in linear correlation performance over baseline deep neural net and benchmark Gradient boosting regression models, respectively.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T05:53:13Z</published>\n",
      "    <arxiv:comment>Findings of EMNLP, 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Praveen Kumar Bodigutla</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aditya Tiwari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Josep Valls Vargas</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lazaros Polymenakos</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Spyros Matsoukas</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02519v2</id>\n",
      "    <title>Improved Analysis of Clipping Algorithms for Non-convex Optimization</title>\n",
      "    <updated>2020-10-29T03:04:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02519v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02519v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Gradient clipping is commonly used in training deep neural networks partly due to its practicability in relieving the exploding gradient problem. Recently, \\citet{zhang2019gradient} show that clipped (stochastic) Gradient Descent (GD) converges faster than vanilla GD/SGD via introducing a new assumption called $(L_0, L_1)$-smoothness, which characterizes the violent fluctuation of gradients typically encountered in deep neural networks. However, their iteration complexities on the problem-dependent parameters are rather pessimistic, and theoretical justification of clipping combined with other crucial techniques, e.g. momentum acceleration, are still lacking. In this paper, we bridge the gap by presenting a general framework to study the clipping algorithms, which also takes momentum methods into consideration. We provide convergence analysis of the framework in both deterministic and stochastic setting, and demonstrate the tightness of our results by comparing them with existing lower bounds. Our results imply that the efficiency of clipping methods will not degenerate even in highly non-smooth regions of the landscape. Experiments confirm the superiority of clipping-based methods in deep learning tasks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T14:36:59Z</published>\n",
      "    <arxiv:comment>41 pages, 12 figures, to appear in NeurIPS 2020. arXiv admin note: text overlap with arXiv:1905.11881 by other authors</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Bohang Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jikai Jin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cong Fang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liwei Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02577v1</id>\n",
      "    <title>Memory and Computation-Efficient Kernel SVM via Binary Embedding and Ternary Model Coefficients</title>\n",
      "    <updated>2020-10-06T09:41:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02577v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02577v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Kernel approximation is widely used to scale up kernel SVM training and prediction. However, the memory and computation costs of kernel approximation models are still too high if we want to deploy them on memory-limited devices such as mobile phones, smartwatches, and IoT devices. To address this challenge, we propose a novel memory and computation-efficient kernel SVM model by using both binary embedding and binary model coefficients. First, we propose an efficient way to generate compact binary embedding of the data, preserving the kernel similarity. Second, we propose a simple but effective algorithm to learn a linear classification model with ternary coefficients that can support different types of loss function and regularizer. Our algorithm can achieve better generalization accuracy than existing works on learning binary coefficients since we allow coefficient to be $-1$, $0$, or $1$ during the training stage, and coefficient $0$ can be removed during model inference for binary classification. Moreover, we provide a detailed analysis of the convergence of our algorithm and the inference complexity of our model. The analysis shows that the convergence to a local optimum is guaranteed, and the inference complexity of our model is much lower than other competing methods. Our experimental results on five large real-world datasets have demonstrated that our proposed method can build accurate nonlinear SVM models with memory costs less than 30KB.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T09:41:54Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Zijian Lei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liang Lan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02598v1</id>\n",
      "    <title>Embedding Words in Non-Vector Space with Unsupervised Graph Learning</title>\n",
      "    <updated>2020-10-06T10:17:49Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02598v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02598v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe). While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to be revealed and encoded by word embeddings. We introduce GraphGlove: unsupervised graph word representations which are learned end-to-end. In our setting, each word is a node in a weighted graph and the distance between words is the shortest path distance between the corresponding nodes. We adopt a recent method learning a representation of data in the form of a differentiable weighted graph and use it to modify the GloVe training algorithm. We show that our graph-based representations substantially outperform vector-based methods on word similarity and analogy tasks. Our analysis reveals that the structure of the learned graphs is hierarchical and similar to that of WordNet, the geometry is highly non-trivial and contains subgraphs with different local topology.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T10:17:49Z</published>\n",
      "    <arxiv:comment>Accepted as a long paper for EMNLP 2020. 15 pages, 6 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Max Ryabinin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sergei Popov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liudmila Prokhorenkova</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Elena Voita</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02616v1</id>\n",
      "    <title>On the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers</title>\n",
      "    <updated>2020-10-06T10:54:00Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02616v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02616v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the representations of pre-trained models and thereby the linguistic knowledge they encode. This paper contributes towards closing this gap. We study three different pre-trained models: BERT, RoBERTa, and ALBERT, and investigate through sentence-level probing how fine-tuning affects their representations. We find that for some probing tasks fine-tuning leads to substantial changes in accuracy, possibly suggesting that fine-tuning introduces or even removes linguistic knowledge from a pre-trained model. These changes, however, vary greatly across different models, fine-tuning and probing tasks. Our analysis reveals that while fine-tuning indeed changes the representations of a pre-trained model and these changes are typically larger for higher layers, only in very few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of fine-tuning on probing require a careful interpretation.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T10:54:00Z</published>\n",
      "    <arxiv:comment>Accepted at Findings of EMNLP 2020 and BlackboxNLP 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Marius Mosbach</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anna Khokhlova</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michael A. Hedderich</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dietrich Klakow</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02675v1</id>\n",
      "    <title>Recovering Causal Structures from Low-Order Conditional Independencies</title>\n",
      "    <updated>2020-10-06T12:47:20Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02675v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02675v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>One of the common obstacles for learning causal models from data is that high-order conditional independence (CI) relationships between random variables are difficult to estimate. Since CI tests with conditioning sets of low order can be performed accurately even for a small number of observations, a reasonable approach to determine casual structures is to base merely on the low-order CIs. Recent research has confirmed that, e.g. in the case of sparse true causal models, structures learned even from zero- and first-order conditional independencies yield good approximations of the models. However, a challenging task here is to provide methods that faithfully explain a given set of low-order CIs. In this paper, we propose an algorithm which, for a given set of conditional independencies of order less or equal to $k$, where $k$ is a small fixed number, computes a faithful graphical representation of the given set. Our results complete and generalize the previous work on learning from pairwise marginal independencies. Moreover, they enable to improve upon the 0-1 graph model which, e.g. is heavily used in the estimation of genome networks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-06T12:47:20Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <arxiv:journal_ref>Published in Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI'20) New York, New York USA, pp. 10302-10309, AAAI Press, 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Marcel Wienöbst</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maciej Liśkiewicz</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1609/aaai.v34i06.6593</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1609/aaai.v34i06.6593\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.02748v1</id>\n",
      "    <title>Neural Generation of Blocks for Video Coding</title>\n",
      "    <updated>2020-10-05T04:40:33Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.02748v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.02748v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Well-trained generative neural networks (GNN) are very efficient at compressing visual information for static images in their learned parameters but not as efficient as inter- and intra-prediction for most video content. However, for content entering a frame, such as during panning or zooming out, and content with curves, irregular shapes, or fine detail, generation by a GNN can give better compression efficiency (lower rate-distortion). This paper proposes encoding content-specific learned parameters of a GNN within a video bitstream at specific times and using the GNN to generate content for specific ranges of blocks and frames. The blocks to generate are just the ones for which generation gives more efficient compression than inter- or intra- prediction. This approach maximizes the usefulness of the information contained in the learned parameters.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T04:40:33Z</published>\n",
      "    <arxiv:comment>12 pages, 4 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Jonah Probell</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01794v2</id>\n",
      "    <title>GenAug: Data Augmentation for Finetuning Text Generators</title>\n",
      "    <updated>2020-10-10T06:00:03Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01794v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01794v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we investigate data augmentation for text generation, which we call GenAug. Text generation and language modeling are important tasks within natural language processing, and are especially challenging for low-data regimes. We propose and evaluate various augmentation methods, including some that incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp Reviews. We also examine the relationship between the amount of augmentation and the quality of the generated text. We utilize several metrics that evaluate important aspects of the generated text including its diversity and fluency. Our experiments demonstrate that insertion of character-level synthetic noise and keyword replacement with hypernyms are effective augmentation methods, and that the quality of generations improves to a peak at approximately three times the amount of original data.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T05:46:39Z</published>\n",
      "    <arxiv:comment>EMNLP 2020 Deep Learning Inside Out (DeeLIO) Workshop; Code available at https://github.com/styfeng/GenAug</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Steven Y. Feng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Varun Gangal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dongyeop Kang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Teruko Mitamura</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eduard Hovy</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01799v2</id>\n",
      "    <title>Understanding Catastrophic Overfitting in Single-step Adversarial Training</title>\n",
      "    <updated>2020-12-15T08:41:08Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01799v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01799v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Although fast adversarial training has demonstrated both robustness and efficiency, the problem of \"catastrophic overfitting\" has been observed. This is a phenomenon in which, during single-step adversarial training, the robust accuracy against projected gradient descent (PGD) suddenly decreases to 0% after a few epochs, whereas the robust accuracy against fast gradient sign method (FGSM) increases to 100%. In this paper, we demonstrate that catastrophic overfitting is very closely related to the characteristic of single-step adversarial training which uses only adversarial examples with the maximum perturbation, and not all adversarial examples in the adversarial direction, which leads to decision boundary distortion and a highly curved loss surface. Based on this observation, we propose a simple method that not only prevents catastrophic overfitting, but also overrides the belief that it is difficult to prevent multi-step adversarial attacks with single-step adversarial training.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T06:13:35Z</published>\n",
      "    <arxiv:comment>Accepted to AAAI 2021. Preprint</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Hoki Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Woojin Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jaewook Lee</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01947v1</id>\n",
      "    <title>A Comparative Study of Existing and New Deep Learning Methods for Detecting Knee Injuries using the MRNet Dataset</title>\n",
      "    <updated>2020-10-05T12:27:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01947v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01947v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This work presents a comparative study of existing and new techniques to detect knee injuries by leveraging Stanford's MRNet Dataset. All approaches are based on deep learning and we explore the comparative performances of transfer learning and a deep residual network trained from scratch. We also exploit some characteristics of Magnetic Resonance Imaging (MRI) data by, for example, using a fixed number of slices or 2D images from each of the axial, coronal and sagittal planes as well as combining the three planes into one multi-plane network. Overall we achieved a performance of 93.4% AUC on the validation data by using the more recent deep learning architectures and data augmentation strategies. More flexible architectures are also proposed that might help with the development and training of models that process MRIs. We found that transfer learning and a carefully tuned data augmentation strategy were the crucial factors in determining best performance.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T12:27:18Z</published>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>David Azcona</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kevin McGuinness</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alan F. Smeaton</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01986v1</id>\n",
      "    <title>A Spherical Hidden Markov Model for Semantics-Rich Human Mobility Modeling</title>\n",
      "    <updated>2020-10-05T13:18:38Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01986v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01986v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study the problem of modeling human mobility from semantic trace data, wherein each GPS record in a trace is associated with a text message that describes the user's activity. Existing methods fall short in unveiling human movement regularities, because they either do not model the text data at all or suffer from text sparsity severely. We propose SHMM, a multi-modal spherical hidden Markov model for semantics-rich human mobility modeling. Under the hidden Markov assumption, SHMM models the generation process of a given trace by jointly considering the observed location, time, and text at each step of the trace. The distinguishing characteristic of SHMM is the text modeling part. We use fixed-size vector representations to encode the semantics of the text messages, and model the generation of the l2-normalized text embeddings on a unit sphere with the von Mises-Fisher (vMF) distribution. Compared with other alternatives like multi-variate Gaussian, our choice of the vMF distribution not only incurs much fewer parameters, but also better leverages the discriminative power of text embeddings in a directional metric space. The parameter inference for the vMF distribution is non-trivial since it involves functional inversion of ratios of Bessel functions. We theoretically prove that: 1) the classical Expectation-Maximization algorithm can work with vMF distributions; and 2) while closed-form solutions are hard to be obtained for the M-step, Newton's method is guaranteed to converge to the optimal solution with quadratic convergence rate. We have performed extensive experiments on both synthetic and real-life data. The results on synthetic data verify our theoretical analysis; while the results on real-life data demonstrate that SHMM learns meaningful semantics-rich mobility models, outperforms state-of-the-art mobility models for next location prediction, and incurs lower training cost.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T13:18:38Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Wanzheng Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chao Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shuochao Yao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaobin Gao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiawei Han</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01471v1</id>\n",
      "    <title>Deep Reinforcement Learning for Delay-Oriented IoT Task Scheduling in Space-Air-Ground Integrated Network</title>\n",
      "    <updated>2020-10-04T02:58:03Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01471v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01471v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we investigate a computing task scheduling problem in space-air-ground integrated network (SAGIN) for delay-oriented Internet of Things (IoT) services. In the considered scenario, an unmanned aerial vehicle (UAV) collects computing tasks from IoT devices and then makes online offloading decisions, in which the tasks can be processed at the UAV or offloaded to the nearby base station or the remote satellite. Our objective is to design a task scheduling policy that minimizes offloading and computing delay of all tasks given the UAV energy capacity constraint. To this end, we first formulate the online scheduling problem as an energy-constrained Markov decision process (MDP). Then, considering the task arrival dynamics, we develop a novel deep risk-sensitive reinforcement learning algorithm. Specifically, the algorithm evaluates the risk, which measures the energy consumption that exceeds the constraint, for each state and searches the optimal parameter weighing the minimization of delay and risk while learning the optimal policy. Extensive simulation results demonstrate that the proposed algorithm can reduce the task processing delay by up to 30% compared to probabilistic configuration methods while satisfying the UAV energy capacity constraint.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-04T02:58:03Z</published>\n",
      "    <arxiv:comment>14 pages, 8 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Conghao Zhou</name>\n",
      "      <arxiv:affiliation>Sherman</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wen Wu</name>\n",
      "      <arxiv:affiliation>Sherman</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hongli He</name>\n",
      "      <arxiv:affiliation>Sherman</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Peng Yang</name>\n",
      "      <arxiv:affiliation>Sherman</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Feng Lyu</name>\n",
      "      <arxiv:affiliation>Sherman</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nan Cheng</name>\n",
      "      <arxiv:affiliation>Sherman</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name> Xuemin</name>\n",
      "      <arxiv:affiliation>Sherman</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name> Shen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01478v1</id>\n",
      "    <title>Explanation Ontology in Action: A Clinical Use-Case</title>\n",
      "    <updated>2020-10-04T03:52:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01478v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01478v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We addressed the problem of a lack of semantic representation for user-centric explanations and different explanation types in our Explanation Ontology (https://purl.org/heals/eo). Such a representation is increasingly necessary as explainability has become an important problem in Artificial Intelligence with the emergence of complex methods and an uptake in high-precision and user-facing settings. In this submission, we provide step-by-step guidance for system designers to utilize our ontology, introduced in our resource track paper, to plan and model for explanations during the design of their Artificial Intelligence systems. We also provide a detailed example with our utilization of this guidance in a clinical setting.</summary>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-04T03:52:39Z</published>\n",
      "    <arxiv:comment>5 pages, 2 figures, 1 protocol</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.AI\"/>\n",
      "    <arxiv:journal_ref>International Semantic Web Conference, Poster and Demo Track, 2020</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Shruthi Chari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Oshani Seneviratne</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniel M. Gruen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Morgan A. Foreman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amar K. Das</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Deborah L. McGuinness</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01546v2</id>\n",
      "    <title>Feature Whitening via Gradient Transformation for Improved Convergence</title>\n",
      "    <updated>2020-11-08T08:35:17Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01546v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01546v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Feature whitening is a known technique for speeding up training of DNN. Under certain assumptions, whitening the activations reduces the Fisher information matrix to a simple identity matrix, in which case stochastic gradient descent is equivalent to the faster natural gradient descent. Due to the additional complexity resulting from transforming the layer inputs and their corresponding gradients in the forward and backward propagation, and from repeatedly computing the Eigenvalue decomposition (EVD), this method is not commonly used to date. In this work, we address the complexity drawbacks of feature whitening. Our contribution is twofold. First, we derive an equivalent method, which replaces the sample transformations by a transformation to the weight gradients, applied to every batch of B samples. The complexity is reduced by a factor of S=(2B), where S denotes the feature dimension of the layer output. As the batch size increases with distributed training, the benefit of using the proposed method becomes more compelling. Second, motivated by the theoretical relation between the condition number of the sample covariance matrix and the convergence speed, we derive an alternative sub-optimal algorithm which recursively reduces the condition number of the latter matrix. Compared to EVD, complexity is reduced by a factor of the input feature dimension M. We exemplify the proposed algorithms with ResNet-based networks for image classification demonstrated on the CIFAR and Imagenet datasets. Parallelizing the proposed algorithms is straightforward and we implement a distributed version thereof. Improved convergence, in terms of speed and attained accuracy, can be observed in our experiments.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-04T11:30:20Z</published>\n",
      "    <arxiv:comment>NeurIPS 2020 Workshop Beyond Backpropagation - Novel Ideas for Training Neural Architectures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Shmulik Markovich-Golan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Barak Battash</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Amit Bleiweiss</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01659v1</id>\n",
      "    <title>Data-efficient Online Classification with Siamese Networks and Active Learning</title>\n",
      "    <updated>2020-10-04T19:07:19Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01659v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01659v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>An ever increasing volume of data is nowadays becoming available in a streaming manner in many application areas, such as, in critical infrastructure systems, finance and banking, security and crime and web analytics. To meet this new demand, predictive models need to be built online where learning occurs on-the-fly. Online learning poses important challenges that affect the deployment of online classification systems to real-life problems. In this paper we investigate learning from limited labelled, nonstationary and imbalanced data in online classification. We propose a learning method that synergistically combines siamese neural networks and active learning. The proposed method uses a multi-sliding window approach to store data, and maintains separate and balanced queues for each class. Our study shows that the proposed method is robust to data nonstationarity and imbalance, and significantly outperforms baselines and state-of-the-art algorithms in terms of both learning speed and performance. Importantly, it is effective even when only 1% of the labels of the arriving instances are available.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-04T19:07:19Z</published>\n",
      "    <arxiv:comment>2020 International Joint Conference on Neural Networks (IJCNN), Glasgow, UK, 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Kleanthis Malialis</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christos G. Panayiotou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marios M. Polycarpou</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1109/IJCNN48605.2020.9206730</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1109/IJCNN48605.2020.9206730\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01664v1</id>\n",
      "    <title>Multi-Resolution Fusion and Multi-scale Input Priors Based Crowd Counting</title>\n",
      "    <updated>2020-10-04T19:30:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01664v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01664v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Crowd counting in still images is a challenging problem in practice due to huge crowd-density variations, large perspective changes, severe occlusion, and variable lighting conditions. The state-of-the-art patch rescaling module (PRM) based approaches prove to be very effective in improving the crowd counting performance. However, the PRM module requires an additional and compromising crowd-density classification process. To address these issues and challenges, the paper proposes a new multi-resolution fusion based end-to-end crowd counting network. It employs three deep-layers based columns/branches, each catering the respective crowd-density scale. These columns regularly fuse (share) the information with each other. The network is divided into three phases with each phase containing one or more columns. Three input priors are introduced to serve as an efficient and effective alternative to the PRM module, without requiring any additional classification operations. Along with the final crowd count regression head, the network also contains three auxiliary crowd estimation regression heads, which are strategically placed at each phase end to boost the overall performance. Comprehensive experiments on three benchmark datasets demonstrate that the proposed approach outperforms all the state-of-the-art models under the RMSE evaluation metric. The proposed approach also has better generalization capability with the best results during the cross-dataset experiments.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-04T19:30:13Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Usman Sajid</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wenchi Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Guanghui Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01732v1</id>\n",
      "    <title>Lipschitz Bounded Equilibrium Networks</title>\n",
      "    <updated>2020-10-05T01:00:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01732v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01732v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper introduces new parameterizations of equilibrium neural networks, i.e. networks defined by implicit equations. This model class includes standard multilayer and residual networks as special cases. The new parameterization admits a Lipschitz bound during training via unconstrained optimization: no projections or barrier functions are required. Lipschitz bounds are a common proxy for robustness and appear in many generalization bounds. Furthermore, compared to previous works we show well-posedness (existence of solutions) under less restrictive conditions on the network weights and more natural assumptions on the activation functions: that they are monotone and slope restricted. These results are proved by establishing novel connections with convex optimization, operator splitting on non-Euclidean spaces, and contracting neural ODEs. In image classification experiments we show that the Lipschitz bounds are very accurate and improve robustness to adversarial attacks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T01:00:40Z</published>\n",
      "    <arxiv:comment>Conference submission, 19 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Max Revay</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruigang Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ian R. Manchester</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01766v2</id>\n",
      "    <title>DEMI: Discriminative Estimator of Mutual Information</title>\n",
      "    <updated>2020-11-30T02:59:02Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01766v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01766v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Estimating mutual information between continuous random variables is often intractable and extremely challenging for high-dimensional data. Recent progress has leveraged neural networks to optimize variational lower bounds on mutual information. Although showing promise for this difficult problem, the variational methods have been theoretically and empirically proven to have serious statistical limitations: 1) many methods struggle to produce accurate estimates when the underlying mutual information is either low or high; 2) the resulting estimators may suffer from high variance. Our approach is based on training a classifier that provides the probability that a data sample pair is drawn from the joint distribution rather than from the product of its marginal distributions. Moreover, we establish a direct connection between mutual information and the average log odds estimate produced by the classifier on a test set, leading to a simple and accurate estimator of mutual information. We show theoretically that our method and other variational approaches are equivalent when they achieve their optimum, while our method sidesteps the variational bound. Empirical results demonstrate high accuracy of our approach and the advantages of our estimator in the context of representation learning. Our demo is available at https://github.com/RayRuizhiLiao/demi_mi_estimator.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T04:19:27Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Ruizhi Liao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniel Moyer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Polina Golland</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>William M. Wells</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01787v1</id>\n",
      "    <title>Improving Relational Regularized Autoencoders with Spherical Sliced Fused Gromov Wasserstein</title>\n",
      "    <updated>2020-10-05T05:26:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01787v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01787v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Relational regularized autoencoder (RAE) is a framework to learn the distribution of data by minimizing a reconstruction loss together with a relational regularization on the latent space. A recent attempt to reduce the inner discrepancy between the prior and aggregated posterior distributions is to incorporate sliced fused Gromov-Wasserstein (SFG) between these distributions. That approach has a weakness since it treats every slicing direction similarly, meanwhile several directions are not useful for the discriminative task. To improve the discrepancy and consequently the relational regularization, we propose a new relational discrepancy, named spherical sliced fused Gromov Wasserstein (SSFG), that can find an important area of projections characterized by a von Mises-Fisher distribution. Then, we introduce two variants of SSFG to improve its performance. The first variant, named mixture spherical sliced fused Gromov Wasserstein (MSSFG), replaces the vMF distribution by a mixture of von Mises-Fisher distributions to capture multiple important areas of directions that are far from each other. The second variant, named power spherical sliced fused Gromov Wasserstein (PSSFG), replaces the vMF distribution by a power spherical distribution to improve the sampling time in high dimension settings. We then apply the new discrepancies to the RAE framework to achieve its new variants. Finally, we conduct extensive experiments to show that the new proposed autoencoders have favorable performance in learning latent manifold structure, image generation, and reconstruction.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-05T05:26:50Z</published>\n",
      "    <arxiv:comment>39 pages, 19 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Khai Nguyen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Son Nguyen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nhat Ho</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tung Pham</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hung Bui</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01342v1</id>\n",
      "    <title>End-to-End Training of CNN Ensembles for Person Re-Identification</title>\n",
      "    <updated>2020-10-03T12:40:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01342v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01342v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose an end-to-end ensemble method for person re-identification (ReID) to address the problem of overfitting in discriminative models. These models are known to converge easily, but they are biased to the training data in general and may produce a high model variance, which is known as overfitting. The ReID task is more prone to this problem due to the large discrepancy between training and test distributions. To address this problem, our proposed ensemble learning framework produces several diverse and accurate base learners in a single DenseNet. Since most of the costly dense blocks are shared, our method is computationally efficient, which makes it favorable compared to the conventional ensemble models. Experiments on several benchmark datasets demonstrate that our method achieves state-of-the-art results. Noticeable performance improvements, especially on relatively small datasets, indicate that the proposed method deals with the overfitting problem effectively.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-03T12:40:13Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <arxiv:journal_ref>Pattern Recognition, 104, 107319 (2020)</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Ayse Serbetci</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yusuf Sinan Akgul</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.01401v1</id>\n",
      "    <title>Adversarial and Natural Perturbations for General Robustness</title>\n",
      "    <updated>2020-10-03T17:53:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.01401v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.01401v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper we aim to explore the general robustness of neural network classifiers by utilizing adversarial as well as natural perturbations. Different from previous works which mainly focus on studying the robustness of neural networks against adversarial perturbations, we also evaluate their robustness on natural perturbations before and after robustification. After standardizing the comparison between adversarial and natural perturbations, we demonstrate that although adversarial training improves the performance of the networks against adversarial perturbations, it leads to drop in the performance for naturally perturbed samples besides clean samples. In contrast, natural perturbations like elastic deformations, occlusions and wave does not only improve the performance against natural perturbations, but also lead to improvement in the performance for the adversarial perturbations. Additionally they do not drop the accuracy on the clean images.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-03T17:53:18Z</published>\n",
      "    <arxiv:comment>Currently under review</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Sadaf Gulshad</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jan Hendrik Metzen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Arnold Smeulders</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14230v1</id>\n",
      "    <title>A Comparison of Discrete Latent Variable Models for Speech Representation Learning</title>\n",
      "    <updated>2020-10-24T01:22:14Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14230v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14230v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Neural latent variable models enable the discovery of interesting structure in speech audio data. This paper presents a comparison of two different approaches which are broadly based on predicting future time-steps or auto-encoding the input signal. Our study compares the representations learned by vq-vae and vq-wav2vec in terms of sub-word unit discovery and phoneme recognition performance. Results show that future time-step prediction with vq-wav2vec achieves better performance. The best system achieves an error rate of 13.22 on the ZeroSpeech 2019 ABX phoneme discrimination challenge</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-24T01:22:14Z</published>\n",
      "    <arxiv:comment>7 pages, 4 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Henry Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexei Baevski</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michael Auli</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14233v1</id>\n",
      "    <title>Align-Refine: Non-Autoregressive Speech Recognition via Iterative Realignment</title>\n",
      "    <updated>2020-10-24T09:35:37Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14233v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14233v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Non-autoregressive models greatly improve decoding speed over typical sequence-to-sequence models, but suffer from degraded performance. Infilling and iterative refinement models make up some of this gap by editing the outputs of a non-autoregressive model, but are constrained in the edits that they can make. We propose iterative realignment, where refinements occur over latent alignments rather than output sequence space. We demonstrate this in speech recognition with Align-Refine, an end-to-end Transformer-based model which refines connectionist temporal classification (CTC) alignments to allow length-changing insertions and deletions. Align-Refine outperforms Imputer and Mask-CTC, matching an autoregressive baseline on WSJ at 1/14th the real-time factor and attaining a LibriSpeech test-other WER of 9.0% without an LM. Our model is strong even in one iteration with a shallower decoder.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-24T09:35:37Z</published>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Ethan A. Chi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Julian Salazar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Katrin Kirchhoff</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14234v2</id>\n",
      "    <title>Global Sentiment Analysis Of COVID-19 Tweets Over Time</title>\n",
      "    <updated>2020-11-10T08:24:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14234v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14234v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The Coronavirus pandemic has affected the normal course of life. People around the world have taken to social media to express their opinions and general emotions regarding this phenomenon that has taken over the world by storm. The social networking site, Twitter showed an unprecedented increase in tweets related to the novel Coronavirus in a very short span of time. This paper presents the global sentiment analysis of tweets related to Coronavirus and how the sentiment of people in different countries has changed over time. Furthermore, to determine the impact of Coronavirus on daily aspects of life, tweets related to Work From Home (WFH) and Online Learning were scraped and the change in sentiment over time was observed. In addition, various Machine Learning models such as Long Short Term Memory (LSTM) and Artificial Neural Networks (ANN) were implemented for sentiment classification and their accuracies were determined. Exploratory data analysis was also performed for a dataset providing information about the number of confirmed cases on a per-day basis in a few of the worst-hit countries to provide a comparison between the change in sentiment with the change in cases since the start of this pandemic till June 2020.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-27T12:10:10Z</published>\n",
      "    <arxiv:comment>7 pages, 20 figures, Submitted to International journal of Data Science and Analytics</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Muvazima Mansoor</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kirthika Gurumurthy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anantharam R U</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>V R Badri Prasad</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14242v1</id>\n",
      "    <title>Deep generative factorization for speech signal</title>\n",
      "    <updated>2020-10-27T12:27:58Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14242v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14242v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Various information factors are blended in speech signals, which forms the primary difficulty for most speech information processing tasks. An intuitive idea is to factorize speech signal into individual information factors (e.g., phonetic content and speaker trait), though it turns out to be highly challenging. This paper presents a speech factorization approach based on a novel factorial discriminative normalization flow model (factorial DNF). Experiments conducted on a two-factor case that involves phonetic content and speaker trait demonstrates that the proposed factorial DNF has powerful capability to factorize speech signals and outperforms several comparative models in terms of information representation and manipulation.</summary>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-27T12:27:58Z</published>\n",
      "    <arxiv:comment>Submitted to ICASSP 2021</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.SD\"/>\n",
      "    <author>\n",
      "      <name>Haoran Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lantian Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yunqi Cai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yang Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Thomas Fang Zheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dong Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14322v1</id>\n",
      "    <title>An efficient nonconvex reformulation of stagewise convex optimization problems</title>\n",
      "    <updated>2020-10-27T14:30:32Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14322v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14322v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Convex optimization problems with staged structure appear in several contexts, including optimal control, verification of deep neural networks, and isotonic regression. Off-the-shelf solvers can solve these problems but may scale poorly. We develop a nonconvex reformulation designed to exploit this staged structure. Our reformulation has only simple bound constraints, enabling solution via projected gradient methods and their accelerated variants. The method automatically generates a sequence of primal and dual feasible solutions to the original convex problem, making optimality certification easy. We establish theoretical properties of the nonconvex formulation, showing that it is (almost) free of spurious local minima and has the same global optimum as the convex problem. We modify PGD to avoid spurious local minimizers so it always converges to the global minimizer. For neural network verification, our approach obtains small duality gaps in only a few gradient steps. Consequently, it can quickly solve large-scale verification problems faster than both off-the-shelf and specialized solvers.</summary>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-27T14:30:32Z</published>\n",
      "    <arxiv:comment>First and second authors made equal contribution. To appear in Neurips 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"math.OC\"/>\n",
      "    <author>\n",
      "      <name>Rudy Bunel</name>\n",
      "      <arxiv:affiliation>Dj</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Oliver Hinder</name>\n",
      "      <arxiv:affiliation>Dj</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Srinadh Bhojanapalli</name>\n",
      "      <arxiv:affiliation>Dj</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name> Krishnamurthy</name>\n",
      "      <arxiv:affiliation>Dj</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name> Dvijotham</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14323v1</id>\n",
      "    <title>Sub-sampling for Efficient Non-Parametric Bandit Exploration</title>\n",
      "    <updated>2020-10-27T14:31:55Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14323v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14323v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper we propose the first multi-armed bandit algorithm based on re-sampling that achieves asymptotically optimal regret simultaneously for different families of arms (namely Bernoulli, Gaussian and Poisson distributions). Unlike Thompson Sampling which requires to specify a different prior to be optimal in each case, our proposal RB-SDA does not need any distribution-dependent tuning. RB-SDA belongs to the family of Sub-sampling Duelling Algorithms (SDA) which combines the sub-sampling idea first used by the BESA [1] and SSMC [2] algorithms with different sub-sampling schemes. In particular, RB-SDA uses Random Block sampling. We perform an experimental study assessing the flexibility and robustness of this promising novel approach for exploration in bandit models.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-27T14:31:55Z</published>\n",
      "    <arxiv:comment>NeurIPS 2020, Dec 2020, Vancouver, Canada</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Dorian Baudry</name>\n",
      "      <arxiv:affiliation>CNRS, CRIStAL, SEQUEL</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Emilie Kaufmann</name>\n",
      "      <arxiv:affiliation>CNRS, CRIStAL, SEQUEL</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Odalric-Ambrym Maillard</name>\n",
      "      <arxiv:affiliation>SEQUEL</arxiv:affiliation>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.13993v2</id>\n",
      "    <title>Combining Label Propagation and Simple Models Out-performs Graph Neural Networks</title>\n",
      "    <updated>2020-11-02T19:42:34Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.13993v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.13993v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Graph Neural Networks (GNNs) are the predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an \"error correlation\" that spreads residual errors in training data to correct errors in test data and (ii) a \"prediction correlation\" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C&amp;S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as was done in traditional techniques) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains. Our code for the OGB results is at https://github.com/Chillee/CorrectAndSmooth.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-27T02:10:52Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Qian Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Horace He</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abhay Singh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ser-Nam Lim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Austin R. Benson</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14002v1</id>\n",
      "    <title>Graph Blind Deconvolution with Sparseness Constraint</title>\n",
      "    <updated>2020-10-27T02:21:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14002v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14002v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose a blind deconvolution method for signals on graphs, with the exact sparseness constraint for the original signal. Graph blind deconvolution is an algorithm for estimating the original signal on a graph from a set of blurred and noisy measurements. Imposing a constraint on the number of nonzero elements is desirable for many different applications. This paper deals with the problem with constraints placed on the exact number of original sources, which is given by an optimization problem with an $\\ell_0$ norm constraint. We solve this non-convex optimization problem using the ADMM iterative solver. Numerical experiments using synthetic signals demonstrate the effectiveness of the proposed method.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-27T02:21:53Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Kazuma Iwata</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Koki Yamada</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuichi Tanaka</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14023v1</id>\n",
      "    <title>FaceLeaks: Inference Attacks against Transfer Learning Models via Black-box Queries</title>\n",
      "    <updated>2020-10-27T03:02:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14023v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14023v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Transfer learning is a useful machine learning framework that allows one to build task-specific models (student models) without significantly incurring training costs using a single powerful model (teacher model) pre-trained with a large amount of data. The teacher model may contain private data, or interact with private inputs. We investigate if one can leak or infer such private information without interacting with the teacher model directly. We describe such inference attacks in the context of face recognition, an application of transfer learning that is highly sensitive to personal privacy.\n",
      "  Under black-box and realistic settings, we show that existing inference techniques are ineffective, as interacting with individual training instances through the student models does not reveal information about the teacher. We then propose novel strategies to infer from aggregate-level information. Consequently, membership inference attacks on the teacher model are shown to be possible, even when the adversary has access only to the student models.\n",
      "  We further demonstrate that sensitive attributes can be inferred, even in the case where the adversary has limited auxiliary information. Finally, defensive strategies are discussed and evaluated. Our extensive study indicates that information leakage is a real privacy threat to the transfer learning framework widely used in real-life situations.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-27T03:02:40Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Seng Pei Liew</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tsubasa Takahashi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14054v1</id>\n",
      "    <title>A Probabilistic Representation of Deep Learning for Improving The Information Theoretic Interpretability</title>\n",
      "    <updated>2020-10-27T05:14:28Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14054v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14054v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we propose a probabilistic representation of MultiLayer Perceptrons (MLPs) to improve the information-theoretic interpretability. Above all, we demonstrate that the activations being i.i.d. is not valid for all the hidden layers of MLPs, thus the existing mutual information estimators based on non-parametric inference methods, e.g., empirical distributions and Kernel Density Estimate (KDE), are invalid for measuring the information flow in MLPs. Moreover, we introduce explicit probabilistic explanations for MLPs: (i) we define the probability space (Omega_F, t, P_F) for a fully connected layer f and demonstrate the great effect of an activation function on the probability measure P_F ; (ii) we prove the entire architecture of MLPs as a Gibbs distribution P; and (iii) the back-propagation aims to optimize the sample space Omega_F of all the fully connected layers of MLPs for learning an optimal Gibbs distribution P* to express the statistical connection between the input and the label. Based on the probabilistic explanations for MLPs, we improve the information-theoretic interpretability of MLPs in three aspects: (i) the random variable of f is discrete and the corresponding entropy is finite; (ii) the information bottleneck theory cannot correctly explain the information flow in MLPs if we take into account the back-propagation; and (iii) we propose novel information-theoretic explanations for the generalization of MLPs. Finally, we demonstrate the proposed probabilistic representation and information-theoretic explanations for MLPs in a synthetic dataset and benchmark datasets.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-27T05:14:28Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Xinjie Lan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kenneth E. Barner</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14149v1</id>\n",
      "    <title>Active Learning for Noisy Data Streams Using Weak and Strong Labelers</title>\n",
      "    <updated>2020-10-27T09:18:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14149v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14149v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Labeling data correctly is an expensive and challenging task in machine learning, especially for on-line data streams. Deep learning models especially require a large number of clean labeled data that is very difficult to acquire in real-world problems. Choosing useful data samples to label while minimizing the cost of labeling is crucial to maintain efficiency in the training process. When confronted with multiple labelers with different expertise and respective labeling costs, deciding which labeler to choose is nontrivial. In this paper, we consider a novel weak and strong labeler problem inspired by humans natural ability for labeling, in the presence of data streams with noisy labels and constrained by a limited budget. We propose an on-line active learning algorithm that consists of four steps: filtering, adding diversity, informative sample selection, and labeler selection. We aim to filter out the suspicious noisy samples and spend the budget on the diverse informative data using strong and weak labelers in a cost-effective manner. We derive a decision function that measures the information gain by combining the informativeness of individual samples and model confidence. We evaluate our proposed algorithm on the well-known image classification datasets CIFAR10 and CIFAR100 with up to 60% noise. Experiments show that by intelligently deciding which labeler to query, our algorithm maintains the same accuracy compared to the case of having only one of the labelers available while spending less of the budget.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-27T09:18:35Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Taraneh Younesian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dick Epema</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lydia Y. Chen</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.13269v1</id>\n",
      "    <title>Revisiting convolutional neural network on graphs with polynomial approximations of Laplace-Beltrami spectral filtering</title>\n",
      "    <updated>2020-10-26T01:18:05Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.13269v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.13269v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper revisits spectral graph convolutional neural networks (graph-CNNs) given in Defferrard (2016) and develops the Laplace-Beltrami CNN (LB-CNN) by replacing the graph Laplacian with the LB operator. We then define spectral filters via the LB operator on a graph. We explore the feasibility of Chebyshev, Laguerre, and Hermite polynomials to approximate LB-based spectral filters and define an update of the LB operator for pooling in the LBCNN. We employ the brain image data from Alzheimer's Disease Neuroimaging Initiative (ADNI) and demonstrate the use of the proposed LB-CNN. Based on the cortical thickness of the ADNI dataset, we showed that the LB-CNN didn't improve classification accuracy compared to the spectral graph-CNN. The three polynomials had a similar computational cost and showed comparable classification accuracy in the LB-CNN or spectral graph-CNN. Our findings suggest that even though the shapes of the three polynomials are different, deep learning architecture allows us to learn spectral filters such that the classification performance is not dependent on the type of the polynomials or the operators (graph Laplacian and LB operator).</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-26T01:18:05Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Shih-Gu Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Moo K. Chung</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anqi Qiu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alzheimer's Disease Neuroimaging Initiative</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.13271v1</id>\n",
      "    <title>Interpreting Uncertainty in Model Predictions For COVID-19 Diagnosis</title>\n",
      "    <updated>2020-10-26T01:27:29Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.13271v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.13271v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>COVID-19, due to its accelerated spread has brought in the need to use assistive tools for faster diagnosis in addition to typical lab swab testing. Chest X-Rays for COVID cases tend to show changes in the lungs such as ground glass opacities and peripheral consolidations which can be detected by deep neural networks. However, traditional convolutional networks use point estimate for predictions, lacking in capture of uncertainty, which makes them less reliable for adoption. There have been several works so far in predicting COVID positive cases with chest X-Rays. However, not much has been explored on quantifying the uncertainty of these predictions, interpreting uncertainty, and decomposing this to model or data uncertainty. To address these needs, we develop a visualization framework to address interpretability of uncertainty and its components, with uncertainty in predictions computed with a Bayesian Convolutional Neural Network. This framework aims to understand the contribution of individual features in the Chest-X-Ray images to predictive uncertainty. Providing this as an assistive tool can help the radiologist understand why the model came up with a prediction and whether the regions of interest captured by the model for the specific prediction are of significance in diagnosis. We demonstrate the usefulness of the tool in chest x-ray interpretation through several test cases from a benchmark dataset.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-26T01:27:29Z</published>\n",
      "    <arxiv:comment>Submitted to ISBI 2021</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Gayathiri Murugamoorthy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Naimul Khan</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.13297v1</id>\n",
      "    <title>Discriminatively Constrained Semi-supervised Multi-view Nonnegative Matrix Factorization with Graph Regularization</title>\n",
      "    <updated>2020-10-26T02:58:11Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.13297v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.13297v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In recent years, semi-supervised multi-view nonnegative matrix factorization (MVNMF) algorithms have achieved promising performances for multi-view clustering. While most of semi-supervised MVNMFs have failed to effectively consider discriminative information among clusters and feature alignment from multiple views simultaneously. In this paper, a novel Discriminatively Constrained Semi-Supervised Multi-View Nonnegative Matrix Factorization (DCS^2MVNMF) is proposed. Specifically, a discriminative weighting matrix is introduced for the auxiliary matrix of each view, which enhances the inter-class distinction. Meanwhile, a new graph regularization is constructed with the label and geometrical information. In addition, we design a new feature scale normalization strategy to align the multiple views and complete the corresponding iterative optimization schemes. Extensive experiments conducted on several real world multi-view datasets have demonstrated the effectiveness of the proposed method.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-26T02:58:11Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Guosheng Cui</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruxin Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dan Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ye Li</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.13369v1</id>\n",
      "    <title>Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping</title>\n",
      "    <updated>2020-10-26T06:50:07Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.13369v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.13369v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recently, Transformer-based language models have demonstrated remarkable performance across many NLP domains. However, the unsupervised pre-training step of these models suffers from unbearable overall computational expenses. Current methods for accelerating the pre-training either rely on massive parallelism with advanced hardware or are not applicable to language modeling. In this work, we propose a method based on progressive layer dropping that speeds the training of Transformer-based language models, not at the cost of excessive hardware resources but from model architecture change and training technique boosted efficiency. Extensive experiments on BERT show that the proposed method achieves a 24% time reduction on average per sample and allows the pre-training to be 2.5 times faster than the baseline to get a similar accuracy on downstream tasks. While being faster, our pre-trained models are equipped with strong knowledge transferability, achieving comparable and sometimes higher GLUE score than the baseline when pre-trained with the same number of samples.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-26T06:50:07Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Minjia Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuxiong He</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.13527v1</id>\n",
      "    <title>Robust Disentanglement of a Few Factors at a Time</title>\n",
      "    <updated>2020-10-26T12:34:23Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.13527v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.13527v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Disentanglement is at the forefront of unsupervised learning, as disentangled representations of data improve generalization, interpretability, and performance in downstream tasks. Current unsupervised approaches remain inapplicable for real-world datasets since they are highly variable in their performance and fail to reach levels of disentanglement of (semi-)supervised approaches. We introduce population-based training (PBT) for improving consistency in training variational autoencoders (VAEs) and demonstrate the validity of this approach in a supervised setting (PBT-VAE). We then use Unsupervised Disentanglement Ranking (UDR) as an unsupervised heuristic to score models in our PBT-VAE training and show how models trained this way tend to consistently disentangle only a subset of the generative factors. Building on top of this observation we introduce the recursive rPU-VAE approach. We train the model until convergence, remove the learned factors from the dataset and reiterate. In doing so, we can label subsets of the dataset with the learned factors and consecutively use these labels to train one model that fully disentangles the whole dataset. With this approach, we show striking improvement in state-of-the-art unsupervised disentanglement performance and robustness across multiple datasets and metrics.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-26T12:34:23Z</published>\n",
      "    <arxiv:comment>The first two authors contributed equally. Code is available at this url https://github.com/besterma/robust_disentanglement</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Benjamin Estermann</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Markus Marks</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mehmet Fatih Yanik</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14611v1</id>\n",
      "    <title>Hybrid Backpropagation Parallel Reservoir Networks</title>\n",
      "    <updated>2020-10-27T21:03:35Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14611v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14611v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In many real-world applications, fully-differentiable RNNs such as LSTMs and GRUs have been widely deployed to solve time series learning tasks. These networks train via Backpropagation Through Time, which can work well in practice but involves a biologically unrealistic unrolling of the network in time for gradient updates, are computationally expensive, and can be hard to tune. A second paradigm, Reservoir Computing, keeps the recurrent weight matrix fixed and random. Here, we propose a novel hybrid network, which we call Hybrid Backpropagation Parallel Echo State Network (HBP-ESN) which combines the effectiveness of learning random temporal features of reservoirs with the readout power of a deep neural network with batch normalization. We demonstrate that our new network outperforms LSTMs and GRUs, including multi-layer \"deep\" versions of these networks, on two complex real-world multi-dimensional time series datasets: gesture recognition using skeleton keypoints from ChaLearn, and the DEAP dataset for emotion recognition from EEG measurements. We show also that the inclusion of a novel meta-ring structure, which we call HBP-ESN M-Ring, achieves similar performance to one large reservoir while decreasing the memory required by an order of magnitude. We thus offer this new hybrid reservoir deep learning paradigm as a new alternative direction for RNN learning of temporal or sequential data.</summary>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-27T21:03:35Z</published>\n",
      "    <arxiv:primary_category term=\"cs.NE\"/>\n",
      "    <author>\n",
      "      <name>Matthew Evanusa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Snehesh Shrestha</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michelle Girvan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cornelia Fermüller</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yiannis Aloimonos</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14657v1</id>\n",
      "    <title>Temporal Difference Learning as Gradient Splitting</title>\n",
      "    <updated>2020-10-27T22:50:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14657v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14657v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Temporal difference learning with linear function approximation is a popular method to obtain a low-dimensional approximation of the value function of a policy in a Markov Decision Process. We give a new interpretation of this method in terms of a splitting of the gradient of an appropriately chosen function. As a consequence of this interpretation, convergence proofs for gradient descent can be applied almost verbatim to temporal difference learning. Beyond giving a new, fuller explanation of why temporal difference works, our interpretation also yields improved convergence times. We consider the setting with $1/\\sqrt{T}$ step-size, where previous comparable finite-time convergence time bounds for temporal difference learning had the multiplicative factor $1/(1-γ)$ in front of the bound, with $γ$ being the discount factor. We show that a minor variation on TD learning which estimates the mean of the value function separately has a convergence time where $1/(1-γ)$ only multiplies an asymptotically negligible term.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-27T22:50:39Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Rui Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alex Olshevsky</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14664v2</id>\n",
      "    <title>System Identification via Meta-Learning in Linear Time-Varying Environments</title>\n",
      "    <updated>2020-11-25T21:09:29Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14664v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14664v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>System identification is a fundamental problem in reinforcement learning, control theory and signal processing, and the non-asymptotic analysis of the corresponding sample complexity is challenging and elusive, even for linear time-varying (LTV) systems. To tackle this challenge, we develop an episodic block model for the LTV system where the model parameters remain constant within each block but change from block to block. Based on the observation that the model parameters across different blocks are related, we treat each episodic block as a learning task and then run meta-learning over many blocks for system identification, using two steps, namely offline meta-learning and online adaptation. We carry out a comprehensive non-asymptotic analysis of the performance of meta-learning based system identification. To deal with the technical challenges rooted in the sample correlation and small sample sizes in each block, we devise a new two-scale martingale small-ball approach for offline meta-learning, for arbitrary model correlation structure across blocks. We then quantify the finite time error of online adaptation by leveraging recent advances in linear stochastic approximation with correlated samples.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-27T23:02:35Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Sen Lin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hang Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Junshan Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14700v1</id>\n",
      "    <title>Sparse Symmetric Tensor Regression for Functional Connectivity Analysis</title>\n",
      "    <updated>2020-10-28T02:07:39Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14700v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14700v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Tensor regression models, such as CP regression and Tucker regression, have many successful applications in neuroimaging analysis where the covariates are of ultrahigh dimensionality and possess complex spatial structures. The high-dimensional covariate arrays, also known as tensors, can be approximated by low-rank structures and fit into the generalized linear models. The resulting tensor regression achieves a significant reduction in dimensionality while remaining efficient in estimation and prediction. Brain functional connectivity is an essential measure of brain activity and has shown significant association with neurological disorders such as Alzheimer's disease. The symmetry nature of functional connectivity is a property that has not been explored in previous tensor regression models. In this work, we propose a sparse symmetric tensor regression that further reduces the number of free parameters and achieves superior performance over symmetrized and ordinary CP regression, under a variety of simulation settings. We apply the proposed method to a study of Alzheimer's disease (AD) and normal ageing from the Berkeley Aging Cohort Study (BACS) and detect two regions of interest that have been identified important to AD.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-28T02:07:39Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Da Xu</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14713v1</id>\n",
      "    <title>CompRess: Self-Supervised Learning by Compressing Representations</title>\n",
      "    <updated>2020-10-28T02:49:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14713v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14713v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Self-supervised learning aims to learn good representations with unlabeled data. Recent works have shown that larger models benefit more from self-supervised learning than smaller models. As a result, the gap between supervised and self-supervised learning has been greatly reduced for larger models. In this work, instead of designing a new pseudo task for self-supervised learning, we develop a model compression method to compress an already learned, deep self-supervised model (teacher) to a smaller one (student). We train the student model so that it mimics the relative similarity between the data points in the teacher's embedding space. For AlexNet, our method outperforms all previous methods including the fully supervised model on ImageNet linear evaluation (59.0% compared to 56.5%) and on nearest neighbor evaluation (50.7% compared to 41.4%). To the best of our knowledge, this is the first time a self-supervised AlexNet has outperformed supervised one on ImageNet classification. Our code is available here: https://github.com/UMBCvision/CompRess</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-28T02:49:18Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Soroush Abbasi Koohpayegani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ajinkya Tejankar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hamed Pirsiavash</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.13639v1</id>\n",
      "    <title>Stochastic Optimization with Laggard Data Pipelines</title>\n",
      "    <updated>2020-10-26T14:55:31Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.13639v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.13639v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>State-of-the-art optimization is steadily shifting towards massively parallel pipelines with extremely large batch sizes. As a consequence, CPU-bound preprocessing and disk/memory/network operations have emerged as new performance bottlenecks, as opposed to hardware-accelerated gradient computations. In this regime, a recently proposed approach is data echoing (Choi et al., 2019), which takes repeated gradient steps on the same batch while waiting for fresh data to arrive from upstream. We provide the first convergence analyses of \"data-echoed\" extensions of common optimization methods, showing that they exhibit provable improvements over their synchronous counterparts. Specifically, we show that in convex optimization with stochastic minibatches, data echoing affords speedups on the curvature-dominated part of the convergence rate, while maintaining the optimal statistical rate.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-26T14:55:31Z</published>\n",
      "    <arxiv:comment>Published as a conference paper at NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Naman Agarwal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rohan Anil</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tomer Koren</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kunal Talwar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cyril Zhang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.13903v1</id>\n",
      "    <title>T$^2$-Net: A Semi-supervised Deep Model for Turbulence Forecasting</title>\n",
      "    <updated>2020-10-26T21:14:15Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.13903v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.13903v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Accurate air turbulence forecasting can help airlines avoid hazardous turbulence, guide the routes that keep passengers safe, maximize efficiency, and reduce costs. Traditional turbulence forecasting approaches heavily rely on painstakingly customized turbulence indexes, which are less effective in dynamic and complex weather conditions. The recent availability of high-resolution weather data and turbulence records allows more accurate forecasting of the turbulence in a data-driven way. However, it is a non-trivial task for developing a machine learning based turbulence forecasting system due to two challenges: (1) Complex spatio-temporal correlations, turbulence is caused by air movement with complex spatio-temporal patterns, (2) Label scarcity, very limited turbulence labels can be obtained. To this end, in this paper, we develop a unified semi-supervised framework, T$^2$-Net, to address the above challenges. Specifically, we first build an encoder-decoder paradigm based on the convolutional LSTM to model the spatio-temporal correlations. Then, to tackle the label scarcity problem, we propose a novel Dual Label Guessing method to take advantage of massive unlabeled turbulence data. It integrates complementary signals from the main Turbulence Forecasting task and the auxiliary Turbulence Detection task to generate pseudo-labels, which are dynamically utilized as additional training data. Finally, extensive experimental results on a real-world turbulence dataset validate the superiority of our method on turbulence forecasting.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-26T21:14:15Z</published>\n",
      "    <arxiv:comment>Accepted by ICDM 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Denghui Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yanchi Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wei Cheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bo Zong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jingchao Ni</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhengzhang Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Haifeng Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hui Xiong</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.13914v1</id>\n",
      "    <title>Processing of incomplete images by (graph) convolutional neural networks</title>\n",
      "    <updated>2020-10-26T21:40:03Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.13914v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.13914v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We investigate the problem of training neural networks from incomplete images without replacing missing values. For this purpose, we first represent an image as a graph, in which missing pixels are entirely ignored. The graph image representation is processed using a spatial graph convolutional network (SGCN) -- a type of graph convolutional networks, which is a proper generalization of classical CNNs operating on images. On one hand, our approach avoids the problem of missing data imputation while, on the other hand, there is a natural correspondence between CNNs and SGCN. Experiments confirm that our approach performs better than analogical CNNs with the imputation of missing values on typical classification and reconstruction tasks.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-26T21:40:03Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Tomasz Danel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marek Śmieja</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Łukasz Struski</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Przemysław Spurek</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Łukasz Maziarka</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14462v2</id>\n",
      "    <title>Deep Probabilistic Imaging: Uncertainty Quantification and Multi-modal Solution Characterization for Computational Imaging</title>\n",
      "    <updated>2020-12-17T06:13:58Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14462v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14462v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Computational image reconstruction algorithms generally produce a single image without any measure of uncertainty or confidence. Regularized Maximum Likelihood (RML) and feed-forward deep learning approaches for inverse problems typically focus on recovering a point estimate. This is a serious limitation when working with underdetermined imaging systems, where it is conceivable that multiple image modes would be consistent with the measured data. Characterizing the space of probable images that explain the observational data is therefore crucial. In this paper, we propose a variational deep probabilistic imaging approach to quantify reconstruction uncertainty. Deep Probabilistic Imaging (DPI) employs an untrained deep generative model to estimate a posterior distribution of an unobserved image. This approach does not require any training data; instead, it optimizes the weights of a neural network to generate image samples that fit a particular measurement dataset. Once the network weights have been learned, the posterior distribution can be efficiently sampled. We demonstrate this approach in the context of interferometric radio imaging, which is used for black hole imaging with the Event Horizon Telescope, and compressed sensing Magnetic Resonance Imaging (MRI).</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"astro-ph.IM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-27T17:23:09Z</published>\n",
      "    <arxiv:comment>This paper has been accepted to AAAI 2021. Keywords: Computational Imaging, Normalizing Flow, Uncertainty Quantification, Interferometry, MRI</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>He Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Katherine L. Bouman</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2010.14484v2</id>\n",
      "    <title>One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL</title>\n",
      "    <updated>2020-12-07T22:33:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2010.14484v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2010.14484v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>While reinforcement learning algorithms can learn effective policies for complex tasks, these policies are often brittle to even minor task variations, especially when variations are not explicitly provided during training. One natural approach to this problem is to train agents with manually specified variation in the training task or environment. However, this may be infeasible in practical situations, either because making perturbations is not possible, or because it is unclear how to choose suitable perturbation strategies without sacrificing performance. The key insight of this work is that learning diverse behaviors for accomplishing a task can directly lead to behavior that generalizes to varying environments, without needing to perform explicit perturbations during training. By identifying multiple solutions for the task in a single environment during training, our approach can generalize to new situations by abandoning solutions that are no longer effective and adopting those that are. We theoretically characterize a robustness set of environments that arises from our algorithm and empirically find that our diversity-driven approach can extrapolate to various changes in the environment and task.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-10-27T17:41:57Z</published>\n",
      "    <arxiv:comment>Accepted at NeurIPS 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Saurabh Kumar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Aviral Kumar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sergey Levine</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chelsea Finn</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.11655v1</id>\n",
      "    <title>How to tune the RBF SVM hyperparameters?: An empirical evaluation of 18 search algorithms</title>\n",
      "    <updated>2020-08-26T16:28:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.11655v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.11655v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>SVM with an RBF kernel is usually one of the best classification algorithms for most data sets, but it is important to tune the two hyperparameters $C$ and $γ$ to the data itself. In general, the selection of the hyperparameters is a non-convex optimization problem and thus many algorithms have been proposed to solve it, among them: grid search, random search, Bayesian optimization, simulated annealing, particle swarm optimization, Nelder Mead, and others. There have also been proposals to decouple the selection of $γ$ and $C$. We empirically compare 18 of these proposed search algorithms (with different parameterizations for a total of 47 combinations) on 115 real-life binary data sets. We find (among other things) that trees of Parzen estimators and particle swarm optimization select better hyperparameters with only a slight increase in computation time with respect to a grid search with the same number of evaluations. We also find that spending too much computational effort searching the hyperparameters will not likely result in better performance for future data and that there are no significant differences among the different procedures to select the best set of hyperparameters when more than one is found by the search algorithms.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-26T16:28:48Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Jacques Wainer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pablo Fonseca</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.11672v3</id>\n",
      "    <title>DeepSOCIAL: Social Distancing Monitoring and Infection Risk Assessment in COVID-19 Pandemic</title>\n",
      "    <updated>2020-11-28T13:46:27Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.11672v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.11672v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Social distancing is a recommended solution by the World Health Organisation (WHO) to minimise the spread of COVID-19 in public places. The majority of governments and national health authorities have set the 2-meter physical distancing as a mandatory safety measure in shopping centres, schools and other covered areas. In this research, we develop a hybrid Computer Vision and YOLOv4-based Deep Neural Network model for automated people detection in the crowd in indoor and outdoor environments using common CCTV security cameras. The proposed DNN model in combination with an adapted inverse perspective mapping (IPM) technique and SORT tracking algorithm leads to a robust people detection and social distancing monitoring. The model has been trained against two most comprehensive datasets by the time of the research the Microsoft Common Objects in Context (MS COCO) and Google Open Image datasets. The system has been evaluated against the Oxford Town Centre dataset with superior performance compared to three state-of-the-art methods. The evaluation has been conducted in challenging conditions, including occlusion, partial visibility, and under lighting variations with the mean average precision of 99.8% and the real-time speed of 24.1 fps. We also provide an online infection risk assessment scheme by statistical analysis of the Spatio-temporal data from people's moving trajectories and the rate of social distancing violations. The developed model is a generic and accurate people detection and tracking solution that can be applied in many other fields such as autonomous vehicles, human action recognition, anomaly detection, sports, crowd analysis, or any other research areas where the human detection is in the centre of attention.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"physics.med-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-26T16:56:57Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <arxiv:journal_ref>Applied Sciences. 2020, 10, 7514</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Mahdi Rezaei</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mohsen Azarmi</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.3390/app10217514</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.3390/app10217514\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.11788v1</id>\n",
      "    <title>Share Price Prediction of Aerospace Relevant Companies with Recurrent Neural Networks based on PCA</title>\n",
      "    <updated>2020-08-26T20:16:33Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.11788v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.11788v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The capital market plays a vital role in marketing operations for aerospace industry. However, due to the uncertainty and complexity of the stock market and many cyclical factors, the stock prices of listed aerospace companies fluctuate significantly. This makes the share price prediction challengeable. To improve the prediction of share price for aerospace industry sector and well understand the impact of various indicators on stock prices, we provided a hybrid prediction model by the combination of Principal Component Analysis (PCA) and Recurrent Neural Networks. We investigated two types of aerospace industries (manufacturer and operator). The experimental results show that PCA could improve both accuracy and efficiency of prediction. Various factors could influence the performance of prediction models, such as finance data, extracted features, optimisation algorithms, and parameters of the prediction model. The selection of features may depend on the stability of historical data: technical features could be the first option when the share price is stable, whereas fundamental features could be better when the share price has high fluctuation. The delays of RNN also depend on the stability of historical data for different types of companies. It would be more accurate through using short-term historical data for aerospace manufacturers, whereas using long-term historical data for aerospace operating airlines. The developed model could be an intelligent agent in an automatic stock prediction system, with which, the financial industry could make a prompt decision for their economic strategies and business activities in terms of predicted future share price, thus improving the return on investment. Currently, COVID-19 severely influences aerospace industries. The developed approach can be used to predict the share price of aerospace industries at post COVID-19 time.</summary>\n",
      "    <category term=\"q-fin.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-26T20:16:33Z</published>\n",
      "    <arxiv:comment>38 pages, 17 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"q-fin.ST\"/>\n",
      "    <author>\n",
      "      <name>Linyu Zheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hongmei He</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.11811v1</id>\n",
      "    <title>Constrained Markov Decision Processes via Backward Value Functions</title>\n",
      "    <updated>2020-08-26T20:56:16Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.11811v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.11811v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Although Reinforcement Learning (RL) algorithms have found tremendous success in simulated domains, they often cannot directly be applied to physical systems, especially in cases where there are hard constraints to satisfy (e.g. on safety or resources). In standard RL, the agent is incentivized to explore any behavior as long as it maximizes rewards, but in the real world, undesired behavior can damage either the system or the agent in a way that breaks the learning process itself. In this work, we model the problem of learning with constraints as a Constrained Markov Decision Process and provide a new on-policy formulation for solving it. A key contribution of our approach is to translate cumulative cost constraints into state-based constraints. Through this, we define a safe policy improvement method which maximizes returns while ensuring that the constraints are satisfied at every step. We provide theoretical guarantees under which the agent converges while ensuring safety over the course of training. We also highlight the computational advantages of this approach. The effectiveness of our approach is demonstrated on safe navigation tasks and in safety-constrained versions of MuJoCo environments, with deep neural networks.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-26T20:56:16Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Harsh Satija</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Philip Amortila</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joelle Pineau</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.11824v1</id>\n",
      "    <title>Optimization for Supervised Machine Learning: Randomized Algorithms for Data and Parameters</title>\n",
      "    <updated>2020-08-26T21:15:18Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.11824v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.11824v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Many key problems in machine learning and data science are routinely modeled as optimization problems and solved via optimization algorithms. With the increase of the volume of data and the size and complexity of the statistical models used to formulate these often ill-conditioned optimization tasks, there is a need for new efficient algorithms able to cope with these challenges. In this thesis, we deal with each of these sources of difficulty in a different way. To efficiently address the big data issue, we develop new methods which in each iteration examine a small random subset of the training data only. To handle the big model issue, we develop methods which in each iteration update a random subset of the model parameters only. Finally, to deal with ill-conditioned problems, we devise methods that incorporate either higher-order information or Nesterov's acceleration/momentum. In all cases, randomness is viewed as a powerful algorithmic tool that we tune, both in theory and in experiments, to achieve the best results. Our algorithms have their primary application in training supervised machine learning models via regularized empirical risk minimization, which is the dominant paradigm for training such models. However, due to their generality, our methods can be applied in many other fields, including but not limited to data science, engineering, scientific computing, and statistics.</summary>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-26T21:15:18Z</published>\n",
      "    <arxiv:comment>PhD thesis, 425 pages, 75 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"math.OC\"/>\n",
      "    <author>\n",
      "      <name>Filip Hanzely</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.25781/KAUST-4F2DH</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.25781/KAUST-4F2DH\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.10148v1</id>\n",
      "    <title>Drive Safe: Cognitive-Behavioral Mining for Intelligent Transportation Cyber-Physical System</title>\n",
      "    <updated>2020-08-24T01:19:40Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.10148v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.10148v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper presents a cognitive behavioral-based driver mood repairment platform in intelligent transportation cyber-physical systems (IT-CPS) for road safety. In particular, we propose a driving safety platform for distracted drivers, namely \\emph{drive safe}, in IT-CPS. The proposed platform recognizes the distracting activities of the drivers as well as their emotions for mood repair. Further, we develop a prototype of the proposed drive safe platform to establish proof-of-concept (PoC) for the road safety in IT-CPS. In the developed driving safety platform, we employ five AI and statistical-based models to infer a vehicle driver's cognitive-behavioral mining to ensure safe driving during the drive. Especially, capsule network (CN), maximum likelihood (ML), convolutional neural network (CNN), Apriori algorithm, and Bayesian network (BN) are deployed for driver activity recognition, environmental feature extraction, mood recognition, sequential pattern mining, and content recommendation for affective mood repairment of the driver, respectively. Besides, we develop a communication module to interact with the systems in IT-CPS asynchronously. Thus, the developed drive safe PoC can guide the vehicle drivers when they are distracted from driving due to the cognitive-behavioral factors. Finally, we have performed a qualitative evaluation to measure the usability and effectiveness of the developed drive safe platform. We observe that the P-value is 0.0041 (i.e., &lt; 0.05) in the ANOVA test. Moreover, the confidence interval analysis also shows significant gains in prevalence value which is around 0.93 for a 95% confidence level. The aforementioned statistical results indicate high reliability in terms of driver's safety and mental state.</summary>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-24T01:19:40Z</published>\n",
      "    <arxiv:comment>Submitted to IEEE Transactions on Intelligent Transportation Systems, Special Issue on Technologies for risk mitigation and support of impaired drivers</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.HC\"/>\n",
      "    <author>\n",
      "      <name>Md. Shirajum Munir</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sarder Fakhrul Abedin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ki Tae Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Do Hyeon Kim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Md. Golam Rabiul Alam</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Choong Seon Hong</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.10224v3</id>\n",
      "    <title>Variable Compliance Control for Robotic Peg-in-Hole Assembly: A Deep Reinforcement Learning Approach</title>\n",
      "    <updated>2020-11-21T04:05:04Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.10224v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.10224v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Industrial robot manipulators are playing a more significant role in modern manufacturing industries. Though peg-in-hole assembly is a common industrial task which has been extensively researched, safely solving complex high precision assembly in an unstructured environment remains an open problem. Reinforcement Learning (RL) methods have been proven successful in solving manipulation tasks autonomously. However, RL is still not widely adopted on real robotic systems because working with real hardware entails additional challenges, especially when using position-controlled manipulators. The main contribution of this work is a learning-based method to solve peg-in-hole tasks with position uncertainty of the hole. We proposed the use of an off-policy model-free reinforcement learning method and bootstrap the training speed by using several transfer learning techniques (sim2real) and domain randomization. Our proposed learning framework for position-controlled robots was extensively evaluated on contact-rich insertion tasks on a variety of environments.</summary>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-24T06:53:19Z</published>\n",
      "    <arxiv:comment>17 pages,12 figures, supplemental video https://youtu.be/v4fREpMk7kU</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.RO\"/>\n",
      "    <arxiv:journal_ref>Appl. Sci. 2020, 10(19), 6923</arxiv:journal_ref>\n",
      "    <author>\n",
      "      <name>Cristian C. Beltran-Hernandez</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Damien Petit</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ixchel G. Ramirez-Alpizar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kensuke Harada</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.3390/app10196923</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.3390/app10196923\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2008.08523v1</id>\n",
      "    <title>Scene Text Detection with Selected Anchor</title>\n",
      "    <updated>2020-08-19T16:03:13Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2008.08523v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2008.08523v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Object proposal technique with dense anchoring scheme for scene text detection were applied frequently to achieve high recall. It results in the significant improvement in accuracy but waste of computational searching, regression and classification. In this paper, we propose an anchor selection-based region proposal network (AS-RPN) using effective selected anchors instead of dense anchors to extract text proposals. The center, scales, aspect ratios and orientations of anchors are learnable instead of fixing, which leads to high recall and greatly reduced numbers of anchors. By replacing the anchor-based RPN in Faster RCNN, the AS-RPN-based Faster RCNN can achieve comparable performance with previous state-of-the-art text detecting approaches on standard benchmarks, including COCO-Text, ICDAR2013, ICDAR2015 and MSRA-TD500 when using single-scale and single model (ResNet50) testing only.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-19T16:03:13Z</published>\n",
      "    <arxiv:comment>8 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Anna Zhu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hang Du</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shengwu Xiong</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.01008v1</id>\n",
      "    <title>Cross-Utterance Language Models with Acoustic Error Sampling</title>\n",
      "    <updated>2020-08-19T17:40:11Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.01008v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.01008v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The effective exploitation of richer contextual information in language models (LMs) is a long-standing research problem for automatic speech recognition (ASR). A cross-utterance LM (CULM) is proposed in this paper, which augments the input to a standard long short-term memory (LSTM) LM with a context vector derived from past and future utterances using an extraction network. The extraction network uses another LSTM to encode surrounding utterances into vectors which are integrated into a context vector using either a projection of LSTM final hidden states, or a multi-head self-attentive layer. In addition, an acoustic error sampling technique is proposed to reduce the mismatch between training and test-time. This is achieved by considering possible ASR errors into the model training procedure, and can therefore improve the word error rate (WER). Experiments performed on both AMI and Switchboard datasets show that CULMs outperform the LSTM LM baseline WER. In particular, the CULM with a self-attentive layer-based extraction network and acoustic error sampling achieves 0.6% absolute WER reduction on AMI, 0.3% WER reduction on the Switchboard part and 0.9% WER reduction on the Callhome part of Eval2000 test set over the respective baselines.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-19T17:40:11Z</published>\n",
      "    <arxiv:comment>5 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>G. Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>C. Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>P. C. Woodland</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.01048v2</id>\n",
      "    <title>MALCOM: Generating Malicious Comments to Attack Neural Fake News Detection Models</title>\n",
      "    <updated>2020-09-27T10:15:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.01048v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.01048v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In recent years, the proliferation of so-called \"fake news\" has caused much disruptions in society and weakened the news ecosystem. Therefore, to mitigate such problems, researchers have developed state-of-the-art models to auto-detect fake news on social media using sophisticated data science and machine learning techniques. In this work, then, we ask \"what if adversaries attempt to attack such detection models?\" and investigate related issues by (i) proposing a novel threat model against fake news detectors, in which adversaries can post malicious comments toward news articles to mislead fake news detectors, and (ii) developing MALCOM, an end-to-end adversarial comment generation framework to achieve such an attack. Through a comprehensive evaluation, we demonstrate that about 94% and 93.5% of the time on average MALCOM can successfully mislead five of the latest neural detection models to always output targeted real and fake news labels. Furthermore, MALCOM can also fool black box fake news detectors to always output real news labels 90% of the time on average. We also compare our attack model with four baselines across two real-world datasets, not only on attack performance but also on generated quality, coherency, transferability, and robustness.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-01T01:26:01Z</published>\n",
      "    <arxiv:comment>Accepted at the 20th IEEE International Conference on Data Mining (ICDM 2020)</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Thai Le</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Suhang Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dongwon Lee</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.01076v1</id>\n",
      "    <title>Teaching a Machine to Diagnose a Heart Disease; Beginning from digitizing scanned ECGs to detecting the Brugada Syndrome (BrS)</title>\n",
      "    <updated>2020-08-27T09:12:50Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.01076v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.01076v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Medical diagnoses can shape and change the life of a person drastically. Therefore, it is always best advised to collect as much evidence as possible to be certain about the diagnosis. Unfortunately, in the case of the Brugada Syndrome (BrS), a rare and inherited heart disease, only one diagnostic criterion exists, namely, a typical pattern in the Electrocardiogram (ECG). In the following treatise, we question whether the investigation of ECG strips by the means of machine learning methods improves the detection of BrS positive cases and hence, the diagnostic process. We propose a pipeline that reads in scanned images of ECGs, and transforms the encaptured signals to digital time-voltage data after several processing steps. Then, we present a long short-term memory (LSTM) classifier that is built based on the previously extracted data and that makes the diagnosis. The proposed pipeline distinguishes between three major types of ECG images and recreates each recorded lead signal. Features and quality are retained during the digitization of the data, albeit some encountered issues are not fully removed (Part I). Nevertheless, the results of the aforesaid program are suitable for further investigation of the ECG by a computational method such as the proposed classifier which proves the concept and could be the architectural basis for future research (Part II). This thesis is divided into two parts as they are part of the same process but conceptually different. It is hoped that this work builds a new foundation for computational investigations in the case of the BrS and its diagnosis.</summary>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-27T09:12:50Z</published>\n",
      "    <arxiv:primary_category term=\"eess.SP\"/>\n",
      "    <author>\n",
      "      <name>Simon Jaxy</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.01215v3</id>\n",
      "    <title>Excavating \"Excavating AI\": The Elephant in the Gallery</title>\n",
      "    <updated>2020-12-24T01:27:53Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.01215v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.01215v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Two art exhibitions, \"Training Humans\" and \"Making Faces,\" and the accompanying essay \"Excavating AI: The politics of images in machine learning training sets\" by Kate Crawford and Trevor Paglen, are making substantial impact on discourse taking place in the social and mass media networks, and some scholarly circles. Critical scrutiny reveals, however, a self-contradictory stance regarding informed consent for the use of facial images, as well as serious flaws in their critique of ML training sets. Our analysis underlines the non-negotiability of informed consent when using human data in artistic and other contexts, and clarifies issues relating to the description of ML training sets.</summary>\n",
      "    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-02T17:42:06Z</published>\n",
      "    <arxiv:comment>15 pages, 4 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CY\"/>\n",
      "    <author>\n",
      "      <name>Michael J. Lyons</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.5281/zenodo.4037538</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.5281/zenodo.4037538\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.00104v1</id>\n",
      "    <title>A Framework For Contrastive Self-Supervised Learning And Designing A New Approach</title>\n",
      "    <updated>2020-08-31T21:11:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.00104v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.00104v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Contrastive self-supervised learning (CSL) is an approach to learn useful representations by solving a pretext task that selects and compares anchor, negative and positive (APN) features from an unlabeled dataset. We present a conceptual framework that characterizes CSL approaches in five aspects (1) data augmentation pipeline, (2) encoder selection, (3) representation extraction, (4) similarity measure, and (5) loss function. We analyze three leading CSL approaches--AMDIM, CPC, and SimCLR--, and show that despite different motivations, they are special cases under this framework. We show the utility of our framework by designing Yet Another DIM (YADIM) which achieves competitive results on CIFAR-10, STL-10 and ImageNet, and is more robust to the choice of encoder and the representation extraction strategy. To support ongoing CSL research, we release the PyTorch implementation of this conceptual framework along with standardized implementations of AMDIM, CPC (V2), SimCLR, BYOL, Moco (V2) and YADIM.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-31T21:11:48Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>William Falcon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kyunghyun Cho</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.00105v1</id>\n",
      "    <title>Fast Grant Learning-Based Approach for Machine Type Communications with NOMA</title>\n",
      "    <updated>2020-08-31T21:14:21Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.00105v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.00105v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>In this paper, we propose a non-orthogonal multiple access (NOMA)-based communication framework that allows machine type devices (MTDs) to access the network while avoiding congestion. The proposed technique is a 2-step mechanism that first employs fast uplink grant to schedule the devices without sending a request to the base station (BS). Secondly, NOMA pairing is employed in a distributed manner to reduce signaling overhead. Due to the limited capability of information gathering at the BS in massive scenarios, learning techniques are best fit for such problems. Therefore, multi-arm bandit learning is adopted to schedule the fast grant MTDs. Then, constrained random NOMA pairing is proposed that assists in decoupling the two main challenges of fast uplink grant schemes namely, active set prediction and optimal scheduling. Using NOMA, we were able to significantly reduce the resource wastage due to prediction errors. Additionally, the results show that the proposed scheme can easily attain the impractical optimal OMA performance, in terms of the achievable rewards, at an affordable complexity.</summary>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-08-31T21:14:21Z</published>\n",
      "    <arxiv:primary_category term=\"cs.IT\"/>\n",
      "    <author>\n",
      "      <name>Manal El Tanab</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Walaa Hamouda</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.00584v1</id>\n",
      "    <title>Quality-aware semi-supervised learning for CMR segmentation</title>\n",
      "    <updated>2020-09-01T17:18:22Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.00584v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.00584v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>One of the challenges in developing deep learning algorithms for medical image segmentation is the scarcity of annotated training data. To overcome this limitation, data augmentation and semi-supervised learning (SSL) methods have been developed. However, these methods have limited effectiveness as they either exploit the existing data set only (data augmentation) or risk negative impact by adding poor training examples (SSL). Segmentations are rarely the final product of medical image analysis - they are typically used in downstream tasks to infer higher-order patterns to evaluate diseases. Clinicians take into account a wealth of prior knowledge on biophysics and physiology when evaluating image analysis results. We have used these clinical assessments in previous works to create robust quality-control (QC) classifiers for automated cardiac magnetic resonance (CMR) analysis. In this paper, we propose a novel scheme that uses QC of the downstream task to identify high quality outputs of CMR segmentation networks, that are subsequently utilised for further network training. In essence, this provides quality-aware augmentation of training data in a variant of SSL for segmentation networks (semiQCSeg). We evaluate our approach in two CMR segmentation tasks (aortic and short axis cardiac volume segmentation) using UK Biobank data and two commonly used network architectures (U-net and a Fully Convolutional Network) and compare against supervised and SSL strategies. We show that semiQCSeg improves training of the segmentation networks. It decreases the need for labelled data, while outperforming the other methods in terms of Dice and clinical metrics. SemiQCSeg can be an efficient approach for training segmentation networks for medical image data when labelled datasets are scarce.</summary>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-01T17:18:22Z</published>\n",
      "    <arxiv:comment>MICCAI STACOM 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.IV\"/>\n",
      "    <author>\n",
      "      <name>Bram Ruijsink</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Esther Puyol-Anton</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ye Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wenja Bai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eric Kerfoot</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Reza Razavi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrew P. King</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.00675v1</id>\n",
      "    <title>Applying a random projection algorithm to optimize machine learning model for predicting peritoneal metastasis in gastric cancer patients using CT images</title>\n",
      "    <updated>2020-09-01T19:53:09Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.00675v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.00675v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Background and Objective: Non-invasively predicting the risk of cancer metastasis before surgery plays an essential role in determining optimal treatment methods for cancer patients (including who can benefit from neoadjuvant chemotherapy). Although developing radiomics based machine learning (ML) models has attracted broad research interest for this purpose, it often faces a challenge of how to build a highly performed and robust ML model using small and imbalanced image datasets. Methods: In this study, we explore a new approach to build an optimal ML model. A retrospective dataset involving abdominal computed tomography (CT) images acquired from 159 patients diagnosed with gastric cancer is assembled. Among them, 121 cases have peritoneal metastasis (PM), while 38 cases do not have PM. A computer-aided detection (CAD) scheme is first applied to segment primary gastric tumor volumes and initially computes 315 image features. Then, two Gradient Boosting Machine (GBM) models embedded with two different feature dimensionality reduction methods, namely, the principal component analysis (PCA) and a random projection algorithm (RPA) and a synthetic minority oversampling technique, are built to predict the risk of the patients having PM. All GBM models are trained and tested using a leave-one-case-out cross-validation method. Results: Results show that the GBM embedded with RPA yielded a significantly higher prediction accuracy (71.2%) than using PCA (65.2%) (p&lt;0.05). Conclusions: The study demonstrated that CT images of the primary gastric tumors contain discriminatory information to predict the risk of PM, and RPA is a promising method to generate optimal feature vector, improving the performance of ML models of medical images.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-01T19:53:09Z</published>\n",
      "    <arxiv:comment>24 pages, 7 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Seyedehnafiseh Mirniaharikandehei</name>\n",
      "      <arxiv:affiliation>School of Electrical and Computer Engineering, University of Oklahoma, Norman, OK, USA</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Morteza Heidari</name>\n",
      "      <arxiv:affiliation>School of Electrical and Computer Engineering, University of Oklahoma, Norman, OK, USA</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gopichandh Danala</name>\n",
      "      <arxiv:affiliation>School of Electrical and Computer Engineering, University of Oklahoma, Norman, OK, USA</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sivaramakrishnan Lakshmivarahan</name>\n",
      "      <arxiv:affiliation>School of Computer Sciences, University of Oklahoma, Norman, OK, USA</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bin Zheng</name>\n",
      "      <arxiv:affiliation>School of Electrical and Computer Engineering, University of Oklahoma, Norman, OK, USA</arxiv:affiliation>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.11974v1</id>\n",
      "    <title>Bayesian Topological Learning for Classifying the Structure of Biological Networks</title>\n",
      "    <updated>2020-09-24T22:43:03Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.11974v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.11974v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Actin cytoskeleton networks generate local topological signatures due to the natural variations in the number, size, and shape of holes of the networks. Persistent homology is a method that explores these topological properties of data and summarizes them as persistence diagrams. In this work, we analyze and classify these filament networks by transforming them into persistence diagrams whose variability is quantified via a Bayesian framework on the space of persistence diagrams. The proposed generalized Bayesian framework adopts an independent and identically distributed cluster point process characterization of persistence diagrams and relies on a substitution likelihood argument. This framework provides the flexibility to estimate the posterior cardinality distribution of points in a persistence diagram and the posterior spatial distribution simultaneously. We present a closed form of the posteriors under the assumption of Gaussian mixtures and binomials for prior intensity and cardinality respectively. Using this posterior calculation, we implement a Bayes factor algorithm to classify the actin filament networks and benchmark it against several state-of-the-art classification methods.</summary>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-24T22:43:03Z</published>\n",
      "    <arxiv:comment>30 pages and 10 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"stat.ML\"/>\n",
      "    <author>\n",
      "      <name>Vasileios Maroulas</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cassie Putman Micucci</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Farzana Nasrin</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.11992v1</id>\n",
      "    <title>A physics-informed operator regression framework for extracting data-driven continuum models</title>\n",
      "    <updated>2020-09-25T01:13:51Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.11992v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.11992v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The application of deep learning toward discovery of data-driven models requires careful application of inductive biases to obtain a description of physics which is both accurate and robust. We present here a framework for discovering continuum models from high fidelity molecular simulation data. Our approach applies a neural network parameterization of governing physics in modal space, allowing a characterization of differential operators while providing structure which may be used to impose biases related to symmetry, isotropy, and conservation form. We demonstrate the effectiveness of our framework for a variety of physics, including local and nonlocal diffusion processes and single and multiphase flows. For the flow physics we demonstrate this approach leads to a learned operator that generalizes to system characteristics not included in the training sets, such as variable particle sizes, densities, and concentration.</summary>\n",
      "    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-25T01:13:51Z</published>\n",
      "    <arxiv:comment>37 pages, 15 figures</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"physics.comp-ph\"/>\n",
      "    <author>\n",
      "      <name>Ravi G. Patel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nathaniel A. Trask</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mitchell A. Wood</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eric C. Cyr</name>\n",
      "    </author>\n",
      "    <arxiv:doi>10.1016/j.cma.2020.113500</arxiv:doi>\n",
      "    <link rel=\"related\" href=\"https://doi.org/10.1016/j.cma.2020.113500\" title=\"doi\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.12013v2</id>\n",
      "    <title>Revealing the Myth of Higher-Order Inference in Coreference Resolution</title>\n",
      "    <updated>2020-09-28T22:47:33Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.12013v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.12013v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>This paper analyzes the impact of higher-order inference (HOI) on the task of coreference resolution. HOI has been adapted by almost all recent coreference resolution models without taking much investigation on its true effectiveness over representation learning. To make a comprehensive analysis, we implement an end-to-end coreference system as well as four HOI approaches, attended antecedent, entity equalization, span clustering, and cluster merging, where the latter two are our original methods. We find that given a high-performing encoder such as SpanBERT, the impact of HOI is negative to marginal, providing a new perspective of HOI to this task. Our best model using cluster merging shows the Avg-F1 of 80.2 on the CoNLL 2012 shared task dataset in English.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-25T03:28:07Z</published>\n",
      "    <arxiv:comment>Accepted to EMNLP 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Liyan Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jinho D. Choi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.12042v1</id>\n",
      "    <title>Deep Autoencoding GMM-based Unsupervised Anomaly Detection in Acoustic Signals and its Hyper-parameter Optimization</title>\n",
      "    <updated>2020-09-25T06:14:59Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.12042v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.12042v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Failures or breakdowns in factory machinery can be costly to companies, so there is an increasing demand for automatic machine inspection. Existing approaches to acoustic signal-based unsupervised anomaly detection, such as those using a deep autoencoder (DA) or Gaussian mixture model (GMM), have poor anomaly-detection performance. In this work, we propose a new method based on a deep autoencoding Gaussian mixture model with hyper-parameter optimization (DAGMM-HO). In our method, the DAGMM-HO applies the conventional DAGMM to the audio domain for the first time, with the idea that its total optimization on reduction of dimensions and statistical modelling will improve the anomaly-detection performance. In addition, the DAGMM-HO solves the hyper-parameter sensitivity problem of the conventional DAGMM by performing hyper-parameter optimization based on the gap statistic and the cumulative eigenvalues. Our evaluation of the proposed method with experimental data of the industrial fans showed that it significantly outperforms previous approaches and achieves up to a 20% improvement based on the standard AUC score.</summary>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-25T06:14:59Z</published>\n",
      "    <arxiv:comment>5 pages, to appear in DCASE 2020 Workshop</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"eess.AS\"/>\n",
      "    <author>\n",
      "      <name>Harsh Purohit</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ryo Tanabe</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Takashi Endo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kaori Suefusa</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuki Nikaido</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yohei Kawaguchi</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.12056v2</id>\n",
      "    <title>No Answer is Better Than Wrong Answer: A Reflection Model for Document Level Machine Reading Comprehension</title>\n",
      "    <updated>2020-09-29T09:29:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.12056v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.12056v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The Natural Questions (NQ) benchmark set brings new challenges to Machine Reading Comprehension: the answers are not only at different levels of granularity (long and short), but also of richer types (including no-answer, yes/no, single-span and multi-span). In this paper, we target at this challenge and handle all answer types systematically. In particular, we propose a novel approach called Reflection Net which leverages a two-step training procedure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May.~20,~2020), our approach achieved the top 1 on both long and short answer leaderboard, with F1 scores of 77.2 and 64.1, respectively.</summary>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-25T06:57:52Z</published>\n",
      "    <arxiv:comment>Accepted by Findings of EMNLP 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CL\"/>\n",
      "    <author>\n",
      "      <name>Xuguang Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Linjun Shou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ming Gong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nan Duan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daxin Jiang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.12088v1</id>\n",
      "    <title>Training CNNs in Presence of JPEG Compression: Multimedia Forensics vs Computer Vision</title>\n",
      "    <updated>2020-09-25T08:47:21Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.12088v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.12088v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Convolutional Neural Networks (CNNs) have proved very accurate in multiple computer vision image classification tasks that required visual inspection in the past (e.g., object recognition, face detection, etc.). Motivated by these astonishing results, researchers have also started using CNNs to cope with image forensic problems (e.g., camera model identification, tampering detection, etc.). However, in computer vision, image classification methods typically rely on visual cues easily detectable by human eyes. Conversely, forensic solutions rely on almost invisible traces that are often very subtle and lie in the fine details of the image under analysis. For this reason, training a CNN to solve a forensic task requires some special care, as common processing operations (e.g., resampling, compression, etc.) can strongly hinder forensic traces. In this work, we focus on the effect that JPEG has on CNN training considering different computer vision and forensic image classification problems. Specifically, we consider the issues that rise from JPEG compression and misalignment of the JPEG grid. We show that it is necessary to consider these effects when generating a training dataset in order to properly train a forensic detector not losing generalization capability, whereas it is almost possible to ignore these effects for computer vision tasks.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-25T08:47:21Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Sara Mandelli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nicolò Bonettini</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paolo Bestagini</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stefano Tubaro</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.12098v1</id>\n",
      "    <title>Resource-Constrained On-Device Learning by Dynamic Averaging</title>\n",
      "    <updated>2020-09-25T09:29:10Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.12098v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.12098v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The communication between data-generating devices is partially responsible for a growing portion of the world's power consumption. Thus reducing communication is vital, both, from an economical and an ecological perspective. For machine learning, on-device learning avoids sending raw data, which can reduce communication substantially. Furthermore, not centralizing the data protects privacy-sensitive data. However, most learning algorithms require hardware with high computation power and thus high energy consumption. In contrast, ultra-low-power processors, like FPGAs or micro-controllers, allow for energy-efficient learning of local models. Combined with communication-efficient distributed learning strategies, this reduces the overall energy consumption and enables applications that were yet impossible due to limited energy on local devices. The major challenge is then, that the low-power processors typically only have integer processing capabilities. This paper investigates an approach to communication-efficient on-device learning of integer exponential families that can be executed on low-power processors, is privacy-preserving, and effectively minimizes communication. The empirical evaluation shows that the approach can reach a model quality comparable to a centrally learned regular model with an order of magnitude less communication. Comparing the overall energy consumption, this reduces the required energy for solving the machine learning task by a significant amount.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-25T09:29:10Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Lukas Heppe</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michael Kamp</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Linara Adilova</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Danny Heinrich</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nico Piatkowski</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Katharina Morik</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.12829v3</id>\n",
      "    <title>Domain Generalization for Medical Imaging Classification with Linear-Dependency Regularization</title>\n",
      "    <updated>2020-10-29T10:28:49Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.12829v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.12829v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recently, we have witnessed great progress in the field of medical imaging classification by adopting deep neural networks. However, the recent advanced models still require accessing sufficiently large and representative datasets for training, which is often unfeasible in clinically realistic environments. When trained on limited datasets, the deep neural network is lack of generalization capability, as the trained deep neural network on data within a certain distribution (e.g. the data captured by a certain device vendor or patient population) may not be able to generalize to the data with another distribution.\n",
      "  In this paper, we introduce a simple but effective approach to improve the generalization capability of deep neural networks in the field of medical imaging classification. Motivated by the observation that the domain variability of the medical images is to some extent compact, we propose to learn a representative feature space through variational encoding with a novel linear-dependency regularization term to capture the shareable information among medical data collected from different domains. As a result, the trained neural network is expected to equip with better generalization capability to the \"unseen\" medical data. Experimental results on two challenging medical imaging classification tasks indicate that our method can achieve better cross-domain generalization capability compared with state-of-the-art baselines.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-27T12:30:30Z</published>\n",
      "    <arxiv:comment>Accepted by NeurIPS, 2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Haoliang Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>YuFei Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Renjie Wan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shiqi Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tie-Qiang Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alex C. Kot</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.12864v1</id>\n",
      "    <title>Predicting Sim-to-Real Transfer with Probabilistic Dynamics Models</title>\n",
      "    <updated>2020-09-27T15:06:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.12864v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.12864v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We propose a method to predict the sim-to-real transfer performance of RL policies. Our transfer metric simplifies the selection of training setups (such as algorithm, hyperparameters, randomizations) and policies in simulation, without the need for extensive and time-consuming real-world rollouts. A probabilistic dynamics model is trained alongside the policy and evaluated on a fixed set of real-world trajectories to obtain the transfer metric. Experiments show that the transfer metric is highly correlated with policy performance in both simulated and real-world robotic environments for complex manipulation tasks. We further show that the transfer metric can predict the effect of training setups on policy transfer performance.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-27T15:06:54Z</published>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Lei M. Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matthias Plappert</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wojciech Zaremba</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.12569v1</id>\n",
      "    <title>DT-Net: A novel network based on multi-directional integrated convolution and threshold convolution</title>\n",
      "    <updated>2020-09-26T11:12:06Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.12569v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.12569v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Since medical image data sets contain few samples and singular features, lesions are viewed as highly similar to other tissues. The traditional neural network has a limited ability to learn features. Even if a host of feature maps is expanded to obtain more semantic information, the accuracy of segmenting the final medical image is slightly improved, and the features are excessively redundant. To solve the above problems, in this paper, we propose a novel end-to-end semantic segmentation algorithm, DT-Net, and use two new convolution strategies to better achieve end-to-end semantic segmentation of medical images. 1. In the feature mining and feature fusion stage, we construct a multi-directional integrated convolution (MDIC). The core idea is to use the multi-scale convolution to enhance the local multi-directional feature maps to generate enhanced feature maps and to mine the generated features that contain more semantics without increasing the number of feature maps. 2. We also aim to further excavate and retain more meaningful deep features reduce a host of noise features in the training process. Therefore, we propose a convolution thresholding strategy. The central idea is to set a threshold to eliminate a large number of redundant features and reduce computational complexity. Through the two strategies proposed above, the algorithm proposed in this paper produces state-of-the-art results on two public medical image datasets. We prove in detail that our proposed strategy plays an important role in feature mining and eliminating redundant features. Compared with the existing semantic segmentation algorithms, our proposed algorithm has better robustness.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-26T11:12:06Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Hongfeng You</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Long Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shengwei Tian</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiang Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yan Xing</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaojie Ma</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2009.12576v2</id>\n",
      "    <title>Inverse Rational Control with Partially Observable Continuous Nonlinear Dynamics</title>\n",
      "    <updated>2020-10-30T07:09:41Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2009.12576v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2009.12576v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>A fundamental question in neuroscience is how the brain creates an internal model of the world to guide actions using sequences of ambiguous sensory information. This is naturally formulated as a reinforcement learning problem under partial observations, where an agent must estimate relevant latent variables in the world from its evidence, anticipate possible future states, and choose actions that optimize total expected reward. This problem can be solved by control theory, which allows us to find the optimal actions for a given system dynamics and objective function. However, animals often appear to behave suboptimally. Why? We hypothesize that animals have their own flawed internal model of the world, and choose actions with the highest expected subjective reward according to that flawed model. We describe this behavior as rational but not optimal. The problem of Inverse Rational Control (IRC) aims to identify which internal model would best explain an agent's actions. Our contribution here generalizes past work on Inverse Rational Control which solved this problem for discrete control in partially observable Markov decision processes. Here we accommodate continuous nonlinear dynamics and continuous actions, and impute sensory observations corrupted by unknown noise that is private to the animal. We first build an optimal Bayesian agent that learns an optimal policy generalized over the entire model space of dynamics and subjective rewards using deep reinforcement learning. Crucially, this allows us to compute a likelihood over models for experimentally observable action trajectories acquired from a suboptimal agent. We then find the model parameters that maximize the likelihood using gradient ascent.</summary>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2020-09-26T11:47:48Z</published>\n",
      "    <arxiv:comment>NeurIPS2020</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.LG\"/>\n",
      "    <author>\n",
      "      <name>Minhae Kwon</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Saurabh Daptardar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Paul Schrater</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xaq Pitkow</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "</feed>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "api = \"http://export.arxiv.org/api/query?search_query=cat:cs.LG&start=0&max_results=1000\"\n",
    "\n",
    "response = requests.get(api)\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e037c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_raw = '../data/raw/arxiv_papers.csv'\n",
    "data_path_processed = '../data/processed/arxiv_papers.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41faa19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features\n",
    "features = ['id', 'title', 'summary', 'category', 'published']\n",
    "root = xml.etree.ElementTree.fromstring(response.text) # parses the raw XML text into a XML element tree.\n",
    "entries = root.findall('{http://www.w3.org/2005/Atom}entry') # finds all XML elements tagged as <entry>\n",
    "\n",
    "atom_ns = '{http://www.w3.org/2005/Atom}' # Define the Atom XML namespace\n",
    "\n",
    "with open(data_path_raw, 'w') as f:\n",
    "    f.write(','.join(features) + '\\n') # header row (features)\n",
    "    for entry in entries:\n",
    "        row = []\n",
    "        for feature_name in features:\n",
    "            if feature_name == 'category':\n",
    "                categories = []\n",
    "                # Find all category elements within the current entry\n",
    "                for cat_elem in entry.findall(atom_ns + 'category'):\n",
    "                    categories.append(cat_elem.attrib.get('term', ''))\n",
    "                row.append('|'.join(categories)) # Join the list of categories into a string\n",
    "            else:\n",
    "                element = entry.find(atom_ns + feature_name)\n",
    "                if element is not None:\n",
    "                    # remove newlines, extra spaces, and escape commas for CSV\n",
    "                    text_content = element.text.replace('\\n', ' ').strip() if element.text else ''\n",
    "                    row.append(text_content.replace(',', ''))\n",
    "                else:\n",
    "                    row.append('') # Appending empty string for missing data\n",
    "        f.write(','.join(row) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84106865",
   "metadata": {},
   "source": [
    "## EDA and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f7b3c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>category</th>\n",
       "      <th>published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2012.11510v1</td>\n",
       "      <td>Design Rule Checking with a CNN Based Feature ...</td>\n",
       "      <td>Design rule checking (DRC) is getting increasi...</td>\n",
       "      <td>[cs.LG]</td>\n",
       "      <td>2020-12-21T17:26:31Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2012.11638v1</td>\n",
       "      <td>Unsupervised in-distribution anomaly detection...</td>\n",
       "      <td>Anomaly detection is a key application of mach...</td>\n",
       "      <td>[cs.LG, hep-ex, physics.data-an]</td>\n",
       "      <td>2020-12-21T19:05:22Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2012.11325v1</td>\n",
       "      <td>Detecting Botnet Attacks in IoT Environments: ...</td>\n",
       "      <td>The increased reliance on the Internet and the...</td>\n",
       "      <td>[cs.CR, cs.LG, cs.NI]</td>\n",
       "      <td>2020-12-16T16:39:55Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2012.11327v1</td>\n",
       "      <td>Collaborative residual learners for automatic ...</td>\n",
       "      <td>Clinical coding is an administrative process t...</td>\n",
       "      <td>[cs.IR, cs.LG]</td>\n",
       "      <td>2020-12-16T07:07:27Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2012.11333v1</td>\n",
       "      <td>Ensemble model for pre-discharge icd10 coding ...</td>\n",
       "      <td>The translation of medical diagnosis to clinic...</td>\n",
       "      <td>[cs.IR, cs.LG]</td>\n",
       "      <td>2020-12-16T07:02:56Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0  http://arxiv.org/abs/2012.11510v1   \n",
       "1  http://arxiv.org/abs/2012.11638v1   \n",
       "2  http://arxiv.org/abs/2012.11325v1   \n",
       "3  http://arxiv.org/abs/2012.11327v1   \n",
       "4  http://arxiv.org/abs/2012.11333v1   \n",
       "\n",
       "                                               title  \\\n",
       "0  Design Rule Checking with a CNN Based Feature ...   \n",
       "1  Unsupervised in-distribution anomaly detection...   \n",
       "2  Detecting Botnet Attacks in IoT Environments: ...   \n",
       "3  Collaborative residual learners for automatic ...   \n",
       "4  Ensemble model for pre-discharge icd10 coding ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Design rule checking (DRC) is getting increasi...   \n",
       "1  Anomaly detection is a key application of mach...   \n",
       "2  The increased reliance on the Internet and the...   \n",
       "3  Clinical coding is an administrative process t...   \n",
       "4  The translation of medical diagnosis to clinic...   \n",
       "\n",
       "                           category             published  \n",
       "0                           [cs.LG]  2020-12-21T17:26:31Z  \n",
       "1  [cs.LG, hep-ex, physics.data-an]  2020-12-21T19:05:22Z  \n",
       "2             [cs.CR, cs.LG, cs.NI]  2020-12-16T16:39:55Z  \n",
       "3                    [cs.IR, cs.LG]  2020-12-16T07:07:27Z  \n",
       "4                    [cs.IR, cs.LG]  2020-12-16T07:02:56Z  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the raw data\n",
    "data = pd.read_csv(data_path_raw)\n",
    "data['category'] = data['category'].apply(lambda x: x.split('|')) # converts string -> list for category\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e3f292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories: ['cs.LG', 'hep-ex', 'physics.data-an', 'cs.CR', 'cs.NI', 'cs.IR', 'cs.SI', 'stat.ML', 'eess.IV', 'cs.CV', 'math.OC', 'math.PR', 'cs.RO', 'eess.AS', 'cs.SD', 'cs.CL', 'cs.AI', 'cs.CY', 'cs.IT', 'eess.SP', 'cs.DB', 'stat.AP', 'cs.CG', 'cs.NE', 'q-bio.NC', 'cs.GT', 'hep-lat', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'cs.SE', 'cs.LO', 'cs.DC', 'cond-mat.soft', 'math.ST', 'q-bio.MN', 'math.AT', 'q-bio.GN', 'cs.DS', 'stat.CO', 'cs.MA', 'cs.DM', 'cs.GR', 'econ.EM', 'q-bio.OT', 'stat.ME', 'quant-ph', 'cs.PL', 'physics.med-ph', 'q-bio.QM', 'cs.ET', 'eess.SY', 'physics.comp-ph', 'cs.HC', 'cs.DL', 'q-fin.ST', 'math.NA', 'q-bio.BM', 'q-bio.TO', 'cs.MS', 'physics.geo-ph', 'q-fin.CP', 'cs.MM', 'q-bio.PE', 'econ.GN', 'cond-mat.mtrl-sci', 'cs.AR', 'math.OA', 'cs.CE', 'astro-ph.IM', 'math.AP', 'math.MG', 'math.DS', 'math.CO', 'q-fin.GN', 'physics.soc-ph', 'cs.PF', 'cs.CC', 'cs.SC', 'q-fin.TR', 'astro-ph.EP']\n",
      "Number of unique categories: 80\n"
     ]
    }
   ],
   "source": [
    "# Unique categories\n",
    "unique_categories = []\n",
    "for categories in data['category']:\n",
    "    for category in categories:\n",
    "        if category not in unique_categories:\n",
    "            unique_categories.append(category)\n",
    "\n",
    "print(f\"Unique categories: {unique_categories}\")\n",
    "print(f\"Number of unique categories: {len(unique_categories)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62538eb1",
   "metadata": {},
   "source": [
    "## Summary cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69647806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      design rule checking (drc) is getting increasi...\n",
       "1      anomaly detection is a key application of mach...\n",
       "2      the increased reliance on the internet and the...\n",
       "3      clinical coding is an administrative process t...\n",
       "4      the translation of medical diagnosis to clinic...\n",
       "                             ...                        \n",
       "995    the communication between data-generating devi...\n",
       "996    recently we have witnessed great progress in t...\n",
       "997    we propose a method to predict the sim-to-real...\n",
       "998    since medical image data sets contain few samp...\n",
       "999    a fundamental question in neuroscience is how ...\n",
       "Name: summary, Length: 1000, dtype: str"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['summary'] = data['summary'].apply(lambda x: x.strip()) # extra space  \n",
    "data['summary'] = data['summary'].apply(lambda x: x.lower()) # lowercase\n",
    "data['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63b71d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(data_path_processed, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67d9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
